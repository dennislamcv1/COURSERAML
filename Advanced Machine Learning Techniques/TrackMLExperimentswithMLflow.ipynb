{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4645511",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Track and Compare Multiple Model Runs with MLflow\n",
    "\n",
    "## üìã Overview\n",
    "Embrace the journey of managing and comparing multiple model iterations with MLflow, a robust tool for experiment tracking in machine learning. Through this activity, you will gain hands-on experience in organizing and assessing various model runs to improve both reproducibility and predictability in your projects.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- ‚úÖ Perform multiple model runs with different hyperparameters\n",
    "- ‚úÖ Compare and analyze model performance by examining logged metrics and artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47581655",
   "metadata": {},
   "source": [
    "## Task 1: Dataset Selection and Preprocessing\n",
    "\n",
    "**Context:** Proper dataset selection and preprocessing ensures the data is clean and ready for modeling.\n",
    "\n",
    "**Steps:**\n",
    "1. Ensure the dataset is preprocessed: handle missing data, normalize features as needed, and split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aeaef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "# Task 1: Dataset Selection and Preprocessing\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae44eb8",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `train_test_split` from `sklearn.model_selection` for data splitting.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- The dataset should show the features and corresponding labels, demonstrating the preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d918ea6",
   "metadata": {},
   "source": [
    "## Task 2: Create Multiple Experiment Runs\n",
    "\n",
    "**Context:** Performing multiple experiments runs with varied hyperparameters allows comparison.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Develop a simple model using Logistic Regression.\n",
    "2. Conduct several runs by varying hyperparameters such as maximum number of iterations and regularization strength.\n",
    "3. Log these experiments with unique identifiers in MLflow for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Create Multiple Experiment Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac270a94",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `mlflow.start_run()` to encapsulate each experiment run.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Plots should clearly show the relationship between actual and predicted values for each run. \n",
    "- Legends should correctly identify each run based on hyperparameter configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f9ee1",
   "metadata": {},
   "source": [
    "## Task 3: Logging Parameters, Metrics, and Artifacts\n",
    "\n",
    "**Context:** Logging parameters, metrics, and artifacts helps to track and compare model runs in detail.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. For each run, log hyperparameters (e.g., regularization parameters), performance metrics (e.g., accuracy), and confusion matrices as artifacts.\n",
    "2. Ensure that logs are sufficiently detailed to support revisiting and comparing results across all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Logging Parameters, Metrics, and Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc451fa",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `mlflow.log_param`, `mlflow.log_metric`, and `mlflow.log_artifact` to save details for each run.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Logs should clearly show hyperparameters, metrics, and artifacts for each run in MLflow. \n",
    "- Entries should be well-documented to facilitate easy comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d2ab8",
   "metadata": {},
   "source": [
    "## Task 4: Analyze Model Performances\n",
    "\n",
    "**Context:** Analyzing logged information helps determine patterns and the impact of hyperparameter changes on model performance.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Assess the logged information to determine patterns and impact of hyperparameter changes on model performance.\n",
    "2. Reflect on how experiment logging aids in understanding model behavior and in decision-making processes for future model iterations.\n",
    "\n",
    "**üí° Tip:** Look for trends and correlations between hyperparameter settings and performance metrics.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "- Analysis should clearly show the relationship between hyperparameter changes and model performance. \n",
    "- Reflective documentation should provide insights into model behavior and recommendations for future experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63ef30",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "\n",
    "- Successfully selected and preprocessed the dataset\n",
    "- Performed and logged multiple experiment runs with different hyperparameters\n",
    "- Logged detailed parameters, metrics, and artifacts for each run\n",
    "- Analyzed and documented model performance and insights\n",
    "\n",
    "### üîç Common Issues & Solutions\n",
    "\n",
    "**Problem:** MLflow installation errors.   \n",
    "**Solution:** Ensure correct installation using `pip install mlflow` and verify using `mlflow --version`.\n",
    "\n",
    "**Problem:** Experiment logging issues.   \n",
    "**Solution:** Verify the use of `mlflow.start_run()` and ensure all details are correctly logged using `mlflow.log_param`, `mlflow.log_metric`, and `mlflow.log_artifact`.\n",
    "\n",
    "\n",
    "### üîë Key Points\n",
    "\n",
    "- MLflow provides robust tools for tracking, comparing, and analyzing multiple model runs.\n",
    "- Logging detailed parameters, metrics, and artifacts enhances reproducibility and predictability.\n",
    "- Analyzing logged information helps in decision-making for future experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb121f44",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48b7d5",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>    \n",
    "\n",
    "```python\n",
    "# Import Libraries    \n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    \n",
    "# Task 1: Dataset Selection and Preprocessing\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Impute missing values (if any) with the mean of each column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(iris.data)\n",
    "\n",
    "# Handle feature scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "# Splitting data   \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Task 2: Create Multiple Experiment Runs\n",
    "mlflow.set_experiment(\"Iris Classification\")\n",
    "\n",
    "hyperparameter_configs = [\n",
    "    {\"max_iter\": 100, \"C\": 1.0},\n",
    "    {\"max_iter\": 200, \"C\": 0.5},\n",
    "    {\"max_iter\": 300, \"C\": 0.75}\n",
    "]\n",
    "\n",
    "for config in hyperparameter_configs:\n",
    "    with mlflow.start_run():\n",
    "        model = LogisticRegression(max_iter=config[\"max_iter\"], C=config[\"C\"], solver='lbfgs', multi_class='auto')\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "# Task 3: Logging Parameters, Metrics, and Artifacts\n",
    "# Log model and parameters\n",
    "mlflow.sklearn.log_model(model, \"logistic_regression_model\")\n",
    "mlflow.log_param(\"max_iter\", config[\"max_iter\"])\n",
    "mlflow.log_param(\"C\", config[\"C\"])\n",
    "    \n",
    "# Evaluate and log metrics\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        \n",
    "# Log confusion matrix as an artifact\n",
    "confusion_mat = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(confusion_mat, index=iris.target_names, columns=iris.target_names)\n",
    "cm_df.to_csv(\"confusion_matrix.csv\")\n",
    "mlflow.log_artifact(\"confusion_matrix.csv\")\n",
    "```                                                                                               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
