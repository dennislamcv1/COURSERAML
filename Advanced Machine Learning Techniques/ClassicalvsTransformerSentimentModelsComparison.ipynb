{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5a951d",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Classical vs. Transformer Sentiment Models: A Head-to-Head Comparison\n",
    "\n",
    "## üìã Overview\n",
    "In this hands-on activity, you'll have the opportunity to put classical machine learning models like Naive Bayes against transformer-based models such as BERT in a head-to-head sentiment analysis showdown. By examining and contrasting these approaches, you'll gain insights into how each model performs, their strengths in different contexts, and situations where one might be preferred over the other.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- ‚úÖ Implement and evaluate classical machine learning models for sentiment analysis\n",
    "- ‚úÖ Fine-tune and assess transformer-based models for the same task\n",
    "- ‚úÖ Compare the performance and suitability of classical versus transformer models in various scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42801930",
   "metadata": {},
   "source": [
    "## Task 1: Data Exploration and Preparation\n",
    "\n",
    "**Context:** Proper data preparation ensures the sentiment dataset is clean and consistent for both models.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. A collection of product reviews or social media comments has been provided.\n",
    "2. Clean and preprocess the data consistently for both models, including tokenization and removal of unwanted characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e628713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Data Exploration and Preparation\n",
    "# Handle Imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Data Preparation\n",
    "reviews = [\"I love this movie! Amazing storyline and acting.\",\n",
    "           \"Horrible experience, I disliked this product.\",\n",
    "           \"Incredibly moving and inspiring film.\",\n",
    "           \"Waste of time, not recommended.\"]\n",
    "labels = [1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267f744",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `nltk` for tokenization and cleaning tasks.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "\n",
    "- Print the cleaned and preprocessed version of the first 5 text entries.\n",
    "\n",
    "**Expected output:** Cleaned and standardized text ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbf78d",
   "metadata": {},
   "source": [
    "## Task 2: Implementing Classical Machine Learning Models\n",
    "\n",
    "**Context:** Classical machine learning models like Naive Bayes use vectorized text data for classification.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Use the TF-IDF vectorization method to transform the text data into numerical vectors.\n",
    "2. Train a classical machine learning model such as Naive Bayes or SVM on the TF-IDF features.\n",
    "3. Evaluate the model's performance using metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b41c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Implementing Classical Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99145535",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `TfidfVectorizer` and `MultinomialNB` from sklearn.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "\n",
    "- Print the classification metrics for the Naive Bayes model.\n",
    "\n",
    "**Expected output:** Accuracy, precision, recall, and F1-score for the Naive Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45562bfa",
   "metadata": {},
   "source": [
    "## Task 3: Exploring Transformer Models\n",
    "\n",
    "**Context:** Transformer models like BERT require tokenization and fine-tuning for sentiment analysis.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Fine-tune a pre-trained transformer model such as BERT using the same dataset.\n",
    "2. Tokenize the data using the appropriate tokenizer from the Hugging Face library.\n",
    "3. Evaluate the model's performance with the same metrics used for the classical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f53c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Exploring Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c34e4",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `BertTokenizer` and `BertForSequenceClassification` from the `transformers` library.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "\n",
    "- Print the classification metrics for the BERT model.\n",
    "\n",
    "**Expected output:** Accuracy, precision, recall, and F1-score for the BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd35365",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "\n",
    "- Successfully obtained and preprocessed the sentiment dataset\n",
    "- Implemented and evaluated classical machine learning models for sentiment analysis\n",
    "- Fine-tuned and assessed transformer-based models for the same task\n",
    "- Compared the performance of classical vs. transformer models\n",
    "- Provided reflections and recommendations based on findings\n",
    "\n",
    "### üîç Common Issues & Solutions\n",
    "\n",
    "**Problem:** Text data not cleaning properly.   \n",
    "**Solution:** Ensure tokenization and cleaning steps are correctly specified and applied.\n",
    "\n",
    "**Problem:** Tokenization errors for transformer models.   \n",
    "**Solution:** Verify the use of the correct tokenizer model from the Hugging Face library.\n",
    "\n",
    "**Problem:** Differences in model performance not noticeable.   \n",
    "**Solution:** Ensure the dataset has enough variety and complexity to showcase the strengths of transformer models.\n",
    "\n",
    "### üîë Key Points\n",
    "\n",
    "- Classical models are fast and require less computing power but may struggle with nuanced language.\n",
    "- Transformer models are more nuanced and accurate but require more resources and longer training times.\n",
    "- Choosing the right model depends on the specific requirements and constraints of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9085de9",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c6f57",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>    \n",
    "\n",
    "```python\n",
    "# Task 1: Data Exploration and Preparation\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Data Preparation\n",
    "reviews = [\"I love this movie! Amazing storyline and acting.\",\n",
    "           \"Horrible experience, I disliked this product.\",\n",
    "           \"Incredibly moving and inspiring film.\",\n",
    "           \"Waste of time, not recommended.\"]\n",
    "labels = np.array([1, 0, 1, 0])\n",
    "\n",
    "# Task 2: Classical Machine Learning Model - Naive Bayes\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Naive Bayes\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "nb_accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "nb_report = classification_report(y_test, y_pred_nb)\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", nb_report)\n",
    "\n",
    "# Task 3: Transformer-Based Model - BERT\n",
    "# Tokenization and Encoding\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "train_encodings = tokenizer(reviews, truncation=True, padding=True, return_tensors='pt')\n",
    "train_labels = torch.tensor(labels)\n",
    "\n",
    "# Create class for wrapping training_dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            key: torch.tensor(val[idx]) for key, val in self.encodings.items()\n",
    "        } | {\n",
    "            \"labels\": torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)            \n",
    "\n",
    "# Fine-tune BERT\n",
    "bert_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir='./', num_train_epochs=2, per_device_train_batch_size=4)\n",
    "trainer = Trainer(model=bert_model, args=training_args, train_dataset=train_dataset)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate BERT\n",
    "test_review = tokenizer(reviews[1], return_tensors='pt')\n",
    "test_outputs = bert_model(**test_review)\n",
    "test_prediction = torch.argmax(test_outputs.logits, dim=-1)\n",
    "print(f\"BERT Predicted Sentiment for '{reviews[1]}': {test_prediction.item()}\")\n",
    "\n",
    "# Compare\n",
    "print(\"Comparison Insights:\")\n",
    "print(\"Classical models like Naive Bayes are fast and require less computing power but may struggle with nuanced language.\")\n",
    "print(\"Transformers like BERT are more nuanced and accurate but require more resources and longer training times.\")\n",
    "```  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
