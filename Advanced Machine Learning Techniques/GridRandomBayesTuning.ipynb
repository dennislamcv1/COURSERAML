{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a11d74d",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Grid, Random, or Bayesian? Tune and Compare Your Models\n",
    "\n",
    "## üìã Overview\n",
    "This activity challenges you to implement and compare three distinct hyperparameter tuning techniques: Grid Search, Random Search, and Bayesian Optimization. By the end, you‚Äôll have a hands-on understanding of how each method operates, its strengths and limitations, and where it might be best applied. You'll experience firsthand how these techniques impact model performance across different datasets, helping you make informed decisions in your future machine learning projects.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- ‚úÖ Implement and compare Grid Search, Random Search, and Bayesian Optimization for hyperparameter tuning\n",
    "- ‚úÖ Evaluate the effectiveness of different tuning methods using performance metrics\n",
    "- ‚úÖ Make informed decisions on which hyperparameter tuning method to use in various scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d2e50",
   "metadata": {},
   "source": [
    "## Task 1: Dataset Selection and Preprocessing\n",
    "\n",
    "**Context:** Proper dataset selection and preprocessing ensures the data is clean and ready for modeling.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Ensure the dataset is preprocessed: handle missing data, normalize features as needed, and split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f77a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Task 1: Dataset Selection and Preprocessing\n",
    "# Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52450200",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `train_test_split` from `sklearn.model_selection` for data splitting.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- The dataset should show the features and corresponding labels, demonstrating the preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb6acfd",
   "metadata": {},
   "source": [
    "## Task 2: Implementing Grid Search\n",
    "\n",
    "**Context:** Grid Search involves exhaustively searching through a predefined hyperparameter grid.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Set up a hyperparameter grid for a Random Forest model.\n",
    "2. Use `GridSearchCV` from scikit-learn to carry out the grid search.\n",
    "3. Record the best parameters along with performance metrics like accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Implementing Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c25e737",
   "metadata": {},
   "source": [
    "üí° **Tip:** Define a comprehensive grid for key hyperparameters.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Plots should clearly show the relationship between actual and predicted values for the best model from Grid Search. Legends - should correctly identify each data series in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf503c70",
   "metadata": {},
   "source": [
    "## Task 3: Applying Random Search\n",
    "\n",
    "**Context:** Random Search involves sampling hyperparameters randomly from a defined space.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Configure a random search space for the same Random Forest model.\n",
    "2. Implement `RandomizedSearchCV` to sample configurations and carry out the search.\n",
    "3. Record the best parameters along with performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c920066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Applying Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1288e",
   "metadata": {},
   "source": [
    "üí° **Tip:** Ensure the random search space is broad and varied.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Plots should clearly show the relationship between actual and predicted values for the best model from Random Search. Legends should correctly identify each data series in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf806e6",
   "metadata": {},
   "source": [
    "## Task 4: Exploring Bayesian Optimization\n",
    "\n",
    "**Context:** Bayesian Optimization iteratively focuses on promising hyperparameter configurations.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Utilize a tool like `optuna` to apply Bayesian Optimization for the Random Forest model's hyperparameters.\n",
    "2. Observe and document how Bayesian Optimization iteratively improves the configurations.\n",
    "3. Record the best parameters along with performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Exploring Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4465dc",
   "metadata": {},
   "source": [
    "üí° **Tip:** Define an objective function for `optuna` to optimize.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Plots should clearly show the relationship between actual and predicted values for the best model from Bayesian Optimization. Legends should correctly identify each data series in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a159b",
   "metadata": {},
   "source": [
    "## Task 5: Comparing Approaches and Performance\n",
    "\n",
    "**Context:** Comparing results helps evaluate the strengths and weaknesses of different hyperparameter tuning methods.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Systematically compare the results from Grid Search, Random Search, and Bayesian Optimization.\n",
    "2. Reflect on key aspects such as speed, accuracy, and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Comparing Approaches and Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9c31b",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use visualizations or statistical summaries to aid comparison.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Plots should clearly illustrate the performance comparison between Grid Search, Random Search, and Bayesian Optimization. Legends should correctly identify the tuning method and corresponding performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab658c",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "\n",
    "- Successfully selected and preprocessed the dataset\n",
    "- Implemented and tuned hyperparameters using Grid Search, Random Search, and Bayesian Optimization\n",
    "- Compared and analyzed results from the three tuning methods\n",
    "- Provided reflections and recommendations based on findings\n",
    "\n",
    "### üîç Common Issues & Solutions\n",
    "\n",
    "**Problem:** Dataset not loading correctly.   \n",
    "**Solution:** Verify the data source and ensure proper loading using pandas.\n",
    "\n",
    "**Problem:** Hyperparameter tuning errors.   \n",
    "**Solution:** Check the parameter grid and search space to ensure compatibility with the chosen model.\n",
    "\n",
    "**Problem:** Bayesian Optimization not providing good results.   \n",
    "**Solution:** Verify the objective function and optimization setup, and try increasing the number of trials.\n",
    "\n",
    "### üîë Key Points\n",
    "\n",
    "- Grid Search exhaustively searches through predefined hyperparameter combinations.\n",
    "- Random Search samples hyperparameter configurations randomly and can be more time-efficient.\n",
    "- Bayesian Optimization iteratively focuses on promising configurations to refine performance.\n",
    "- Comparing different tuning methods helps understand their strengths, weaknesses, and suitable applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7be7ac",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efedc712",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>    \n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "    \n",
    "\n",
    "# Task 1: Load and preprocess dataset\n",
    "iris = load_iris()\n",
    "    \n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "  \n",
    "# Impute missing values (if any) with the mean of each column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Handle feature scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Task 2: Grid Search\n",
    "param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]}\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=5, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_best_params = grid_search.best_params_\n",
    "grid_accuracy = accuracy_score(y_test, grid_search.best_estimator_.predict(X_test))\n",
    "\n",
    "# Task 3: Random Search\n",
    "param_distributions = {'n_estimators': np.arange(50, 200, 50), 'max_depth': [None, 10, 20], 'min_samples_split': np.arange(2, 11)}\n",
    "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=param_distributions, n_iter=10, cv=5, verbose=2, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "random_best_params = random_search.best_params_\n",
    "random_accuracy = accuracy_score(y_test, random_search.best_estimator_.predict(X_test))\n",
    "\n",
    "# Task 4: Bayesian Optimization using Optuna\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 30)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "    score = np.mean(cross_val_score(model, X_train, y_train, cv=3))\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "bayesian_best_params = study.best_params\n",
    "model = RandomForestClassifier(**study.best_params)\n",
    "model.fit(X_train, y_train)\n",
    "bayesian_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "# Task 5:Comparing Approaches and Performance\n",
    "print(f\"Grid Search Best Parameters: {grid_best_params}, Accuracy: {grid_accuracy}\")\n",
    "print(f\"Random Search Best Parameters: {random_best_params}, Accuracy: {random_accuracy}\")\n",
    "print(f\"Bayesian Optimization Best Parameters: {bayesian_best_params}, Accuracy: {bayesian_accuracy}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
