{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569670eb",
   "metadata": {},
   "source": [
    "# üë©‚ÄçüíªSimulating Real World Task Using RL\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you'll train a DQN (Deep Q-Network) agent to trade stocks using real pricing data. This hands-on activity will introduce you to the core elements of training an RL agent‚Äîcreating the trading environment, defining the reward structure, and implementing the DQN algorithm. By the end of this lab, you will have practical experience in applying reinforcement learning to financial data, understanding the challenges and strategies for effective model training and deployment.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- ‚úÖ Set up a stock trading environment for an RL agent\n",
    "- ‚úÖ Implement a DQN agent for learning stock trading strategies\n",
    "- ‚úÖ Train and evaluate the DQN agent using real stock price data\n",
    "- ‚úÖ Analyze agent performance and explore improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f4974",
   "metadata": {},
   "source": [
    "## Task 1: Environment Setup\n",
    "\n",
    "**Context:** Setting up the trading environment is the first step for your RL agent.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a `TradingEnv` class that represents the stock trading environment.\n",
    "2. Define the states (e.g., price, simple moving average), actions (hold, buy, sell), and reward structure (profit calculation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Environment Setup\n",
    "\n",
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73035d9b",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use a grid size of 5x5 for simplicity.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Print the initial state of the environment.\n",
    "\n",
    "**Expected output:** The starting state with initial values for price, position, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0267db29",
   "metadata": {},
   "source": [
    "## Task 2: Implement Replay Buffer\n",
    "\n",
    "**Context:** A replay buffer stores experiences to sample during training, promoting stability.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define a `ReplayBuffer` class to manage storing and sampling of experiences during training.\n",
    "2. Ensure buffer operations such as `push()` and `sample()` are implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Implement Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f588a45",
   "metadata": {},
   "source": [
    "üí° **Tip:** Set a buffer capacity and batch size for efficient training.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Print the contents of the replay buffer after a few steps.\n",
    "\n",
    "**Expected output:** A list of stored experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3d713a",
   "metadata": {},
   "source": [
    "## Task 3: Implement DQN Network\n",
    "\n",
    "**Context:** The DQN network approximates the Q-values for state-action pairs.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define a `DQN` class using `torch.nn` to create a neural network.\n",
    "2. Implement the forward pass that returns Q-values for given states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f6472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Implement DQN Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa67d0f",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `torch.nn.Linear` for defining fully connected layers, and `torch.optim.Adam` for optimization.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Print the network architecture and output for a sample input.\n",
    "\n",
    "**Expected output:** Q-values for the input state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3429d54",
   "metadata": {},
   "source": [
    "## Task 4: Train the DQN Agent\n",
    "\n",
    "**Context:** Training the DQN agent involves simulating episodes and updating Q-values.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Implement the training loop to simulate episodes, perform action selection, and update the Q-values.\n",
    "2. Use epsilon-greedy action selection, experience replay, and periodic target network updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38610cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Train the DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39dc6b",
   "metadata": {},
   "source": [
    "üí° **Tip:** Track total rewards and portfolio values over episodes for evaluation.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Print the total rewards and epsilon values for each episode.\n",
    "\n",
    "**Expected output:** Training progress with rewards and decay in exploration rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b616c",
   "metadata": {},
   "source": [
    "## Task 5: Analyze and Visualize Results\n",
    "\n",
    "**Context:** Analyzing performance helps evaluate how well the agent learned the trading strategy.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Plot the portfolio values over episodes to visualize performance trends.\n",
    "\n",
    "2. Evaluate the effectiveness of the trading strategy learned by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2bf991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Analyze and Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be67b6f",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `matplotlib` for plotting portfolio value trends over time.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Display a plot of the portfolio values over episodes.\n",
    "\n",
    "**Expected output:** A clear visual representation of the agent's portfolio value growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3e139",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "\n",
    "- Successfully set up the TradingEnv class with states, actions, and rewards\n",
    "- Implemented and tested the ReplayBuffer class for storing experiences\n",
    "- Defined and tested the DQN neural network for Q-value approximation\n",
    "- Trained the DQN agent through simulated episodes\n",
    "- Analyzed and visualized the agent's performance with real stock price data\n",
    "\n",
    "### üîç Common Issues & Solutions\n",
    "\n",
    "**Problem:** Incorrect state or action updates in the environment.   \n",
    "**Solution:** Verify the logic in the `step()` function and ensure states and rewards are correctly updated.  \n",
    "\n",
    "**Problem:** Replay buffer errors.   \n",
    "**Solution:** Check buffer capacity and ensure experiences are added and sampled correctly.  \n",
    "\n",
    "**Problem:** DQN network not learning.   \n",
    "**Solution:**  Adjust learning parameters (e.g., learning rate, gamma) and verify the training loop implementation.\n",
    "\n",
    "### üîë Key Points\n",
    "\n",
    "- Setting up the trading environment correctly is crucial for effective RL training.\n",
    "- Replay buffers help stabilize training through experience replay.\n",
    "- DQN agents use neural networks to approximate Q-values and learn optimal strategies.\n",
    "- Analyzing training performance helps refine and improve agent strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f71cc7d",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610374a2",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>    \n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Synthetic price generator (simple geometric random walk)\n",
    "def generate_random_walk(length=252,            # ~1 trading year\n",
    "                         start_price=100.0,\n",
    "                         mu=0.0005,             # daily drift (‚âà12% annualised)\n",
    "                         sigma=0.01,            # daily volatility (‚âà16% annualised)\n",
    "                         seed=None):\n",
    "    \"\"\"\n",
    "    Generate `length` synthetic prices using a geometric random walk.\n",
    "\n",
    "    Price_{t+1} = Price_t * exp( (mu - 0.5*sigma^2) + sigma * Z_t )\n",
    "    where Z_t ~ N(0, 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray[float32]  shape=(length,)\n",
    "        Simulated close prices.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Pre-draw standard normals\n",
    "    z = np.random.normal(size=length - 1)\n",
    "    log_returns = (mu - 0.5 * sigma**2) + sigma * z\n",
    "    prices = np.empty(length, dtype=np.float32)\n",
    "    prices[0] = start_price\n",
    "    prices[1:] = start_price * np.exp(np.cumsum(log_returns))\n",
    "    return prices\n",
    "\n",
    "\n",
    "# Environment\n",
    "class TradingEnv:\n",
    "    def __init__(self, prices):\n",
    "        self.prices = prices\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.position = 0  # 0 = no position, 1 = long\n",
    "        self.entry_price = 0.0\n",
    "        self.cash = 0.0\n",
    "        self.portfolio_value = []\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        price = float(self.prices[self.t])\n",
    "        sma = float(np.mean(self.prices[max(0, self.t - 5):self.t + 1]))\n",
    "        return np.array([price, sma, float(self.position)], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Guard: episode done\n",
    "        if self.t >= len(self.prices) - 1:\n",
    "            return np.zeros(3, dtype=np.float32), 0.0, True\n",
    "\n",
    "        price = self.prices[self.t]\n",
    "        reward = 0.0\n",
    "\n",
    "        # Actions: 0 = hold, 1 = buy, 2 = sell\n",
    "        if action == 1 and self.position == 0:  # Buy\n",
    "            self.position = 1\n",
    "            self.entry_price = price\n",
    "        elif action == 2 and self.position == 1:  # Sell\n",
    "            reward = price - self.entry_price\n",
    "            self.position = 0\n",
    "            self.cash += reward\n",
    "\n",
    "        # Track portfolio value (cash + current position value)\n",
    "        current_pos_value = price - self.entry_price if self.position == 1 else 0.0\n",
    "        total_value = self.cash + current_pos_value\n",
    "        self.portfolio_value.append(total_value)\n",
    "\n",
    "        self.t += 1\n",
    "        done = self.t >= len(self.prices) - 1\n",
    "\n",
    "        next_state = np.zeros(3, dtype=np.float32) if done else self._get_state()\n",
    "        return next_state, float(reward), done\n",
    "\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=1000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# DQN Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_dqn(prices, episodes=100, gamma=0.99,\n",
    "              epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=0.995,\n",
    "              batch_size=32):\n",
    "\n",
    "    env = TradingEnv(prices)\n",
    "    input_dim = 3\n",
    "    output_dim = 3  # hold, buy, sell\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net = DQN(input_dim, output_dim).to(device)\n",
    "    target_net = DQN(input_dim, output_dim).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "    replay_buffer = ReplayBuffer()\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    portfolio_values_over_episodes = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "\n",
    "        while True:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, output_dim - 1)\n",
    "            else:\n",
    "                state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = policy_net(state_t)\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Training step\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                states = torch.tensor(np.vstack(states), dtype=torch.float32).to(device)\n",
    "                actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(device)\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                next_states = torch.tensor(np.vstack(next_states), dtype=torch.float32).to(device)\n",
    "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "                q_values = policy_net(states).gather(1, actions)\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "                    target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        # Update target network every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # Store portfolio value history for this episode\n",
    "        portfolio_values_over_episodes.append(env.portfolio_value.copy())\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{episodes} ‚îÇ Total Reward: {total_reward:.2f} ‚îÇ Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    return portfolio_values_over_episodes\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate one year (252 trading days) of prices\n",
    "    prices = generate_random_walk(length=252, start_price=100.0, seed=42)\n",
    "\n",
    "    # Train the DQN agent\n",
    "    portfolio_values = train_dqn(prices, episodes=50)\n",
    "\n",
    "    # Optional: quick visual check\n",
    "    plt.plot(prices)\n",
    "    plt.title(\"Synthetic Price Series (Random Walk)\")\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
