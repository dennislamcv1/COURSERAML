{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aac9b11",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Train Your First Q-Learning and REINFORCE Agents\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you'll roll up your sleeves and dive into the world of reinforcement learning by training two types of agents: one using Q-Learning and the other using the REINFORCE algorithm. These two approaches represent the powerful methodologies of value-based and policy-based learning, respectively. Through this practical activity, you'll witness firsthand how each method fosters agent learning and decision-making in a simple environment. By comparing the two, you'll sharpen your understanding of their unique strengths and applications‚Äîequipping you with the skills to choose the right approach for diverse RL scenarios.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- ‚úÖ Set up and simulate an RL environment like GridWorld\n",
    "- ‚úÖ Implement and train a Q-Learning agent\n",
    "- ‚úÖ Implement and train a REINFORCE agent\n",
    "- ‚úÖ Compare and analyze the performance of value-based and policy-based RL techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67ef41",
   "metadata": {},
   "source": [
    "## Task 1: Environment Setup\n",
    "\n",
    "**Context:** Setting up the GridWorld environment is the first step for your RL agents.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a simple GridWorld-like environment for your agents to explore.\n",
    "2. Define basic states, actions, and a reward structure with a goal for agents to reach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2317b7eb",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use a grid size of 5x5 for simplicity.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Print the initial state of the environment.\n",
    "\n",
    "**Expected output:** The starting position of the agent on the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c7488",
   "metadata": {},
   "source": [
    "## Task 2: Implement Q-Learning Agent\n",
    "\n",
    "**Context:** Q-Learning is a value-based method where the agent learns the optimal actions through experience.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Set up a Q-table and implement Q-Learning, allowing the agent to learn the optimal actions through experience-driven updates.\n",
    "2. Initialize exploration parameters, implement the learning loop, and run multiple episodes to refine the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678070aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Implement Q-Learning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74e518",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use parameters like `alpha`, `gamma`, and `epsilon` for learning rate, discount factor, and exploration rate respectively.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Print the Q-table after training.\n",
    "\n",
    "**Expected output:** The Q-table with learned values for each state-action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2153c6ae",
   "metadata": {},
   "source": [
    "## Task 3: Implement REINFORCE Agent\n",
    "\n",
    "**Context:** REINFORCE is a policy-based method where the agent learns policy updates based on episodic reward trajectories.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Construct a policy network to approximate your agent‚Äôs policy.\n",
    "2. Implement the REINFORCE algorithm, which calculates policy updates based on episodic reward trajectories.\n",
    "3. Train the policy network over several episodes to learn effective sequences of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a19a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Implement REINFORCE Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b03538",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `torch.nn` for implementing the policy network.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Print the total rewards and steps taken during the training process.\n",
    "\n",
    "**Expected output:** The performance metrics for each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9eaae",
   "metadata": {},
   "source": [
    "## Task 4: Comparative Analysis\n",
    "\n",
    "**Context:** Comparing the results of Q-Learning and REINFORCE agents helps evaluate their strengths and weaknesses.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Run a set number of episodes for both agents and record their performance, noting the cumulative rewards achieved and the consistency of reaching the goal state.\n",
    "2. Compare the learning processes and outcomes of the Q-Learning and REINFORCE agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ad4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f1196",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use visualizations or statistical summaries to aid comparison.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Compare the performance metrics of both agents and discuss their strategies, speed of learning, and adaptability.\n",
    "\n",
    "**Expected output:** A detailed comparative analysis document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e599e",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "\n",
    "- Successfully set up the GridWorld environment with defined states and rewards\n",
    "- Implemented and trained a Q-Learning agent\n",
    "- Implemented and trained a REINFORCE agent\n",
    "- Compared the performance of both agents\n",
    "- Provided reflections and recommendations based on findings\n",
    "\n",
    "### üîç Common Issues & Solutions\n",
    "\n",
    "**Problem:** Agent actions not updating the state correctly.   \n",
    "**Solution:** Ensure the actions are correctly defined and update the state as intended.\n",
    "\n",
    "**Problem:** Rewards not being calculated correctly.   \n",
    "**Solution:** Verify the reward logic and ensure it's applied correctly for each action.\n",
    "\n",
    "**Problem:** Agent not learning effectively.   \n",
    "**Solution:** Adjust the learning parameters and exploration rate for better training.\n",
    "\n",
    "### üîë Key Points\n",
    "\n",
    "- Q-Learning is a value-based method that updates state-action values to learn the optimal policy.\n",
    "- REINFORCE is a policy-based method that directly optimizes the policy based on rewards received.\n",
    "- Comparing different RL approaches helps in understanding their strengths, weaknesses, and suitable applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d96cf16",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38916f",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see a exemplar solution</strong></summary>    \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Environment Setup: GridWorld\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.state = (0, 0)\n",
    "        self.goal = (4, 4)\n",
    "        self.size = 5\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == \"right\" and x < self.size - 1:\n",
    "            x += 1\n",
    "        elif action == \"down\" and y < self.size - 1:\n",
    "            y += 1\n",
    "        self.state = (x, y)\n",
    "        reward = 1 if self.state == self.goal else -0.04\n",
    "        return self.state, reward\n",
    "\n",
    "# Q-Learning implementation\n",
    "def train_q_learning(env, num_episodes=1000):\n",
    "    q_table = np.zeros((env.size * env.size, 2))\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    epsilon = 0.1\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state_index = state[0] * env.size + state[1]\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action_index = np.random.choice(2)\n",
    "            else:\n",
    "                action_index = np.argmax(q_table[state_index])\n",
    "\n",
    "            action = [\"right\", \"down\"][action_index]\n",
    "            next_state, reward = env.step(action)\n",
    "            next_state_index = next_state[0] * env.size + next_state[1]\n",
    "            q_table[state_index, action_index] += alpha * (reward + gamma * np.max(q_table[next_state_index]) - q_table[state_index, action_index])\n",
    "            state, state_index = next_state, next_state_index\n",
    "            if state == env.goal:\n",
    "                done = True\n",
    "    return q_table\n",
    "\n",
    "# REINFORCE implementation\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "def select_action(policy_net, state, env):\n",
    "    # Create one-hot encoded state\n",
    "    state_tensor = torch.zeros(env.size * env.size)\n",
    "    state_index = state[0] * env.size + state[1]\n",
    "    state_tensor[state_index] = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = policy_net(state_tensor)\n",
    "    action = np.random.choice(2, p=probs.numpy())\n",
    "    return action\n",
    "\n",
    "def train_reinforce(env, num_episodes=1000):\n",
    "    policy_net = PolicyNetwork(num_inputs=env.size * env.size, num_actions=2)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Create one-hot encoded state\n",
    "            state_tensor = torch.zeros(env.size * env.size)\n",
    "            state_index = state[0] * env.size + state[1]\n",
    "            state_tensor[state_index] = 1\n",
    "\n",
    "            action = select_action(policy_net, state, env)\n",
    "            probs = policy_net(state_tensor)\n",
    "            log_prob = torch.log(probs[action])\n",
    "\n",
    "            next_state, reward = env.step([\"right\", \"down\"][action])\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            state = next_state\n",
    "            if state == env.goal:\n",
    "                done = True\n",
    "\n",
    "        # Calculate cumulative reward\n",
    "        returns = sum(rewards)\n",
    "\n",
    "        # Update policy network\n",
    "        policy_loss = []\n",
    "        for log_prob in log_probs:\n",
    "            policy_loss.append(-log_prob * returns)\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {returns}\")\n",
    "\n",
    "# Running the agents and comparing results\n",
    "env = GridWorld()\n",
    "print(\"Training Q-Learning agent...\")\n",
    "q_table = train_q_learning(env)\n",
    "print(\"\\nTraining REINFORCE agent...\")\n",
    "train_reinforce(env)\n",
    "\n",
    "print(\"\\nQ-Learning Q-Table:\")\n",
    "print(q_table)\n",
    "```                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef857404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Environment Setup: GridWorld\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.state = (0, 0)\n",
    "        self.goal = (4, 4)\n",
    "        self.size = 5\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == \"right\" and x < self.size - 1:\n",
    "            x += 1\n",
    "        elif action == \"down\" and y < self.size - 1:\n",
    "            y += 1\n",
    "        self.state = (x, y)\n",
    "        reward = 1 if self.state == self.goal else -0.04\n",
    "        return self.state, reward\n",
    "\n",
    "# Q-Learning implementation\n",
    "def train_q_learning(env, num_episodes=1000):\n",
    "    q_table = np.zeros((env.size * env.size, 2))\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    epsilon = 0.1\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state_index = state[0] * env.size + state[1]\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action_index = np.random.choice(2)\n",
    "            else:\n",
    "                action_index = np.argmax(q_table[state_index])\n",
    "\n",
    "            action = [\"right\", \"down\"][action_index]\n",
    "            next_state, reward = env.step(action)\n",
    "            next_state_index = next_state[0] * env.size + next_state[1]\n",
    "            q_table[state_index, action_index] += alpha * (reward + gamma * np.max(q_table[next_state_index]) - q_table[state_index, action_index])\n",
    "            state, state_index = next_state, next_state_index\n",
    "            if state == env.goal:\n",
    "                done = True\n",
    "    return q_table\n",
    "\n",
    "# REINFORCE implementation\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "def select_action(policy_net, state, env):\n",
    "    # Create one-hot encoded state\n",
    "    state_tensor = torch.zeros(env.size * env.size)\n",
    "    state_index = state[0] * env.size + state[1]\n",
    "    state_tensor[state_index] = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = policy_net(state_tensor)\n",
    "    action = np.random.choice(2, p=probs.numpy())\n",
    "    return action\n",
    "\n",
    "def train_reinforce(env, num_episodes=1000):\n",
    "    policy_net = PolicyNetwork(num_inputs=env.size * env.size, num_actions=2)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Create one-hot encoded state\n",
    "            state_tensor = torch.zeros(env.size * env.size)\n",
    "            state_index = state[0] * env.size + state[1]\n",
    "            state_tensor[state_index] = 1\n",
    "\n",
    "            action = select_action(policy_net, state, env)\n",
    "            probs = policy_net(state_tensor)\n",
    "            log_prob = torch.log(probs[action])\n",
    "\n",
    "            next_state, reward = env.step([\"right\", \"down\"][action])\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            state = next_state\n",
    "            if state == env.goal:\n",
    "                done = True\n",
    "\n",
    "        # Calculate cumulative reward\n",
    "        returns = sum(rewards)\n",
    "\n",
    "        # Update policy network\n",
    "        policy_loss = []\n",
    "        for log_prob in log_probs:\n",
    "            policy_loss.append(-log_prob * returns)\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {returns}\")\n",
    "\n",
    "# Running the agents and comparing results\n",
    "env = GridWorld()\n",
    "print(\"Training Q-Learning agent...\")\n",
    "q_table = train_q_learning(env)\n",
    "print(\"\\nTraining REINFORCE agent...\")\n",
    "train_reinforce(env)\n",
    "\n",
    "print(\"\\nQ-Learning Q-Table:\")\n",
    "print(q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
