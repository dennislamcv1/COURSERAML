{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12930476",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Clean Your First NLP Dataset: News Headlines Edition\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "In this activity, you will embark on a hands-on journey to preprocess a dataset of news headlines, converting raw text into a cleaned and structured form ready for analysis. Just like turning raw sugarcane into refined sugar, you‚Äôll take gritty, noisy text and transform it into meaningful tokens, each ready to power insightful natural language processing models.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- ‚úÖ Clean and preprocess raw text data by removing noise and standardizing text\n",
    "- ‚úÖ Tokenize text into individual words for analysis\n",
    "- ‚úÖ Remove stopwords to focus on significant content\n",
    "- ‚úÖ Apply stemming or lemmatization to reduce words to their base forms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b662792",
   "metadata": {},
   "source": [
    "## Task 1: Data Loading and Exploration\n",
    "\n",
    "**Context:** Properly loading and exploring the dataset helps in understanding the types of noise present in the text.\n",
    "\n",
    "**Steps:**\n",
    "1. A variable, `news_headlines`, has been created for you to use for this lab.\n",
    "2. Explore its structure to understand the various types of noise present in the data, such as punctuation, URLs, emojis, and stopwords.\n",
    "3. Inspect the first few entries to ascertain common patterns and elements to address in cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c031b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Loading and Exploration\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample dataset of news headlines\n",
    "news_headlines = [\n",
    "    \"Breaking news: Market hits new highs! See details at https://marketnews.com.\",\n",
    "    \"Bitcoin hits 50k! Is it the new gold? ü§î #cryptocurrency\",\n",
    "    \"Experts debate climate impact at global summit on environmental change.\",\n",
    "    \"COVID-19 updates: New variants and vaccination efforts continue.\"\n",
    "]\n",
    "\n",
    "# Explore / Inspect Data\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51839dcb",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `print()` and basic string operations to explore the dataset.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Display the first 5 news headlines with their raw text.\n",
    "\n",
    "**Expected output:** The original raw text headlines showing various types of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5363f9",
   "metadata": {},
   "source": [
    "## Task 2: Cleaning Text Data\n",
    "\n",
    "**Context:** Cleaning raw text data by removing noise is crucial for accurate NLP analysis.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Develop functions to clean the text data:\n",
    "2. Remove any HTML tags, URLs, and unnecessary symbols.\n",
    "3. Handle special characters and numbers, deciding which elements should be retained or discarded.\n",
    "4. Transform all text to a uniform case to standardize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Cleaning Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c114e",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use regular expressions (`re`) for cleaning operations.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "- Print a cleaned version of the first 5 news headlines.\n",
    "\n",
    "**Expected output:** The cleaned text without noise, standardized to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef375ef4",
   "metadata": {},
   "source": [
    "## Task 3: Tokenization\n",
    "\n",
    "**Context:** Tokenizing the cleaned text into individual words enables further analysis steps.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Tokenize the cleaned text into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc40f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2977d1",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `word_tokenize` from `nltk` for tokenization.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Print the tokenized version of the first 5 news headlines.\n",
    "\n",
    "**Expected output:** Lists of tokenized words for each headline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7ed99",
   "metadata": {},
   "source": [
    "## Task 4: Stopword Removal\n",
    "\n",
    "**Context:** Removing stopwords helps focus on the content carrying the most significant insights.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Remove common stopwords from your tokens.\n",
    "2. Reflect on whether additional, context-specific stopwords would enhance your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e9193",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `stopwords.words('english')` from `nltk` to get the list of stopwords.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Print the tokenized version of the first 5 news headlines after stopword removal.\n",
    "\n",
    "**Expected output:** Lists of tokenized words without common stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f07e6e5",
   "metadata": {},
   "source": [
    "## Task 5: Apply Stemming or Lemmatization\n",
    "\n",
    "**Context:** Stemming or lemmatization reduces tokens to their base forms, which is useful for various NLP tasks.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Implement stemming or lemmatization to reduce the tokens to their base forms.\n",
    "2. Decide which method is more appropriate depending on your subsequent analytical tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85837b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Apply Stemming or Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96adf15",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `WordNetLemmatizer` from `nltk` for lemmatization.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Print the tokenized and lemmatized version of the first 5 news headlines.\n",
    "\n",
    "**Expected output:** Lists of lemmatized words for each headline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b63ef09",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "\n",
    "- Successfully loaded and explored the dataset\n",
    "- Cleaned the text data by removing noise and standardizing text\n",
    "- Tokenized the cleaned text into individual words\n",
    "- Removed stopwords to focus on significant content\n",
    "- Applied stemming or lemmatization to reduce words to their base forms\n",
    "\n",
    "### üîç Common Issues & Solutions\n",
    "\n",
    "**Problem:** Text data not cleaning properly.\n",
    "\n",
    "**Solution:** Ensure regular expressions are correctly specified for cleaning.\n",
    "\n",
    "**Problem:** Tokenization errors.\n",
    "\n",
    "**Solution:** Verify that `nltk` resources are correctly downloaded and used. \n",
    "\n",
    "### üîë Key Points\n",
    "\n",
    "- Cleaning and preprocessing text data is crucial for accurate NLP analysis.\n",
    "- Tokenization, stopword removal, and lemmatization help transform raw text into analyzable tokens.\n",
    "- Proper preprocessing ensures that the data is ready for further NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4fdcae",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612e719",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>    \n",
    "\n",
    "```python\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample dataset of news headlines\n",
    "news_headlines = [\n",
    "    \"Breaking news: Market hits new highs! See details at https://marketnews.com.\",\n",
    "    \"Bitcoin hits 50k! Is it the new gold? ü§î #cryptocurrency\",\n",
    "    \"Experts debate climate impact at global summit on environmental change.\",\n",
    "    \"COVID-19 updates: New variants and vaccination efforts continue.\"\n",
    "]\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def preprocess_headlines(headlines):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_data = []\n",
    "    for headline in headlines:\n",
    "        cleaned = clean_text(headline)\n",
    "        tokens = word_tokenize(cleaned)\n",
    "        filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        cleaned_data.append(filtered_tokens)\n",
    "    return cleaned_data\n",
    "\n",
    "# Preprocess the headlines\n",
    "cleaned_headlines = preprocess_headlines(news_headlines)\n",
    "print(cleaned_headlines)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
