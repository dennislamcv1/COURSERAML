{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e6e3c5",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Compare Static vs. Contextual Embeddings for Sentence Similarity\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you'll embark on a hands-on exploration of how different embedding techniques affect the analysis of sentence similarity. By comparing static embeddings (like Word2Vec and GloVe) with advanced contextual embeddings (such as BERT), you'll gain insights into their strengths, weaknesses, and the contextual richness they offer. This exercise will help you appreciate the nuances of each technique and when to apply them effectively in natural language processing tasks.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- ‚úÖ Implement and compare static embeddings (Word2Vec, GloVe) and contextual embeddings (BERT)\n",
    "- ‚úÖ Use cosine similarity to evaluate sentence similarity\n",
    "- ‚úÖ Analyze how different embedding techniques capture nuances in language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d92f65",
   "metadata": {},
   "source": [
    "## Task 1: Data Preparation\n",
    "\n",
    "**Context:** Proper data preparation ensures the sentences are clean and ready for embedding.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Sample data has been provided in the starter code for this lab.\n",
    "2. Preprocess your text data by ensuring all sentences are clean and ready for embedding, removing any irrelevant characters or symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Data Preparation\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import en_core_web_sm\n",
    "\n",
    "# Load spaCy model with word vectors\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"The sky is clear and sunny.\",\n",
    "             \"It is a bright, sunny day.\",\n",
    "             \"The gloomy weather includes clouds and rain.\"]\n",
    "\n",
    "# Prepare data \n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5bb09c",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use basic string operations to clean and standardize the text.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Print the preprocessed version of the sentences.\n",
    "\n",
    "**Expected output:** Cleaned and standardized text ready for embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3cd30",
   "metadata": {},
   "source": [
    "## Task 2: Implementing Static Embeddings\n",
    "\n",
    "**Context:** Static embeddings like Word2Vec and GloVe convert words into continuous vector forms that don't change with context.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Utilize a pre-trained static embedding model such as Word2Vec or GloVe to convert your sentences into vector forms.\n",
    "2. Compute sentence vectors by averaging individual word vectors within the sentence.\n",
    "3. Use cosine similarity to evaluate and compare the similarity between pairs of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96645c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Implementing Static Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e344d5",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `spacy to` leverage pre-trained word vectors.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Print the cosine similarity scores for the sentence pairs.\n",
    "\n",
    "**Expected output:** Similarity scores indicating how similar each pair of sentences is based on static embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9188632",
   "metadata": {},
   "source": [
    "## Task 3: Exploring Contextual Embeddings\n",
    "\n",
    "**Context:** Contextual embeddings like BERT generate dynamic vector representations that change depending on the context within the sentence.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Use a pre-trained model like BERT from Hugging Face Transformers to generate contextual embeddings for the same set of sentences.\n",
    "2. Compute cosine similarity for the sentence pairs, paying special attention to how context might alter perceived similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf32a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Exploring Contextual Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c080a",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `AutoModel` and `AutoTokenizer` from the `transformers library`.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Print the cosine similarity scores using BERT embeddings for the sentence pairs.\n",
    "\n",
    "**Expected output:** Similarity scores indicating how similar each pair of sentences is based on contextual embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009fc08",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "\n",
    "- Successfully gathered and preprocessed the set of sentences\n",
    "- Implemented static embeddings and computed sentence similarities\n",
    "- Utilized contextual embeddings and evaluated sentence similarities\n",
    "- Compared and reflected on the results of static vs. contextual embeddings\n",
    "\n",
    "### üîç Common Issues & Solutions\n",
    "\n",
    "**Problem:** Sentence vectors not computing correctly.   \n",
    "**Solution:** Ensure the pre-trained model and tokenizer are correctly specified and used.\n",
    "\n",
    "**Problem:** Similarity scores not making sense.   \n",
    "**Solution:** Verify that sentence vectors are correctly normalized and cosine similarity is properly computed.\n",
    "\n",
    "**Problem:** Differences in embeddings not noticeable.   \n",
    "**Solution:** Ensure the dataset has enough variety to showcase the advantages of contextual embeddings.\n",
    "\n",
    "### üîë Key Points\n",
    "\n",
    "- Static embeddings capture word-level meanings but lack context sensitivity.\n",
    "- Contextual embeddings provide dynamic representations that adapt to sentence context.\n",
    "- Reflecting on the choice of embeddings helps determine the best approach for various NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d7167c",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb55c5",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>    \n",
    "\n",
    "```python\n",
    "# Task 1: Data Preparation\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import en_core_web_sm\n",
    "\n",
    "# Load spaCy model with word vectors\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"The sky is clear and sunny.\",\n",
    "             \"It is a bright, sunny day.\",\n",
    "             \"The gloomy weather includes clouds and rain.\"]\n",
    "\n",
    "def get_sentence_vector(sentence, model):\n",
    "    # Process the sentence and get its vector\n",
    "    doc = model(sentence)\n",
    "    return doc.vector\n",
    "\n",
    "def get_sentence_similarity_matrix(sentences, model):\n",
    "    # Get vectors for all sentences\n",
    "    vectors = np.vstack([get_sentence_vector(sentence, model) for sentence in sentences])\n",
    "    return cosine_similarity(vectors)\n",
    "\n",
    "# Calculate similarities using spaCy's word vectors\n",
    "similarities = get_sentence_similarity_matrix(sentences, nlp)\n",
    "\n",
    "print(\"Sentence Similarities using spaCy (Static Embeddings):\\n\")\n",
    "for i, sent1 in enumerate(sentences):\n",
    "    for j, sent2 in enumerate(sentences):\n",
    "        print(f\"Similarity between:\\n'{sent1}' and\\n'{sent2}': {similarities[i][j]:.4f}\\n\")\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_sentence_vector(sentence, tokenizer, model):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    sentence_embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return sentence_embedding\n",
    "\n",
    "def get_bert_similarity_matrix(sentences, tokenizer, model):\n",
    "    vectors = np.vstack([get_bert_sentence_vector(sentence, tokenizer, model) for sentence in sentences])\n",
    "    return cosine_similarity(vectors)\n",
    "\n",
    "# Calculate similarities using BERT\n",
    "bert_similarities = get_bert_similarity_matrix(sentences, tokenizer, model)\n",
    "\n",
    "print(\"\\nSentence Similarities using BERT (Contextual Embeddings):\\n\")\n",
    "for i, sent1 in enumerate(sentences):\n",
    "    for j, sent2 in enumerate(sentences):\n",
    "        print(f\"Similarity between:\\n'{sent1}' and\\n'{sent2}': {bert_similarities[i][j]:.4f}\\n\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
