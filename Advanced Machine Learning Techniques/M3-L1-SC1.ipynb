{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Step 1: Setting up the Environment\n","Let's start with importing all essential libraries and prepare the dataset."],"metadata":{"id":"Y6ANy3EI6KJe"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iRoFvGW-6BUb","executionInfo":{"status":"ok","timestamp":1746698508521,"user_tz":240,"elapsed":7281,"user":{"displayName":"Jeremy Samuelson","userId":"13388272830865650692"}},"outputId":"79c58d99-4ff7-49b2-af4b-c56ceb42022e"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["import re\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import nltk\n","\n","nltk.download('punkt_tab')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Sample raw text\n","raw_text = \"Hello, NLP enthusiast! ðŸ˜Š This is a sample text with <b>HTML tags</b>, numbers 1234, and some #hashtags. Visit us at http://example.com.\""]},{"cell_type":"markdown","source":["# Step 2: Removing HTML Tags & URLs\n","Next, let's remove HTML tags and URLs from the raw text."],"metadata":{"id":"ri326kqb6XDZ"}},{"cell_type":"code","source":["# Remove HTML tags\n","cleaned_text = re.sub(r'<.*?>', '', raw_text)\n","\n","# Remove URLs\n","cleaned_text = re.sub(r'http\\S+', '', cleaned_text)"],"metadata":{"id":"jfGVepgS6UjP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 3: Handling Emoticons, Symbols, and Numbers\n","Now we'll remove emoticons, symbols, and numbers from the text.\n"],"metadata":{"id":"VjIMyYlb6uaJ"}},{"cell_type":"code","source":["# Remove emoticons and symbols\n","cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n","\n","# Remove numbers\n","cleaned_text = re.sub(r'\\d+', '', cleaned_text)"],"metadata":{"id":"xJo-xXtG6pM3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 4: Tokenizing the Text\n","The next step is to Tokenize the cleaned text."],"metadata":{"id":"fVWIff8x624g"}},{"cell_type":"code","source":["# Tokenize the text\n","tokens = word_tokenize(cleaned_text)"],"metadata":{"id":"JzflkESC61dT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 5: Removing Stopwords\n","With the tokenization complete, we'll next remove stopwords."],"metadata":{"id":"G95Vkeni69kG"}},{"cell_type":"code","source":["# Remove stopwords\n","filtered_tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n","\n","# Display the cleaned tokens\n","print(filtered_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bB95WuPV68uu","executionInfo":{"status":"ok","timestamp":1746698553822,"user_tz":240,"elapsed":14,"user":{"displayName":"Jeremy Samuelson","userId":"13388272830865650692"}},"outputId":"6e0cb13e-cd3d-45be-e8be-7b710191f90c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', 'NLP', 'enthusiast', 'sample', 'text', 'HTML', 'tags', 'numbers', 'hashtags', 'Visit', 'us']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9eYATIjB7G4L"},"execution_count":null,"outputs":[]}]}