{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc9b3ac",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Comparing Sparse and Dense Text Representations in Practice\n",
    "\n",
    "## üìã Overview\n",
    "This activity invites you to delve into the realm of sparse and dense text representations, providing hands-on experience in comparing their effectiveness in various NLP tasks. Through this exploration, you'll understand how these representations impact machine learning model performance and discover the nuanced ways in which each method addresses language complexity.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- ‚úÖ Implement and compare sparse text representations (Bag-of-Words, TF-IDF)\n",
    "- ‚úÖ Evaluate dense text representations using pre-trained embeddings\n",
    "- ‚úÖ Use text representations in classification tasks and analyze their impact on model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90480d73",
   "metadata": {},
   "source": [
    "## Task 1: Data Preparation\n",
    "\n",
    "**Context:** Proper data preparation is essential for implementing text representations.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Starter code has been provided, including text data to use for this lab and has been loaded into a DataFrame.\n",
    "2. Preprocess the text data by implementing tokenization, stopword removal, and lemmatization or stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6561f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Data Preparation\n",
    "# Required Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "print(\"Downloading NLTK resources...\")\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    nltk.download('punkt_tab')\n",
    "    print(\"NLTK resources downloaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n",
    "    print(\"Please ensure you have an internet connection or try running nltk.download() manually.\")\n",
    "\n",
    "# Define the data\n",
    "texts = [\n",
    "    \"This movie is fantastic and I love it.\",\n",
    "    \"The acting was great but the plot was weak.\",\n",
    "    \"A terrible film, completely boring.\",\n",
    "    \"I hated every minute of this production.\",\n",
    "    \"Wonderful cinematography and compelling story.\",\n",
    "    \"Bad script and poor direction.\",\n",
    "    \"An absolute masterpiece, highly recommend.\",\n",
    "    \"Utterly disappointing experience.\",\n",
    "    \"Enjoyed it thoroughly, a real gem.\",\n",
    "    \"Skip this one, it's a waste of time.\",\n",
    "    \"The food was delicious and the service was excellent.\",\n",
    "    \"Didn't like the atmosphere, too noisy.\",\n",
    "    \"A truly unique and enjoyable experience.\",\n",
    "    \"The product broke after only a week.\",\n",
    "    \"Highly satisfied with my purchase.\",\n",
    "    \"The customer support was unhelpful.\",\n",
    "    \"What a fantastic performance!\",\n",
    "    \"Quite boring, nothing special.\",\n",
    "    \"Loved the vibrant colors and design.\",\n",
    "    \"The instructions were unclear and confusing.\",\n",
    "    \"Would definitely visit again.\",\n",
    "    \"Overpriced for what you get.\",\n",
    "    \"A pleasant surprise, much better than expected.\",\n",
    "    \"Poor quality materials used.\",\n",
    "    \"Seamless transaction and fast delivery.\",\n",
    "    \"The ending was very predictable.\",\n",
    "    \"Outstanding craftsmanship.\",\n",
    "    \"Left feeling disappointed.\",\n",
    "    \"So happy with the results!\",\n",
    "    \"Could have been much better.\"\n",
    "]\n",
    "# Simple sentiment labels (1: positive, 0: negative/neutral)\n",
    "labels = [\n",
    "    1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
    "    1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
    "    1, 0, 1, 0, 1, 0, 1, 0, 1, 0\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'text': texts, 'label': labels})\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(f\"\\nTotal number of examples: {len(df)}\")\n",
    "\n",
    "# Prepare data\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f08ea1",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `nltk` for tokenization, stopword removal, and lemmatization.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Print the preprocessed version of the first 5 text entries.\n",
    "\n",
    "**Expected output:** Cleaned and standardized text ready for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91daa21",
   "metadata": {},
   "source": [
    "## Task 2: Implementing Sparse Representations\n",
    "\n",
    "**Context:** Sparse representations like Bag-of-Words (BoW) and TF-IDF convert text data into numerical form for analysis.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Use the Bag-of-Words (BoW) approach to convert your text data into a sparse representation.\n",
    "2. Visualize this representation to understand how text is transformed into numerical data.\n",
    "3. Apply the TF-IDF technique to enhance the BoW representation.\n",
    "4. Analyze the changes and note any patterns in term weighting and importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2731f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Implementing Sparse Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d3597",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `CountVectorizer` for BoW and `TfidfVectorizer` for TF-IDF.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Print the shape and vocabulary size of the BoW and TF-IDF matrix.\n",
    "\n",
    "**Expected output:** Information about the transformed data including matrix shapes and vocabulary sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d4309",
   "metadata": {},
   "source": [
    "## Task 3: Exploring Dense Representations\n",
    "\n",
    "**Context:** Dense representations like embeddings capture context and semantics in text data.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Adopt a pre-trained embedding model from Hugging Face or similar repository to convert your text into dense vector representations.\n",
    "2. Focus on how context and semantics are captured in these embeddings.\n",
    "3. Use visualization tools like t-SNE or PCA to plot the dense representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc74172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Exploring Dense Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d9904",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `AutoModel` and `AutoTokenizer` from the `transformers` library.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Plot the dense representations using PCA showing clusters based on original labels.\n",
    "\n",
    "**Expected output:** A visual representation of dense embeddings showing clusters of semantically similar texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378ff528",
   "metadata": {},
   "source": [
    "## Task 4: Comparing Representations in a Classification Task\n",
    "\n",
    "**Context:** Comparing text representations in a classification task helps understand their impact on model performance.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Choose a classification algorithm (e.g., SVM, Random Forest) and train it using both the sparse and dense representations separately.\n",
    "2. Evaluate model performance with metrics such as accuracy, precision, recall, or F1-score.\n",
    "3. Compare the results to discern the strengths of each representation in classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc04620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Comparing Representations in a Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1920a223",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `LogisticRegression` from `sklearn.linear_model` for classification tasks.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Print the classification accuracy and metrics for both sparse and dense representations.\n",
    "\n",
    "**Expected output:** Accuracy scores and classification reports indicating model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cfe400",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "\n",
    "- Successfully loaded and preprocessed the dataset\n",
    "- Implemented sparse text representations using BoW and TF-IDF\n",
    "- Explored dense representations using pre-trained embeddings\n",
    "- Compared model performance using different text representations\n",
    "- Reflected on the advantages and challenges of sparse versus dense representations\n",
    "\n",
    "### üîç Common Issues & Solutions\n",
    "\n",
    "**Problem:** Text data not cleaning properly.\n",
    "\n",
    "**Solution:** Ensure regular expressions are correctly specified for cleaning.\n",
    "\n",
    "**Problem:** Sparse representation errors.\n",
    "\n",
    "**Solution:** Verify the setup of `CountVectorizer` and `TfidfVectorizer`.\n",
    "\n",
    "**Problem:** Dense representation issues.\n",
    "\n",
    "**Solution:** Ensure the pre-trained model and tokenizer are correctly specified and used.\n",
    "\n",
    "### üîë Key Points\n",
    "\n",
    "- Comparing sparse and dense text representations provides insights into their impact on NLP tasks.\n",
    "- Proper preprocessing ensures accurate and standardized input text.\n",
    "- Using visualization tools helps in understanding the clustering of dense embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55629148",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fcc16",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>    \n",
    "\n",
    "```python\n",
    "# Required Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "print(\"Downloading NLTK resources...\")\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    nltk.download('punkt_tab')\n",
    "    print(\"NLTK resources downloaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n",
    "    print(\"Please ensure you have an internet connection or try running nltk.download() manually.\")\n",
    "\n",
    "# Define the data\n",
    "texts = [\n",
    "    \"This movie is fantastic and I love it.\",\n",
    "    \"The acting was great but the plot was weak.\",\n",
    "    \"A terrible film, completely boring.\",\n",
    "    \"I hated every minute of this production.\",\n",
    "    \"Wonderful cinematography and compelling story.\",\n",
    "    \"Bad script and poor direction.\",\n",
    "    \"An absolute masterpiece, highly recommend.\",\n",
    "    \"Utterly disappointing experience.\",\n",
    "    \"Enjoyed it thoroughly, a real gem.\",\n",
    "    \"Skip this one, it's a waste of time.\",\n",
    "    \"The food was delicious and the service was excellent.\",\n",
    "    \"Didn't like the atmosphere, too noisy.\",\n",
    "    \"A truly unique and enjoyable experience.\",\n",
    "    \"The product broke after only a week.\",\n",
    "    \"Highly satisfied with my purchase.\",\n",
    "    \"The customer support was unhelpful.\",\n",
    "    \"What a fantastic performance!\",\n",
    "    \"Quite boring, nothing special.\",\n",
    "    \"Loved the vibrant colors and design.\",\n",
    "    \"The instructions were unclear and confusing.\",\n",
    "    \"Would definitely visit again.\",\n",
    "    \"Overpriced for what you get.\",\n",
    "    \"A pleasant surprise, much better than expected.\",\n",
    "    \"Poor quality materials used.\",\n",
    "    \"Seamless transaction and fast delivery.\",\n",
    "    \"The ending was very predictable.\",\n",
    "    \"Outstanding craftsmanship.\",\n",
    "    \"Left feeling disappointed.\",\n",
    "    \"So happy with the results!\",\n",
    "    \"Could have been much better.\"\n",
    "]\n",
    "# Simple sentiment labels (1: positive, 0: negative/neutral)\n",
    "labels = [\n",
    "    1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
    "    1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
    "    1, 0, 1, 0, 1, 0, 1, 0, 1, 0\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'text': texts, 'label': labels})\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(f\"\\nTotal number of examples: {len(df)}\")\n",
    "\n",
    "# Preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # Remove non-alphabetic characters\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "print(\"\\nProcessed Data (first 5 rows):\")\n",
    "print(df.head())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['processed_text'], df['label'], test_size=0.3, random_state=42, stratify=df['label'])\n",
    "\n",
    "print(f\"\\nTrain set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Train set label distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Test set label distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
    "\n",
    "## 2. Implementing Sparse Representations\n",
    "# Bag-of-Words (BoW)\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)\n",
    "print(\"\\n--- Bag-of-Words ---\")\n",
    "print(\"Vocabulary size:\", len(vectorizer_bow.vocabulary_))\n",
    "print(\"Train BoW matrix shape:\", X_train_bow.shape)\n",
    "print(\"Test BoW matrix shape:\", X_test_bow.shape)\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "print(\"\\n--- TF-IDF ---\")\n",
    "print(\"Vocabulary size:\", len(vectorizer_tfidf.vocabulary_))\n",
    "print(\"Train TF-IDF matrix shape:\", X_train_tfidf.shape)\n",
    "print(\"Test TF-IDF matrix shape:\", X_test_tfidf.shape)\n",
    "\n",
    "## 3. Exploring Dense Representations\n",
    "# Using Pre-trained Embeddings (Sentence Transformers via Hugging Face)\n",
    "# Load model and tokenizer\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embeddings(texts):\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Mean pooling to get a single vector representation for each sentence\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    return sentence_embeddings.numpy()\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "X_train_dense = get_embeddings(X_train.tolist())\n",
    "X_test_dense = get_embeddings(X_test.tolist())\n",
    "print(\"\\n--- Dense Embeddings ---\")\n",
    "print(\"Train Dense matrix shape:\", X_train_dense.shape)\n",
    "print(\"Test Dense matrix shape:\", X_test_dense.shape)\n",
    "\n",
    "# Note: The dimension (768 for MiniLM) is fixed, unlike sparse representations.\n",
    "\n",
    "# Visualize Dense Representations (using PCA)\n",
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_dense)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Use the original index from the split to map labels correctly\n",
    "train_indices = X_train.index\n",
    "colors = ['red' if label == 0 else 'blue' for label in y_train]\n",
    "sns.scatterplot(x=X_train_pca[:, 0], y=X_train_pca[:, 1], hue=y_train.tolist(), palette=['red', 'blue'], legend='full')\n",
    "plt.title('PCA of Dense Embeddings (Train Set)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()\n",
    "\n",
    "## 4. Comparing Representations in a Classification Task\n",
    "# We will train a simple Logistic Regression model on each representation.\n",
    "# Classifier Training and Evaluation\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, representation_name):\n",
    "    print(f\"\\n--- Evaluating with {representation_name} ---\")\n",
    "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Evaluate BoW\n",
    "train_and_evaluate(X_train_bow, X_test_bow, y_train, y_test, \"Bag-of-Words\")\n",
    "\n",
    "# Evaluate TF-IDF\n",
    "train_and_evaluate(X_train_tfidf, X_test_tfidf, y_train, y_test, \"TF-IDF\")\n",
    "\n",
    "# Evaluate Dense Embeddings\n",
    "train_and_evaluate(X_train_dense, X_test_dense, y_train, y_test, \"Dense Embeddings\")\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
