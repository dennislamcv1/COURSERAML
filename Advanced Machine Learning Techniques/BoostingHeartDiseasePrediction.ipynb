{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc85da7",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Using Boosting Models to Predict Heart Disease\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you are tasked with using boosting models to predict the presence of heart disease. Each heartbeat, each pause, tells a story, and as data scientists, it's our job to listen. The UCI Heart Disease dataset is your patient today, and boosted trees are your diagnostic tools. Through this hands-on activity, you will harness the power of XGBoost and LightGBM to improve model accuracy and gain insights into key predictive features. By the end of this lab, you will be able to explore and preprocess data, train and tune XGBoost and LightGBM models, and compare their performance.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- ‚úÖ Apply XGBoost and LightGBM to classification problems\n",
    "- ‚úÖ Perform hyperparameter tuning to enhance model performance\n",
    "- ‚úÖ Evaluate and compare model metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406bf72d",
   "metadata": {},
   "source": [
    "## Task 1: Load and Explore Data\n",
    "\n",
    "**Context:** Understanding the data structure is crucial before jumping into model training.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Conduct exploratory data analysis (EDA) including:**\n",
    "    - Displaying summary statistics\n",
    "    - Checking for missing values\n",
    "    - Identifying categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "!pip install xgboost\n",
    "import xgboost as xgb\n",
    "!pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('heart.csv')\n",
    "df = df.drop('id', axis=1)\n",
    "\n",
    "# Conduct EDA: summary stats, missing values, identifying categorical features\n",
    "# ... your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a1967",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use`pd.read_csv()`to load the data, and `df.describe()` to get summary statistics.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "- Display the first 5 rows of the dataset.\n",
    "\n",
    "**Expected output:** A preview of the data with columns such as 'age', 'sex', 'cp', etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1cf75",
   "metadata": {},
   "source": [
    "## Task 2: Data Preprocessing\n",
    "\n",
    "**Context:** Ensuring data is clean and in a suitable format for model training.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Handle any missing data appropriately.\n",
    "2. Encode non-numeric columns using LabelEncoder.\n",
    "3. Normalize or standardize the data using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b2fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0acdc3",
   "metadata": {},
   "source": [
    "üí° **Tip:** Use `LabelEncoder` for categorical variables and `StandardScaler` for normalization.\n",
    "    \n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "- Check the transformed dataset to ensure all features are numeric.\n",
    "\n",
    "**Expected output:** All columns should have numeric types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92520d71",
   "metadata": {},
   "source": [
    "## Task 3: Split Data\n",
    "\n",
    "**Context:** It is essential to split your data to train and test the models effectively.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Divide the dataset into training and testing sets with an 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f9187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3cc38a",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `train_test_split` with a `test_size` of 0.2 and a `random_state` for reproducibility.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "- Print the shapes of the training and testing sets.\n",
    "\n",
    "**Expected output:** Shapes that reflect the 80-20 split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da3fcc",
   "metadata": {},
   "source": [
    "## Task 4: Model Training with XGBoost\n",
    "\n",
    "**Context:** XGBoost is a powerful gradient boosting library for training models.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Instantiate an `XGBClassifier` from `xgboost`.\n",
    "2. Train the model on the training data.\n",
    "3. Adjust hyperparameters like `n_estimators` and `learning_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Model Training with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef81d3",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `fit` to train the model and `predict` to make predictions.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "- Print the classification report for XGBoost model predictions.\n",
    "\n",
    "**Expected output:** Metrics such as accuracy, precision, recall, F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242cafc",
   "metadata": {},
   "source": [
    "## Task 5: Model Training with LightGBM\n",
    "\n",
    "**Context:** LightGBM is another powerful tool for gradient boosting.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Instantiate an `LGBMClassifier` from `lightgbm`.\n",
    "2. Train the model on the training data.\n",
    "3. Adjust hyperparameters like `max_depth` and `subsample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbec666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Model Training with LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953d9d9",
   "metadata": {},
   "source": [
    "**üí° Tip:** Similar to XGBoost, use `fit` to train and `predict` to predict with LightGBM.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "- Print the classification report for LightGBM model predictions.\n",
    "\n",
    "Expected output: Metrics similar to those of the XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b8c634",
   "metadata": {},
   "source": [
    "# Task 6: Evaluate and Compare Models\n",
    "\n",
    "**Context:** Evaluation helps to understand the strengths and weaknesses of each model.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Compare metrics (accuracy, precision, recall, F1 score) of both models.\n",
    "2. Analyze feature importances from each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Evaluate and Compare Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89f53d",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `feature_importances_` attribute for both models to extract feature importances.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "- Display a comparison table of model metrics.\n",
    "\n",
    "**Expected output:** A clear comparison of both models‚Äô performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60fd1d",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "\n",
    "- Successfully loaded and explored the dataset\n",
    "- Preprocessed the data correctly (handled missing data, encoded categorical variables, normalized data)\n",
    "- Split the data into training and testing sets\n",
    "- Trained both XGBoost and LightGBM models\n",
    "- Evaluated and compared model performances using relevant metrics\n",
    "- Documented reflections and insights\n",
    "\n",
    "### üîç Common Issues & Solutions\n",
    "\n",
    "**Problem:** Dataset file not found.  \n",
    "**Solution:** Ensure the dataset file is in the correct folder.\n",
    "\n",
    "**Problem:** Categorical encoding errors.   \n",
    "**Solution:** Double-check the columns being encoded.\n",
    "\n",
    "**Problem:** Model training errors.   \n",
    "**Solution:** Verify that data preprocessing steps were correctly applied.\n",
    "\n",
    "### üîë Key Points\n",
    "\n",
    "- Data preprocessing is critical for model performance.\n",
    "- Hyperparameter tuning can significantly impact boost model outcomes.\n",
    "- Evaluating both model metrics and feature importances helps in understanding model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7f62a",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167075f6",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>\n",
    "\n",
    "```python\n",
    "# Required Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "!pip install xgboost\n",
    "import xgboost as xgb\n",
    "!pip install lightgbm   \n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('heart.csv')\n",
    "df = df.drop('id', axis=1)\n",
    "\n",
    "# Data Exploration\n",
    "# Display summary statistics\n",
    "print(df.describe())\n",
    "print(df.dtypes)\n",
    "df.isnull().sum()\n",
    "\n",
    "# Data Preprocessing\n",
    "label_encoder = LabelEncoder()\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "y = df.pop('num')\n",
    "X = scaler.fit_transform(df)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost Model\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "print(f\"XGBoost Model Report:\\n {classification_report(y_test, y_pred_xgb)}\")\n",
    "\n",
    "# Train LightGBM Model\n",
    "lgb_clf = lgb.LGBMClassifier(n_estimators=100, max_depth=4, subsample=0.8)\n",
    "lgb_clf.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_clf.predict(X_test)\n",
    "print(f\"LightGBM Model Report:\\n {classification_report(y_test, y_pred_lgb)}\")\n",
    "\n",
    "# Feature Importance\n",
    "xgb_importances = xgb_clf.feature_importances_\n",
    "lgb_importances = lgb_clf.feature_importances_\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
