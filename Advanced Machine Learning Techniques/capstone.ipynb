{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0224003b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "I-vng2xrR-Yl",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "762809393001da7a26fed4d50520519e",
     "grade": false,
     "grade_id": "cell-59aae25dffb1f905",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ðŸ‘©â€ðŸ’» Multi-Domain Machine Learning Challenge: From Classification to Optimization Time Estimate: 150 minutes (2.5 hours)\n",
    "\n",
    "## ðŸ“‹ Overview\n",
    "\n",
    "ðŸ“‹ Overview In this comprehensive lab, you'll work with scikit-learn's wine and 20 newsgroups datasets to explore various machine learning domains. You'll implement ensemble methods, perform dimensionality reduction, tackle text classification, and explore optimization techniques. This lab synthesizes key machine learning concepts into a cohesive, practical experience.\n",
    "\n",
    "## ðŸŽ¯ Learning Outcomes\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "âœ… Build and evaluate ensemble models using scikit-learn\n",
    "\n",
    "âœ… Apply PCA for dimensionality reduction and visualization\n",
    "\n",
    "âœ… Implement text classification using NLP techniques\n",
    "\n",
    "âœ… Optimize models using cross-validation and parameter tuning\n",
    "\n",
    "âœ… Interpret and visualize machine learning results\n",
    "\n",
    "\n",
    "## ðŸ–¥ï¸ Tasks\n",
    "\n",
    "### ðŸ“‚ Task 1: Ensemble Learning with Wine Dataset (30 minutes)\n",
    "\n",
    "Using the wine dataset, you'll build an ensemble model to classify wine varieties based on their chemical properties.\n",
    "\n",
    "**Steps:**\n",
    "1. Split data into training and testing sets\n",
    "2. Implement RandomForestClassifier\n",
    "3. Evaluate using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5167e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups, load_wine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score, learning_curve, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the data\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c674c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JnTQRTtVO78",
    "outputId": "77f9cfb1-6e38-4093-e338-89aa395ce0bf"
   },
   "outputs": [],
   "source": [
    "# Step 1: Split data into training and testing sets\n",
    "\n",
    "\n",
    "# Step 2: Implement RandomForestClassifier\n",
    "\n",
    "\n",
    "# Step 3: Evaluate using cross-validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e08e779",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ca9471c2d34f3c54c52c655b1d6e8a4",
     "grade": false,
     "grade_id": "cell-3065b1e947b96167",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Grading of Lab Assignments:\n",
    "The grading of this assignment is based on the test cases throughout this notebook within the `### BEGIN TESTS` and `### END TESTS` comments. \n",
    "\n",
    "Each task has a number of test cells. For example, the cell below uses four tests to confirm the shape of the dataset, the use of RandomForestClassifier, and the expected length and mean of the cross-validation scores.\n",
    "\n",
    "Run all of these test cells throughout the project to confirm you pass the tests and are on the right track. Once you have passed all the tests in the entire notebook, or are happy with your results you can click the `Submit Assignment` button in the top right corner for your final submission and grading. \n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b84220",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8a1f9a8afead6aa3e8397018ff6d28c",
     "grade": true,
     "grade_id": "d48918fbcf144710a02caa3e99b53974",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "nbgrader"
    ]
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert X.shape[1] == 13, \"Dataset should have 13 features\"\n",
    "\n",
    "assert isinstance(rf, RandomForestClassifier), \"rf should be a RandomForestClassifier instance\"\n",
    "\n",
    "assert hasattr(cv_scores, '__len__') and len(cv_scores) == 5, \"There should be 5 cross-validation scores\"\n",
    "\n",
    "assert cv_scores.mean() >= 0.90, f\"Average CV score should be at least 0.90, got {cv_scores.mean()}\"\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5fb7c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f0d8f918e48f0337c3e5ccd78ea57cd",
     "grade": false,
     "grade_id": "cell-41c63579cdd76ae3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## âœ… Success Checklist\n",
    "\n",
    "* Dataset should load successfully with 13 features\n",
    "* Cross-validation scores should be printed\n",
    "* Average score should be above 0.90\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8cc2b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8e2ffaa30e095256248eff84405a95e",
     "grade": false,
     "grade_id": "cell-83f6b04443e0ffbc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ðŸ’¡ Key Points\n",
    "* Class balance shapes metric choice â€” use F1 or precision for imbalanced data.\n",
    "* Random Forest handles complexity well â€” robust, accurate, and interpretable.\n",
    "* Cross-validation checks model stability â€” ensures consistent performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502835f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MPMXc9tIZ4b5",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8a4de9b549abafb18c1b02fb415a9d0",
     "grade": false,
     "grade_id": "cell-4fdeeaed0029d6e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### ðŸ” Task 2: Dimensionality Reduction and Visualization (30 minutes)\n",
    "\n",
    "We'll use PCA to visualize how the different wine varieties cluster based on their chemical properties.\n",
    "\n",
    "Steps:\n",
    "1. Apply PCA to the wine dataset\n",
    "2. Create visualization of the first two principal components\n",
    "3. Analyze feature contributions to principal components\n",
    "4. Plot explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9537551",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QTc9xOUXZmUs",
    "outputId": "899011bf-a458-4ab8-ea92-f03f8ae2fd1f"
   },
   "outputs": [],
   "source": [
    "# Step 1: Apply PCA to the wine dataset\n",
    "\n",
    "\n",
    "# Step 2: Create visualization of the first two principal components\n",
    "\n",
    "\n",
    "# Step 3: Analyze feature contributions to principal components\n",
    "\n",
    "\n",
    "# Step 4: Plot explained variance ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df8596",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b6b74a4b335e85904825462b3b4de79",
     "grade": true,
     "grade_id": "d02e1d6a7ffb4a1c8f9ee7a9593aac61",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "nbgrader"
    ]
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert X_pca.ndim == 2, \"X_pca should be a 2D array\"\n",
    "\n",
    "assert X_pca.shape[1] == X.shape[1], \\\n",
    "    f\"PCA should return {X.shape[1]} components, got {X_pca.shape[1]}\"\n",
    "\n",
    "assert components_df.shape[0] == X.shape[1], \\\n",
    "    \"components_df should have rows equal to number of features\"\n",
    "\n",
    "assert 'PC1' in components_df.columns and 'PC2' in components_df.columns, \\\n",
    "    \"components_df should contain PC1 and PC2\"\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58654e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6718d71393468929ca432f593b6252f",
     "grade": false,
     "grade_id": "cell-f12927f1a366368f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## âœ… Success Checklist\n",
    "* Two plots should be generated\n",
    "* Clusters should be visible in the PCA scatter plot\n",
    "* Explained variance plot should show cumulative variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecbb88c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43642df28ac82a5bc2f39516acd5aa4e",
     "grade": false,
     "grade_id": "cell-f7560f815cc888a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ðŸ’¡ Key Points\n",
    "* PCA simplifies data for visualization, highlighting class separation in 2D.\n",
    "* Explained variance shows how many components matter for retaining information.\n",
    "* Feature loadings reveal which variables drive key components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0843c29a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "G7UT8iJ-bJ3m",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b200479c88280f0e7303c7ab94ba9952",
     "grade": false,
     "grade_id": "cell-27365b28eba9faf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### ðŸ” Task 3: Text Classification with News Data (30 minutes)\n",
    "\n",
    "Using sklearn's 20 newsgroups dataset, we'll build a text classifier to categorize news articles.\n",
    "\n",
    "Steps:\n",
    "1. Load a subset of the 20 newsgroups dataset\n",
    "2. Create a text classification pipeline\n",
    "3. Train and evaluate the model\n",
    "4. Test on sample texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb78a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTAyKWBtadsk",
    "outputId": "703c14c4-080f-4780-ceee-d55db62efa8b"
   },
   "outputs": [],
   "source": [
    "# Step 1: Load a subset of the 20 newsgroups dataset\n",
    "\n",
    "\n",
    "# Step 2: Create a text classification pipeline\n",
    "\n",
    "\n",
    "# Step 3: Train and evaluate the model\n",
    "\n",
    "\n",
    "# Step 4: Test on sample texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1b992",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d23044b380358057b3ab0923abe3025",
     "grade": true,
     "grade_id": "7ed8c4744a914a8799b49b712dbacb88",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "nbgrader"
    ]
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert len(categories) == 4, \"There should be 4 categories loaded\"\n",
    "\n",
    "assert hasattr(text_clf, 'predict'), \"text_clf should have a predict method\"\n",
    "\n",
    "assert len(y_pred) == len(y_test), \\\n",
    "    f\"Number of test-set predictions ({len(y_pred)}) should match test set size ({len(y_test)})\"\n",
    "\n",
    "assert set(y_pred).issubset(set(range(len(categories)))), \\\n",
    "    \"Predicted categories for the test set should be within valid range\"\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc89c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4da5092f9307fcabe3066086601f023f",
     "grade": false,
     "grade_id": "cell-4adcceb841df85d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## âœ… Success Checklist\n",
    "* Model should train without errors\n",
    "* Classification report should show reasonable metrics\n",
    "* Sample text predictions should make logical sense\n",
    "* Top features for each category should be relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047dcc27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b2de7f60095779cfa8f3b3ef7ba9d36",
     "grade": false,
     "grade_id": "cell-9c45590d15c14ef0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ðŸ’¡ Key Points\n",
    "* TF-IDF boosts relevance â€” it emphasizes distinctive terms over frequent ones.\n",
    "* Category choice impacts performance â€” overlapping topics may confuse the model.\n",
    "* Preprocessing boosts accuracy â€” removing noise (e.g. stopwords, headers) improves results.\n",
    "* Naive Bayes with a pipeline is fast and effective â€” especially for text classification.\n",
    "* Top features help explain predictions â€” showing which words drive classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144bbac1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uWm1tLrdbjKY",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9533aa3daf41b051170c293170164da",
     "grade": false,
     "grade_id": "cell-7e220a00e91fbd7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### ðŸ” Task 4: Model Optimization with Cross-Validation (30 minutes)\n",
    "\n",
    "We'll optimize and evaluate our wine classification model using proper cross-validation and visualization techniques.\n",
    "\n",
    "Steps:\n",
    "1. Set up validation strategy\n",
    "2. Create and evaluate base model\n",
    "3. Plot learning curves\n",
    "4. Analyze feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda71a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xPzEwCDBbQck",
    "outputId": "1bbb1c19-5f5a-4df3-95a9-2eeffba0e025"
   },
   "outputs": [],
   "source": [
    "# Step 1: Set up validation strategy\n",
    "\n",
    "\n",
    "# Step 2: Create and evaluate base model\n",
    "\n",
    "\n",
    "# Step 3: Plot learning curves\n",
    "\n",
    "\n",
    "# Step 4: Analyze feature importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c42f22",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6a244c10753b816bc30882f141e0390",
     "grade": true,
     "grade_id": "fd8fb7d5587a45f89d4f51b4e2e1ac90",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "nbgrader"
    ]
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert isinstance(test_accuracy, float), \"test_accuracy should be a float\"\n",
    "assert test_accuracy >= 0.85, f\"Test set accuracy should be at least 0.85, got {test_accuracy}\"\n",
    "assert len(top_features_idx) == 5, \"There should be 5 most important features\"\n",
    "assert len(feature_importance) == X.shape[1], \"feature_importance length should match number of features\"\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5d9a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e7a53f3a202fb828633d38e2099b834",
     "grade": false,
     "grade_id": "cell-b7af8ef7b73595cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## âœ… Success Checklist\n",
    "* Cross-validation scores should be printed\n",
    "* Two plots should be generated:\n",
    "  1. Learning curves showing training and validation scores\n",
    "  2. Feature importance bar plot\n",
    "* Top 5 important features should be listed\n",
    "* Test set accuracy should be above 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a3862",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e639e67cd70accb38989c2fb1d4e8845",
     "grade": false,
     "grade_id": "cell-d573511126fadaca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ðŸ’¡ Key Points\n",
    "* Learning curves reveal bias vs. variance â€” gaps between training and validation scores indicate overfitting or underfitting.\n",
    "* Feature importance highlights key drivers â€” top features align with chemical properties relevant to wine classification.\n",
    "* Random Forest balances power and interpretability â€” effective with scaled data and useful for insight into feature relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac065e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IGMx4MHadoEX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99240d681a40235f560433f5b568cee7",
     "grade": false,
     "grade_id": "cell-94206cb8e1891561",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### ðŸ” Task 5: Comparing Models and Validation (30 minutes)\n",
    "\n",
    "We'll compare different classifiers on the wine dataset and implement K-fold cross-validation for robust evaluation.\n",
    "\n",
    "Steps:\n",
    "1. Set up multiple classifiers\n",
    "  * Initialize Random Forest, SVM, and KNN models\n",
    "  * Prepare data with StandardScaler\n",
    "2. Perform cross-validation\n",
    "  * Run 5-fold cross-validation for each model\n",
    "  * Calculate and compare mean accuracies\n",
    "3. Create visualization comparisons\n",
    "  * Generate box plots of model performances\n",
    "  * Create confusion matrix for best model\n",
    "4. Evaluate best model\n",
    "  * Identify best performing classifier\n",
    "  * Generate detailed classification report\n",
    "  * Test on sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a230f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ft36XiG1buby",
    "outputId": "ad06dcf0-4197-4754-c8ca-28ed69063c7b"
   },
   "outputs": [],
   "source": [
    "# Step 1: Set up multiple classifiers\n",
    "## Initialize Random Forest, SVM, and KNN models\n",
    "\n",
    "\n",
    "## Prepare data with StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Perform cross-validation\n",
    "## Run 5-fold cross-validation for each model\n",
    "\n",
    "\n",
    "## Calculate and compare mean accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd66902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create visualization comparisons\n",
    "## Generate box plots of model performances\n",
    "\n",
    "\n",
    "## Create confusion matrix for best model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate best model\n",
    "## Identify best performing classifier\n",
    "\n",
    "\n",
    "## Generate detailed classification report\n",
    "\n",
    "\n",
    "## Test on sample predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99b64f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb0054450ae90b751160666e62d89ce4",
     "grade": true,
     "grade_id": "4353bd69066e42c39c8f31ac32292336",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "nbgrader"
    ]
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert isinstance(cv_scores, dict), \"cv_results should be a dictionary\"\n",
    "assert all(isinstance(k, str) for k in cv_scores.keys()), \"cv_results keys should be strings\"\n",
    "assert all(hasattr(v, '__len__') for v in cv_scores.values()), \"All cv_results values should be sequences\"\n",
    "assert len(cv_scores) >= 2, \"There should be at least two models compared in cv_results\"\n",
    "assert isinstance(best_model, (RandomForestClassifier, SVC, KNeighborsClassifier)), \"best_model should be an instance of a known classifier\"\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb8f56",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd3baf0bccbde4a6a24a773cdc7dcb4d",
     "grade": false,
     "grade_id": "cell-185ce8ba51b2a6e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## âœ… Success Checklist\n",
    "* Cross-validation scores should be displayed for all models\n",
    "* Box plot should show clear comparison between models\n",
    "* Confusion matrix should be readable and properly labeled\n",
    "* Sample predictions should match expected wine categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765976c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f16ae06800225a255ca084d4ae3abf4c",
     "grade": false,
     "grade_id": "cell-4795dcca85ad8303",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ðŸ’¡ Key Points\n",
    "* Cross-validation highlights consistency â€” boxplots reveal variability and robustness across models.\n",
    "* Confusion matrix shows classification challenges â€” some wine classes are harder to separate.\n",
    "* Model tuning can boost weaker models â€” hyperparameters and preprocessing may improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519a69d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5eb7fb055b4e7b7fd6988d8e4c8d926",
     "grade": false,
     "grade_id": "cell-ac8ccf84295b4003",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ðŸ’» Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f662cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4441747478c5f9d2d57bb8a49aa54e76",
     "grade": false,
     "grade_id": "cell-e13357e75660e3ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>\n",
    "\n",
    "### Task 1: Ensemble Learning with Wine Dataset\n",
    "    \n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Create a DataFrame for better data inspection\n",
    "wine_df = pd.DataFrame(X, columns=wine.feature_names)\n",
    "print(\"Dataset Shape:\", wine_df.shape)\n",
    "print(\"\\nFeature Names:\", wine.feature_names)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(wine_df.head())\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "cv_scores = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"\\nCross-validation scores:\", cv_scores)\n",
    "print(\"Average CV Score:\", cv_scores.mean())\n",
    "```\n",
    "\n",
    "### Task 2: Dimensionality Reduction and Visualization\n",
    "\n",
    "```python\n",
    "# Import additional libraries\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize and fit PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Wine Classes Visualized Using PCA')\n",
    "plt.colorbar(label='Target Class')\n",
    "\n",
    "# Plot explained variance ratio\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "          np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio vs. Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print feature contributions\n",
    "components_df = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(pca.components_.shape[0])],\n",
    "    index=wine.feature_names\n",
    ")\n",
    "print(\"\\nFeature contributions to first two PCs:\")\n",
    "print(components_df[['PC1', 'PC2']].round(3))\n",
    "```\n",
    "\n",
    "### Task 3: Text Classification with News Data\n",
    "\n",
    "\n",
    "```python    \n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load a subset of the 20 newsgroups dataset\n",
    "categories = ['sci.med', 'sci.space', 'rec.sport.baseball', 'talk.politics.misc']\n",
    "print(\"Loading 20 newsgroups dataset for categories:\", categories)\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all',\n",
    "                               categories=categories,\n",
    "                               shuffle=True,\n",
    "                               random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create the pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    newsgroups.data,\n",
    "    newsgroups.target,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# Print performance report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=categories\n",
    "))\n",
    "\n",
    "# Test on some sample texts\n",
    "sample_texts = [\n",
    "    \"The spacecraft launched successfully from Cape Canaveral\",\n",
    "    \"The doctor prescribed antibiotics for the infection\",\n",
    "    \"The Yankees won the baseball game in extra innings\",\n",
    "    \"The senate will vote on the new bill next week\"\n",
    "]\n",
    "\n",
    "print(\"\\nPredicting categories for sample texts:\")\n",
    "predictions = text_clf.predict(sample_texts)\n",
    "for text, predicted in zip(sample_texts, predictions):\n",
    "    print(f'\\nText: {text}')\n",
    "    print(f'Predicted category: {categories[predicted]}')\n",
    "\n",
    "# Get feature names and their importance scores\n",
    "tfidf = text_clf.named_steps['tfidf']\n",
    "clf = text_clf.named_steps['clf']\n",
    "\n",
    "# Get top features for each category\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "for i, category in enumerate(categories):\n",
    "    top_features_idx = clf.feature_log_prob_[i].argsort()[-10:][::-1]\n",
    "    top_features = [feature_names[idx] for idx in top_features_idx]\n",
    "    print(f\"\\nTop 10 features for {category}:\")\n",
    "    print(\", \".join(top_features))\n",
    "```\n",
    "\n",
    "### Task 4: Model Optimization with Cross-Validation\n",
    "\n",
    "\n",
    "```python    \n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import cross_val_score, learning_curve, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the data first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the classifier\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "print(\"Performing cross-validation...\")\n",
    "cv_scores = cross_val_score(rf, X_train_scaled, y_train, cv=5)\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Average CV score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Generate learning curves\n",
    "print(\"\\nGenerating learning curves...\")\n",
    "train_sizes = np.linspace(0.1, 1.0, 5)\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    rf,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    train_sizes=train_sizes,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Calculate mean and std for training and validation scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training score', color='blue', marker='o')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.15, color='blue')\n",
    "plt.plot(train_sizes, val_mean, label='Cross-validation score', color='red', marker='o')\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.15, color='red')\n",
    "\n",
    "plt.xlabel('Training Examples')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curves for Random Forest Classifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Train the model and get feature importance\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Create feature importance plot\n",
    "feature_importance = rf.feature_importances_\n",
    "feature_names = wine.feature_names\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), np.array(feature_names)[sorted_idx])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance in Wine Classification')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top 5 most important features\n",
    "top_features_idx = np.argsort(feature_importance)[::-1][:5]\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for idx in top_features_idx:\n",
    "    print(f\"{feature_names[idx]}: {feature_importance[idx]:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = rf.predict(X_test_scaled)\n",
    "test_accuracy = rf.score(X_test_scaled, y_test)\n",
    "print(f\"\\nTest Set Accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "# Print example predictions\n",
    "print(\"\\nExample Predictions:\")\n",
    "for i in range(5):\n",
    "    true_label = wine.target_names[y_test[i]]\n",
    "    pred_label = wine.target_names[y_pred[i]]\n",
    "    print(f\"True: {true_label}, Predicted: {pred_label}\")\n",
    "\n",
    "    \n",
    "```\n",
    "\n",
    "### Task 5: Comparing Models and Validation\n",
    "    \n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess data\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Perform k-fold cross-validation for each model\n",
    "print(\"Comparing model performances using 5-fold cross-validation:\\n\")\n",
    "cv_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "    cv_scores[name] = scores\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Mean accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "    print()\n",
    "\n",
    "# Visualize cross-validation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "box_data = [cv_scores[name] for name in models.keys()]\n",
    "plt.boxplot(box_data, labels=models.keys()])\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Train the best model (based on cross-validation results)\n",
    "best_model_name = max(cv_scores, key=lambda k: cv_scores[k].mean())\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "\n",
    "# Fit the best model and get predictions\n",
    "best_model.fit(X_scaled, y)\n",
    "y_pred = best_model.predict(X_scaled)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=wine.target_names,\n",
    "            yticklabels=wine.target_names)\n",
    "plt.title(f'Confusion Matrix for {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y, y_pred, target_names=wine.target_names))\n",
    "\n",
    "# Test model on sample predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(0, len(y))\n",
    "    true_label = wine.target_names[y[idx]]\n",
    "    pred_label = wine.target_names[best_model.predict([X_scaled[idx]])[0]]\n",
    "\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"True: {true_label}\")\n",
    "    print(f\"Predicted: {pred_label}\")\n",
    "    print()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
