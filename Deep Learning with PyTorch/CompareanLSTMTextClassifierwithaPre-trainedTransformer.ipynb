{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706f267d",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Compare an LSTM Text Classifier with a Pre-trained Transformer\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you'll build and compare two powerful text classification approaches: a custom LSTM model and a pre-trained transformer model using the Hugging Face library. You'll work with movie review sentiment analysis - a practical application found in product recommendation systems, social media monitoring, and customer feedback analysis. By the end of this lab, you'll understand the trade-offs between model complexity, performance, and implementation effort for these two popular NLP approaches.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Implement an LSTM-based text classification model using PyTorch\n",
    "- Utilize a pre-trained transformer model for sentiment analysis using Hugging Face\n",
    "- Compare the performance metrics (accuracy, training time, inference speed) between LSTM and transformer models\n",
    "- Make informed decisions about which model type to use for different NLP applications\n",
    "\n",
    "## üöÄ Starting Point\n",
    "Access the starter code below to begin your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5768e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Simplified Data Preparation ---\n",
    "# Using a small, synthetic dataset for demonstration.\n",
    "# For a real application, you would load a dataset like IMDb from a CSV or similar.\n",
    "print(\"Generating synthetic dataset...\")\n",
    "texts = [\n",
    "    \"This movie was fantastic and I loved it.\",\n",
    "    \"The acting was terrible, completely ruined the film.\",\n",
    "    \"It was an okay film, nothing special but not bad.\",\n",
    "    \"Absolutely brilliant cinematography and a compelling story.\",\n",
    "    \"I hated every minute, a true waste of time.\",\n",
    "    \"A decent watch, worth seeing if you have nothing else.\",\n",
    "    \"Simply the best film I've seen all year!\",\n",
    "    \"So boring, I fell asleep multiple times.\",\n",
    "    \"Good plot, but the characters were underdeveloped.\",\n",
    "    \"An inspiring and emotional journey.\"\n",
    "]\n",
    "labels = [1, 0, 1, 1, 0, 1, 1, 0, 1, 1] # 1 for positive, 0 for negative/neutral\n",
    "\n",
    "# Split into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f9b86",
   "metadata": {},
   "source": [
    "Required tools/setup:\n",
    "\n",
    "- Python 3.8+\n",
    "- PyTorch 1.10+\n",
    "- Transformers library\n",
    "- Matplotlib for visualization\n",
    "- Internet connection to download datasets and pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf16c14",
   "metadata": {},
   "source": [
    "## Task 1: Prepare Data for LSTM Model\n",
    "**Context:** Data preparation is crucial for NLP tasks. For an LSTM model, we need to tokenize text, build a vocabulary, and convert text to numerical sequences of consistent length.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a tokenization function using `get_tokenizer` from torchtext\n",
    "\n",
    "    - The 'basic_english' tokenizer is suitable for this task\n",
    "    - Consider how different tokenization strategies might affect your model\n",
    "\n",
    "2. Build a vocabulary from the training data\n",
    "\n",
    "    - Use `build_vocab_from_iterator` function\n",
    "    - Set a reasonable vocabulary size (e.g., 25000 most common words)\n",
    "    - Include special tokens like `<unk>` for unknown words\n",
    "\n",
    "3. Create functions to convert tokenized text to numerical sequences\n",
    "\n",
    "    - Map each token to its index in the vocabulary\n",
    "    - Implement padding to ensure all sequences are the same length\n",
    "    - How will you handle sequences longer than your max length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb3a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1: Data Preparation\n",
    "# Step 1: Create tokenizer\n",
    "\n",
    "# Step 2: Build vocabulary function\n",
    "\n",
    "# Step 3: Text to numerical sequences function\n",
    "\n",
    "# Step 4: Create PyTorch datasets and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26f221",
   "metadata": {},
   "source": [
    "**üí° Tip:** Consider using `torch.nn.utils.rnn.pad_sequence` to handle varying sequence lengths efficiently.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Print the vocabulary size\n",
    "- Tokenize and convert a sample review\n",
    "- Verify the dimensions of your batched data (should be [batch_size, seq_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f33969a",
   "metadata": {},
   "source": [
    "## Task 2: Design and Implement LSTM Model\n",
    "**Context:** LSTMs are specialized recurrent neural networks that excel at capturing sequential patterns in text. They're commonly used before transformers became prevalent and are still valuable for many applications.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create an LSTM model class that inherits from `nn.Module`\n",
    "\n",
    "    - Include an embedding layer (`nn.Embeddin`g) to convert token indices to vector representations\n",
    "    - Add LSTM layers (`nn.LSTM`) with appropriate hidden dimensions\n",
    "    - Implement dropout for regularization\n",
    "    - Add a final linear layer (`nn.Linear`) for classification\n",
    "\n",
    "2. Initialize your model with appropriate hyperparameters\n",
    "\n",
    "    - Consider vocabulary size, embedding dimension, hidden dimension, etc.\n",
    "    - How many LSTM layers will you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2: LSTM Model Implementation \n",
    "    # Define model initialization\n",
    "    \n",
    "    # Define forward pass\n",
    "\n",
    "# Initialize model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd793838",
   "metadata": {},
   "source": [
    "**üí° Tip:** Using bidirectional LSTMs (`bidirectional=True parameter`) can improve performance by capturing context from both directions.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Create a small batch of dummy data and pass it through your model\n",
    "- Verify the output shape (should be [batch_size, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b6e11",
   "metadata": {},
   "source": [
    "## Task 3: Train the LSTM Model\n",
    "**Context:** Training deep learning models requires careful monitoring of metrics and hyperparameter tuning to achieve optimal performance.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define training hyperparameters\n",
    "\n",
    "    - Choose an appropriate optimizer (Adam is often a good choice)\n",
    "    - Set learning rate, batch size, number of epochs\n",
    "    - Define a loss function (`nn.BCEWithLogitsLoss` for binary classification)\n",
    "\n",
    "2. Implement the training loop\n",
    "\n",
    "    - Iterate through batches from the training dataloader\n",
    "    - Calculate loss and perform backpropagation\n",
    "    - Track and report training metrics\n",
    "    - Use `model.eval()` when evaluating to disable dropout\n",
    "\n",
    "3. Evaluate model performance on test data\n",
    "\n",
    "    - Calculate accuracy, precision, recall or other relevant metrics\n",
    "    - Track the time taken for training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0da8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3: LSTM Model Training\n",
    "# Define loss and optimizer\n",
    "\n",
    "# Training loop function\n",
    "\n",
    "# Evaluation function\n",
    "\n",
    "# Execute training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da967654",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `torch.no_grad()` context manager during evaluation to conserve memory and speed up inference.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Training loss should decrease over epochs\n",
    "- Test accuracy should improve as training progresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b463c",
   "metadata": {},
   "source": [
    "## Task 4: Implement Sentiment Analysis with a Pre-trained Transformer\n",
    "**Context:** Pre-trained transformer models have revolutionized NLP by providing powerful, ready-to-use models that capture complex language patterns.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Load a pre-trained sentiment analysis model from Hugging Face\n",
    "\n",
    "    - Use the pipeline function for the simplest implementation\n",
    "    - Alternatively, load a specific model like \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "2. Process the test dataset with the transformer model\n",
    "\n",
    "    - Be mindful of input formatting requirements\n",
    "    - Consider batching for efficiency\n",
    "    - Track the time required for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4: Pre-trained Transformer Implementation\n",
    "# Load pre-trained model\n",
    "\n",
    "# Define inference function\n",
    "\n",
    "# Run inference on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04366c61",
   "metadata": {},
   "source": [
    "**üí° Tip:** The `pipeline` function abstracts away much of the complexity, but loading the model and tokenizer separately gives you more control over the process.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Try the model on a few sample reviews\n",
    "- Verify predictions match expected sentiment (positive/negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76dfba0",
   "metadata": {},
   "source": [
    "## Task 5: Compare Model Performance\n",
    "**Context:** Understanding model trade-offs is crucial for choosing the right approach for a given application and resource constraints.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Calculate accuracy for both models\n",
    "\n",
    "    - Use the same test set for fair comparison\n",
    "    - Consider additional metrics like F1 score if appropriate\n",
    "\n",
    "2. Compare computational efficiency\n",
    "\n",
    "    - Record and compare training time (LSTM only)\n",
    "    - Measure and compare inference time for both models\n",
    "    - Calculate model size (number of parameters)\n",
    "\n",
    "3. Create visualizations to illustrate the comparison\n",
    "\n",
    "    - Bar charts for accuracy and time metrics\n",
    "    - Consider visualizing specific examples where models differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5: Model Comparison\n",
    "# Calculate performance metrics\n",
    "\n",
    "# Compare computational efficiency\n",
    "\n",
    "# Create visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e69c1",
   "metadata": {},
   "source": [
    "**üí° Tip:** Consider the trade-offs beyond just accuracy - deployment requirements, inference speed, and explainability are important factors in real-world applications.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Verify that your comparison metrics are calculated correctly\n",
    "- Ensure visualizations clearly communicate the key differences\n",
    "\n",
    "## ‚úÖ Success Checklist\n",
    "- LSTM model correctly implemented and trained\n",
    "- Pre-trained transformer model successfully applied to test data\n",
    "- Accuracy metrics calculated for both models\n",
    "- Computational efficiency (time, parameters) compared between models\n",
    "- Clear understanding of trade-offs between both approaches demonstrated\n",
    "- Program runs without errors\n",
    "\n",
    "## üîç Common Issues & Solutions\n",
    "**Problem:** LSTM training is extremely slow **Solution:** Reduce batch size, ensure you're using GPU if available, or reduce sequence length.\n",
    "\n",
    "**Problem:** Out-of-memory errors when using transformer models **Solution:** Reduce batch size or use a smaller model like DistilBERT instead of BERT.\n",
    "\n",
    "**Problem:** Poor LSTM performance compared to reported benchmarks **Solution:** Check preprocessing steps, increase model capacity, or try bidirectional LSTMs.\n",
    "\n",
    "**Problem:** Hugging Face models download/load slowly **Solution:** Ensure good internet connection or download models once and save locally.\n",
    "\n",
    "## üîë Key Points\n",
    "- LSTMs require more manual implementation but give you full control over the architecture\n",
    "- Pre-trained transformers provide powerful out-of-the-box performance but are less flexible\n",
    "- The performance gap between custom LSTMs and pre-trained transformers highlights the value of transfer learning\n",
    "- Consider computational requirements when choosing between models for production applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093608e3",
   "metadata": {},
   "source": [
    "## üíª Reference Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c61da",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><strong>Click HERE to see a reference solution</strong></summary>    \n",
    "    \n",
    "```python\n",
    "# TASK 1: Data Preparation\n",
    "# Using a Hugging Face tokenizer for consistency and simplicity\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "max_len = 128 # Reduced max_len for smaller dataset and faster processing\n",
    "\n",
    "# Text processing function - now uses the Hugging Face tokenizer\n",
    "def process_text_hf(text, tokenizer, max_len):\n",
    "    # This will pad and truncate automatically\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoding['input_ids'].squeeze(0) # Remove batch dimension\n",
    "\n",
    "# Custom Dataset\n",
    "class SimplifiedIMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "        for text, label in zip(texts, labels):\n",
    "            text_tensor = process_text_hf(text, self.tokenizer, self.max_len)\n",
    "            label_tensor = torch.tensor([float(label)], dtype=torch.float)\n",
    "            self.data.append((text_tensor, label_tensor))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SimplifiedIMDBDataset(train_texts, train_labels, tokenizer, max_len)\n",
    "test_dataset = SimplifiedIMDBDataset(test_texts, test_labels, tokenizer, max_len)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 2 # Reduced batch size for very small dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Determine vocab_size and pad_idx from the Hugging Face tokenizer\n",
    "# Note: For LSTM, we would typically build a custom vocab if not using pretrained embeddings\n",
    "# For simplification here, we'll use a large enough vocab size and HF's pad_token_id\n",
    "vocab_size = tokenizer.vocab_size\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Vocab size (from HF tokenizer): {vocab_size}\")\n",
    "print(f\"Pad token ID (from HF tokenizer): {pad_idx}\")\n",
    "\n",
    "# TASK 2: LSTM Model (Minor adjustments for new data processing)\n",
    "class LSTMTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                            bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: [batch_size, seq_len]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # output shape: [batch_size, seq_len, hidden_dim * n_directions]\n",
    "\n",
    "        if self.lstm.bidirectional:\n",
    "            # Concatenate the last two hidden states (forward and backward)\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        # hidden shape: [batch_size, hidden_dim * n_directions]\n",
    "\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# Initialize LSTM model\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128 # Reduced hidden_dim for smaller model\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "lstm_model = LSTMTextClassifier(\n",
    "    vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "    n_layers, bidirectional, dropout, pad_idx\n",
    ").to(device)\n",
    "\n",
    "# TASK 3: LSTM Training\n",
    "optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion, epochs=3): # Reduced epochs for faster demo\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    epoch_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (text, labels) in enumerate(dataloader):\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 1 == 0: # Print more frequently for tiny dataset\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "    return epoch_losses, training_time\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, labels in dataloader:\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            outputs = model(text)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    inference_time = time.time() - start_time\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Inference time: {inference_time:.2f} seconds\")\n",
    "\n",
    "    return accuracy, inference_time\n",
    "\n",
    "# Train and evaluate LSTM\n",
    "print(\"\\n--- Training LSTM Model ---\")\n",
    "lstm_losses, lstm_train_time = train_model(lstm_model, train_dataloader, optimizer_lstm, criterion, epochs=3)\n",
    "print(\"\\n--- Evaluating LSTM Model ---\")\n",
    "lstm_accuracy, lstm_inference_time = evaluate_model(lstm_model, test_dataloader)\n",
    "\n",
    "# TASK 4: Pre-trained Transformer\n",
    "# Load transformer model\n",
    "transformer_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer_transformer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "model_transformer = AutoModelForSequenceClassification.from_pretrained(transformer_model_name).to(device)\n",
    "\n",
    "# Using pipeline for easy sentiment analysis (as in original)\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_transformer, tokenizer=tokenizer_transformer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "print(\"\\n--- Evaluating Pre-trained Transformer Model ---\")\n",
    "def evaluate_transformer(texts, labels, pipeline_model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for text, true_label in zip(texts, labels):\n",
    "        # The pipeline handles tokenization and inference internally\n",
    "        result = pipeline_model(text)[0]\n",
    "        predicted_sentiment = result['label']\n",
    "\n",
    "        # Map 'POSITIVE' to 1, 'NEGATIVE' to 0 for comparison\n",
    "        predicted_class = 1 if predicted_sentiment == 'POSITIVE' else 0\n",
    "\n",
    "        if predicted_class == true_label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    inference_time = time.time() - start_time\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Transformer Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Transformer Inference time: {inference_time:.2f} seconds\")\n",
    "\n",
    "    return accuracy, inference_time\n",
    "\n",
    "transformer_accuracy, transformer_inference_time = evaluate_transformer(test_texts, test_labels, sentiment_pipeline)\n",
    "\n",
    "# TASK 5: Model Comparison\n",
    "def compare_models():\n",
    "    # Accuracy comparison\n",
    "    models = ['LSTM', 'Transformer']\n",
    "    accuracies = [lstm_accuracy, transformer_accuracy]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(models, accuracies, color=['blue', 'orange'])\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.ylim(0, 100)\n",
    "    for i, v in enumerate(accuracies):\n",
    "        plt.text(i, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "    plt.savefig('accuracy_comparison_simplified.png')\n",
    "    plt.close() # Close plot to prevent display issues in some environments\n",
    "\n",
    "    # Inference time comparison\n",
    "    times = [lstm_inference_time, transformer_inference_time]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(models, times, color=['blue', 'orange'])\n",
    "    plt.title('Inference Time Comparison')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    for i, v in enumerate(times):\n",
    "        plt.text(i, v + 0.1, f\"{v:.2f}s\", ha='center')\n",
    "    plt.savefig('time_comparison_simplified.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Print comparison table\n",
    "    print(\"\\n--- Model Comparison Summary ---\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Metric':<20} | {'LSTM':<15} | {'Transformer':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Accuracy':<20} | {lstm_accuracy:<15.2f}% | {transformer_accuracy:<15.2f}%\")\n",
    "    print(f\"{'Inference Time':<20} | {lstm_inference_time:<15.2f}s | {transformer_inference_time:<15.2f}s\")\n",
    "    print(f\"{'Training Time':<20} | {lstm_train_time:<15.2f}s | {'N/A (pre-trained)':<15}\")\n",
    "\n",
    "    # Parameter count\n",
    "    lstm_params = sum(p.numel() for p in lstm_model.parameters())\n",
    "    transformer_params = sum(p.numel() for p in model_transformer.parameters())\n",
    "    print(f\"{'Parameters':<20} | {lstm_params:<15,d} | {transformer_params:<15,d}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "compare_models()\n",
    "\n",
    "# Specific example comparison\n",
    "def compare_specific_examples():\n",
    "    sample_texts = [\n",
    "        \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "        \"The film was neither good nor bad, just mediocre overall.\",\n",
    "        \"What a terrible waste of time and money. Worst movie ever.\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Example Predictions ---\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Text':<40} | {'LSTM Prediction':<20} | {'Transformer Prediction':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    lstm_model.eval()\n",
    "    for text in sample_texts:\n",
    "        # LSTM prediction\n",
    "        processed = process_text_hf(text, tokenizer, max_len).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            lstm_output = torch.sigmoid(lstm_model(processed)).item()\n",
    "        lstm_sentiment = \"Positive\" if lstm_output > 0.5 else \"Negative\"\n",
    "        lstm_confidence = max(lstm_output, 1 - lstm_output) * 100\n",
    "\n",
    "        # Transformer prediction using the pipeline\n",
    "        transformer_result = sentiment_pipeline(text)[0]\n",
    "        transformer_sentiment = transformer_result['label']\n",
    "        transformer_confidence = transformer_result['score'] * 100\n",
    "\n",
    "        print(f\"{text[:37] + '...':<40} | {lstm_sentiment} ({lstm_confidence:.1f}%) | {transformer_sentiment} ({transformer_confidence:.1f}%)\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "compare_specific_examples()\n",
    "```    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
