{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Fully Connected Layers and Model Summary\n"
      ],
      "metadata": {
        "id": "dTjuuGzSz1yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setting Up PyTorch Environment\n"
      ],
      "metadata": {
        "id": "F1aAHBtOz1pc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHpvW5Uaym7C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Defining Fully Connected Layers in PyTorch\n"
      ],
      "metadata": {
        "id": "naJKagdyz9bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNWithFC(nn.Module):\n",
        "    def __init__(self, input_channels=1, image_height=28, image_width=28, num_classes=10):\n",
        "        super(CNNWithFC, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.image_height = image_height\n",
        "        self.image_width = image_width\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Convolutional layers\n",
        "        # Conv2d(in_channels, out_channels, kernel_size, padding)\n",
        "        self.conv1 = nn.Conv2d(self.input_channels, 32, kernel_size=3, padding=1)\n",
        "        # With kernel_size=3 and padding=1, spatial dimensions (height, width) are preserved.\n",
        "        # Output shape: (batch_size, 32, image_height, image_width)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        # Spatial dimensions are preserved by this convolution as well.\n",
        "        # Input shape to conv2 (after pooling): (batch_size, 32, image_height/2, image_width/2)\n",
        "        # Output shape of conv2: (batch_size, 64, image_height/2, image_width/2)\n",
        "\n",
        "        # Max pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Each pooling operation halves the spatial dimensions.\n",
        "\n",
        "        # Calculate the flattened size of the feature map after conv and pool layers\n",
        "        # Initial dimensions: image_height, image_width\n",
        "        # After conv1 (dims preserved), then pool1 (dims halved):\n",
        "        # height_after_pool1 = image_height // 2\n",
        "        # width_after_pool1 = image_width // 2\n",
        "        # After conv2 (dims preserved), then pool2 (dims halved again):\n",
        "        pooled_height = self.image_height // 2 // 2\n",
        "        pooled_width = self.image_width // 2 // 2\n",
        "\n",
        "        # The number of output channels from the last convolutional layer (conv2) is 64.\n",
        "        self.fc1_in_features = 64 * pooled_height * pooled_width\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.fc1_in_features, 128)\n",
        "        self.fc2 = nn.Linear(128, self.num_classes) # Output layer: num_classes units for classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)    # Apply ReLU activation\n",
        "        x = self.pool(x) # Apply Max Pooling\n",
        "\n",
        "        # Convolutional Block 2\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)    # Apply ReLU activation\n",
        "        x = self.pool(x) # Apply Max Pooling\n",
        "\n",
        "        # Flatten the output from convolutional layers for the fully connected layers\n",
        "        # x.size(0) is the batch size. -1 infers the remaining dimensions.\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully Connected Block\n",
        "        x = F.relu(self.fc1(x)) # Apply ReLU activation\n",
        "        x = self.fc2(x)         # Output raw logits (no activation for CrossEntropyLoss)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3M9lElpIz-Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Visualizing the Model Summary"
      ],
      "metadata": {
        "id": "C-q3b_t70FKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure torchsummary is installed. If not, uncomment and run the next line in a code cell:\n",
        "# !pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "# Define input parameters for the model, matching the example (e.g., MNIST dataset)\n",
        "input_channels = 1    # Grayscale images\n",
        "image_height = 28     # Image height of 28 pixels\n",
        "image_width = 28      # Image width of 28 pixels\n",
        "num_classes = 10      # Number of output classes (e.g., digits 0-9)\n",
        "\n",
        "# Create an instance of the network with specified parameters\n",
        "model = CNNWithFC(input_channels=input_channels,\n",
        "                  image_height=image_height,\n",
        "                  image_width=image_width,\n",
        "                  num_classes=num_classes)\n",
        "\n",
        "# Generate and print the model summary\n",
        "# input_size for torchsummary is (channels, height, width)\n",
        "summary(model, input_size=(input_channels, image_height, image_width))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbGRFW8V0FaH",
        "outputId": "69aaa72f-4656-492d-d264-9923f3c3de65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "         MaxPool2d-2           [-1, 32, 14, 14]               0\n",
            "            Conv2d-3           [-1, 64, 14, 14]          18,496\n",
            "         MaxPool2d-4             [-1, 64, 7, 7]               0\n",
            "            Linear-5                  [-1, 128]         401,536\n",
            "            Linear-6                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 421,642\n",
            "Trainable params: 421,642\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.36\n",
            "Params size (MB): 1.61\n",
            "Estimated Total Size (MB): 1.97\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}