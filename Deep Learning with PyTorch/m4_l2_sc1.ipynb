{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Custom Weight Initialization in PyTorch\n"
      ],
      "metadata": {
        "id": "ebsvlqYBhRCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setting Up the PyTorch Environment\n"
      ],
      "metadata": {
        "id": "YZ9sAy1DhQ7z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26HlLKHBfxI6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# torch.nn.init is commonly imported as init\n",
        "import torch.nn.init as init"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Defining a Simple Neural Network with Custom Initialization\n"
      ],
      "metadata": {
        "id": "Od8ZPyYVhU9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomInitNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feedforward neural network with custom weight initialization.\n",
        "    - fc1 and fc2 (hidden layers) use Kaiming (He) initialization for ReLU activation.\n",
        "    - fc3 (output layer) uses Xavier (Glorot) initialization.\n",
        "    - All biases are initialized to zeros.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(CustomInitNetwork, self).__init__()\n",
        "        # Define the fully connected layers\n",
        "        self.fc1 = nn.Linear(784, 256)  # Input layer (e.g., flattened 28x28 image) to first hidden layer\n",
        "        self.fc2 = nn.Linear(256, 128)  # First hidden layer to second hidden layer\n",
        "        self.fc3 = nn.Linear(128, 10)   # Second hidden layer to output layer (10 classes)\n",
        "\n",
        "        # Apply custom weight initialization immediately after defining layers\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Custom weight initialization method for the network's layers.\n",
        "        Applies Kaiming uniform initialization to hidden layers (fc1, fc2)\n",
        "        and Xavier uniform initialization to the output layer (fc3).\n",
        "        All biases are initialized to zeros.\n",
        "        \"\"\"\n",
        "        # Apply Kaiming (He) initialization for layers followed by ReLU activations\n",
        "        # This is appropriate for self.fc1 and self.fc2\n",
        "        init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
        "        if self.fc1.bias is not None:\n",
        "            init.zeros_(self.fc1.bias)\n",
        "\n",
        "        init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
        "        if self.fc2.bias is not None:\n",
        "            init.zeros_(self.fc2.bias)\n",
        "\n",
        "        # Apply Xavier (Glorot) initialization for the output layer (self.fc3).\n",
        "        # Xavier is often suitable for layers with linear activations or when\n",
        "        # the subsequent activation (like softmax in CrossEntropyLoss) is not ReLU.\n",
        "        init.xavier_uniform_(self.fc3.weight)\n",
        "        if self.fc3.bias is not None:\n",
        "            init.zeros_(self.fc3.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the neural network.\n",
        "        Applies ReLU activation after the hidden layers.\n",
        "        The output layer produces raw logits, as CrossEntropyLoss\n",
        "        will internally apply softmax.\n",
        "        \"\"\"\n",
        "        # Flatten the input tensor (e.g., from [batch_size, channels, height, width]\n",
        "        # to [batch_size, features])\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Pass through the first hidden layer with ReLU activation\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        # Pass through the second hidden layer with ReLU activation\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        # Pass through the output layer (no activation here)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dStXgnD7hX-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Implementing the Initialization in Practice\n"
      ],
      "metadata": {
        "id": "N7BPfqsxhaGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = CustomInitNetwork()\n",
        "\n",
        "# Create some random input data (e.g., batch of 64, MNIST-like images flattened)\n",
        "# Ensure the model is in evaluation mode if only doing inference,\n",
        "# or training mode if intending to train. For this step, default (train) is fine.\n",
        "input_data = torch.randn(64, 784)\n",
        "output = model(input_data)\n",
        "\n",
        "print(\"Custom initialized network output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwVRW6Jvhdlq",
        "outputId": "8922a9d9-f14d-4ec2-96b8-8d7d2665dd63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom initialized network output shape: torch.Size([64, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Training Models with Custom Initialization\n"
      ],
      "metadata": {
        "id": "Kd-LDE9hhg_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a loss function and an optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Dummy target data for demonstration (64 samples, 10 classes)\n",
        "# In a real scenario, this would come from your dataset\n",
        "dummy_targets = torch.randint(0, 10, (64,))\n",
        "\n",
        "# Simple training loop\n",
        "num_epochs = 20\n",
        "model.train() # Set the model to training mode\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "    outputs = model(input_data)  # Forward pass\n",
        "    loss = criterion(outputs, dummy_targets)  # Compute loss\n",
        "    loss.backward()  # Backward pass (compute gradients)\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCEy2Eb7hiOz",
        "outputId": "d5d31cf4-bb46-413b-ef8f-1d01c248ec75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 2.2939\n",
            "Epoch [10/20], Loss: 1.6650\n",
            "Epoch [15/20], Loss: 1.2325\n",
            "Epoch [20/20], Loss: 0.9281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pmu2AVoY4Smo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}