{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d69d23",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Train an MLP for Handwritten Digit Classification\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you will build and train a Multilayer Perceptron (MLP) neural network to classify handwritten digits using the MNIST dataset. This practical application of deep learning represents a fundamental computer vision task that's widely used in real-world applications like postal mail sorting, form-digitization, and document processing. By the end of this lab, you'll have created a complete machine learning pipeline from data loading to model evaluation.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Construct a custom MLP using PyTorch's neural network modules\n",
    "- Implement a complete training and evaluation workflow for neural networks\n",
    "- Visualize and interpret model performance metrics\n",
    "- Apply optimization techniques to improve neural network performance\n",
    "\n",
    "## üöÄ Starting Point\n",
    "Required tools/setup:\n",
    "   - Python 3.6+\n",
    "   - PyTorch\n",
    "   - torchvision\n",
    "   - matplotlib\n",
    "   - numpy\n",
    "\n",
    "Access the starter code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f91580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# TODO: Complete the lab implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43507fa5",
   "metadata": {},
   "source": [
    "## Task 1: Data Loading and Preprocessing\n",
    "**Context:** Before training any machine learning model, we need properly formatted data. For image classification tasks like MNIST digit recognition, we need to load the images, normalize the pixel values, and organize them into batches for efficient training.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define a transformation pipeline to convert images to tensors and normalize the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a4ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a transform composition that converts images to tensors \n",
    "# and normalizes them with mean=0.5 and std=0.5\n",
    "# Hint: Use transforms.Compose with ToTensor() and Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1e2a9",
   "metadata": {},
   "source": [
    "2. Load the MNIST training and test datasets using torchvision's datasets module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed019ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load MNIST training and test data \n",
    "# Set appropriate parameters for:\n",
    "# - data directory location\n",
    "# - train/test split\n",
    "# - downloading if needed\n",
    "# - applying transforms\n",
    "# Hint: Use datasets.MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c98bfe",
   "metadata": {},
   "source": [
    "3. Create data loaders for efficient batch processing during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dda44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create DataLoader objects for training and testing\n",
    "# Consider appropriate batch sizes and whether to shuffle the data\n",
    "# Hint: Use torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2de7ca",
   "metadata": {},
   "source": [
    "4. Visualize sample images from the dataset to verify the loading process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a grid of sample images with their labels\n",
    "# Hint: Use matplotlib's subplot feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c3eed",
   "metadata": {},
   "source": [
    "**üí° Tip:** Normalizing image data to the range [-1, 1] or [0, 1] helps neural networks train more effectively by keeping inputs within a consistent range.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Print the shapes of your datasets and verify they match expected MNIST dimensions\n",
    "- Display a few sample images to ensure they look like handwritten digits\n",
    "- Confirm your DataLoader objects return properly formatted batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee9220",
   "metadata": {},
   "source": [
    "## Task 2: Define a Custom MLP Model\n",
    "**Context:** The architecture of a neural network determines its ability to learn patterns. For digit recognition, we'll create a Multilayer Perceptron that can learn to identify key features in handwritten digit images.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a class that inherits from nn.Module to define our network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d97ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define an MLP class that inherits from nn.Module\n",
    "# Hint: Remember to implement both __init__ and forward methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087fe24",
   "metadata": {},
   "source": [
    "2. Initialize the network with appropriate layer dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: In the __init__ method:\n",
    "# - Create fully connected layers with appropriate input/output dimensions\n",
    "# - Add activation functions like ReLU between layers\n",
    "# - Consider the dimensionality of MNIST images (28x28 pixels)\n",
    "# Hint: Use nn.Linear and nn.ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446eea05",
   "metadata": {},
   "source": [
    "3. Implement the forward method to define how data flows through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: In the forward method:\n",
    "# - Reshape input images into flat vectors\n",
    "# - Pass data through each layer and activation function\n",
    "# - Return the final output\n",
    "# Hint: Use the view method to reshape tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23275919",
   "metadata": {},
   "source": [
    "4. Initialize an instance of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an instance of your MLP class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f2e95",
   "metadata": {},
   "source": [
    "**üí° Tip:** When deciding layer sizes, consider the complexity of the task. For MNIST, a common architecture uses a hidden layer with 128 nodes followed by another with 64 nodes before the output layer of 10 nodes (one for each digit).\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Print your model's structure to verify the architecture\n",
    "- Pass a sample batch through the model to verify output dimensions\n",
    "- Expected output shape should be [batch_size, 10] where 10 represents the possible digits (0-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9baeed",
   "metadata": {},
   "source": [
    "## Task 3: Training Loop Implementation\n",
    "**Context:** Training neural networks requires repeatedly showing examples to the model, calculating loss, and updating weights. This iterative process helps the model gradually learn patterns in the data.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a loss function appropriate for classification\n",
    "# Hint: CrossEntropyLoss works well for multi-class classification\n",
    "\n",
    "# TODO: Create an optimizer to update model parameters\n",
    "# Hint: Adam optimizer typically works well; consider a learning rate around 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8131c6b",
   "metadata": {},
   "source": [
    "2. Implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee912f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up variables to track training progress\n",
    "# - Number of epochs\n",
    "# - Lists to store metrics for visualization\n",
    "\n",
    "# TODO: Implement the training loop:\n",
    "# - Iterate through epochs\n",
    "# - For each batch:\n",
    "#   * Zero gradients\n",
    "#   * Forward pass\n",
    "#   * Calculate loss\n",
    "#   * Backward pass\n",
    "#   * Update weights\n",
    "# - Track and print metrics\n",
    "# Hint: Use optimizer.zero_grad(), loss.backward(), and optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791f1cb0",
   "metadata": {},
   "source": [
    "3. Track and store training metrics for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Record loss values and accuracy for each epoch\n",
    "# Hint: Store values in lists for later visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db0991",
   "metadata": {},
   "source": [
    "**üí° Tip:** Print progress updates during training to monitor how your model is performing. Sudden spikes in loss may indicate issues with the learning rate or batch normalization.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Verify that loss decreases over epochs\n",
    "- Check for any NaN values in loss (indicates numerical problems)\n",
    "- Monitor training time to ensure the process is proceeding efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea228e3f",
   "metadata": {},
   "source": [
    "## Task 4: Model Evaluation and Visualization\n",
    "**Context:** After training, we need to evaluate our model on unseen data to understand its true performance. Visualizations help us interpret results and identify where the model succeeds or fails.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Implement model evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b298de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the trained model on test data:\n",
    "# - Set model to evaluation mode\n",
    "# - Iterate through test batches\n",
    "# - Make predictions and compare with true labels\n",
    "# - Calculate accuracy and other relevant metrics\n",
    "# Hint: Use torch.no_grad() for efficient inference, model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83d817",
   "metadata": {},
   "source": [
    "2. Visualize sample predictions to qualitatively assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b06b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display a grid of test images alongside:\n",
    "# - True labels\n",
    "# - Predicted labels\n",
    "# - Whether the prediction was correct (e.g., using color)\n",
    "# Hint: Use matplotlib's subplots and imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82efa6",
   "metadata": {},
   "source": [
    "3. Plot training metrics to visualize learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272568b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create plots showing:\n",
    "# - Training loss over epochs\n",
    "# - Accuracy over epochs\n",
    "# Hint: Use matplotlib's plot function\n",
    "# Plot training and testing loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb9589",
   "metadata": {},
   "source": [
    "4. Analyze model performance on different digit classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b124d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate and visualize per-class accuracy\n",
    "# - Create a confusion matrix\n",
    "# - Plot per-digit accuracy\n",
    "# Hint: Use sklearn's confusion_matrix and a heatmap visualization\n",
    "# Visualize predictions\n",
    "# Generate confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536b62a",
   "metadata": {},
   "source": [
    "**üí° Tip:** A confusion matrix is a powerful tool for identifying which digits are commonly misclassified as others, helping you understand the model's strengths and weaknesses.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Verify overall accuracy is reasonable (>95% is achievable with a basic MLP on MNIST)\n",
    "- Confirm per-class metrics sum correctly\n",
    "- Check that visualizations clearly show model performance trends\n",
    "\n",
    "## ‚úÖ Success Checklist\n",
    "- Dataset successfully loaded and preprocessed with normalization\n",
    "- MLP model architecture implemented with appropriate layers and activations\n",
    "- Training loop executes without errors and loss decreases over time\n",
    "- Model achieves at least 95% accuracy on the test dataset\n",
    "- Training metrics are properly visualized\n",
    "- Sample predictions are displayed with correct and incorrect results identified\n",
    "- Per-class performance analysis completed\n",
    "\n",
    "## üîç Common Issues & Solutions\n",
    "**Problem:** Model accuracy is too low\n",
    "**Solution:** Check normalizing values, increase model capacity (add more neurons or layers), train for more epochs, or try a different optimizer.\n",
    "\n",
    "**Problem:** Loss isn't decreasing during training\n",
    "**Solution:** Verify learning rate (may be too high or too low), check for proper data normalization, ensure proper model initialization.\n",
    "\n",
    "**Problem:** Memory errors during training\n",
    "**Solution:** Reduce batch size, simplify model architecture, or use CPU instead of GPU if memory is limited.\n",
    "\n",
    "**Problem:** Overfitting (high training accuracy, low test accuracy)\n",
    "**Solution:** Add dropout layers, implement early stopping, or use a simpler model architecture.\n",
    "\n",
    "## üîë Key Points\n",
    "- Data preprocessing is critical for neural network performance\n",
    "- MLP architecture design involves trade-offs between model complexity and generalization\n",
    "- Monitoring training metrics helps diagnose issues early\n",
    "- Visualization is essential for understanding model behavior and performance\n",
    "- MNIST is an excellent starting dataset, but real-world image tasks often require more complex architectures like CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af174e7",
   "metadata": {},
   "source": [
    "## üíª Reference Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253799e1",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see a reference solution</strong></summary>    \n",
    "\n",
    "```python\n",
    "# Task 1: Data Loading and Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Visualize some training examples\n",
    "plt.figure(figsize=(10, 5))\n",
    "examples = iter(train_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "    plt.title(f\"Label: {example_targets[i]}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Task 2: Define MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = MLP()\n",
    "\n",
    "# Task 3: Training Loop Implementation\n",
    "# Define loss function and optimizer    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "# Training Loop\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = []\n",
    "accuracy_list = []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx * 64) + ((epoch - 1) * len(train_loader.dataset)))\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            all_preds.extend(pred.view(-1).tolist())\n",
    "            all_targets.extend(target.tolist())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    \n",
    "    test_losses.append(test_loss)\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "    return test_loss, accuracy, all_preds, all_targets\n",
    "\n",
    "# Task 4: Model Evaluation and Visualization\n",
    "for epoch in range(1, epochs + 1):\n",
    "    avg_loss = train(epoch)\n",
    "    test_loss, accuracy, preds, targets = test()\n",
    "    \n",
    "# Visualize predictions\n",
    "def visualize_predictions():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data, target = next(iter(test_loader))\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True).view(-1)\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        for idx in range(6):\n",
    "            ax = fig.add_subplot(2, 3, idx+1)\n",
    "            ax.imshow(data[idx][0], cmap='gray')\n",
    "            color = 'green' if pred[idx] == target[idx] else 'red'\n",
    "            ax.set_title(f\"True: {target[idx]} Pred: {pred[idx]}\", color=color)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_predictions()\n",
    "\n",
    "# Plot training and testing loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.xlabel('Number of training examples seen')\n",
    "plt.ylabel('Negative log likelihood loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy progression\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), accuracy_list, color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Test Accuracy Progression')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Generate confusion matrix\n",
    "def plot_confusion_matrix(preds, targets):\n",
    "    cm = confusion_matrix(targets, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_accuracy = 100 * np.diag(cm) / np.sum(cm, axis=1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(10), per_class_accuracy)\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Per-Class Accuracy')\n",
    "    plt.xticks(range(10))\n",
    "    plt.ylim(0, 100)\n",
    "    plt.show()\n",
    "\n",
    "_, _, all_preds, all_targets = test()\n",
    "plot_confusion_matrix(all_preds, all_targets)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
