{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08cf9109",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Optimize Your Training Pipeline with Efficiency Tricks\n",
    "\n",
    "## üìã Overview\n",
    "In this activity, you'll enhance the training efficiency and stability of a Convolutional Neural Network on the CIFAR-10 dataset by integrating advanced techniques such as gradient clipping, learning rate scheduling, and mixed precision training. These optimizations are crucial in real-world machine learning scenarios where computational resources and time are often limited, but model performance cannot be sacrificed.\n",
    "\n",
    "This activity ties into practical scenarios where you might need to improve model training efficiency and reliability, essential skills for machine learning engineers and data scientists.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Integrate gradient clipping into training pipelines to manage updates and prevent instability.\n",
    "- Apply learning rate scheduling to enhance model convergence rates and accuracy.\n",
    "- Use mixed precision training to reduce training time and improve resource utilization efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865c742",
   "metadata": {},
   "source": [
    "## Task 1: Baseline Model Training\n",
    "**Context:** Start by setting up a baseline training for your CNN model on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and validation datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "val_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Simple CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        pass\n",
    "\n",
    "# Training function\n",
    "def train_baseline(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    # TODO: Implement the baseline training loop\n",
    "    # 1. Record start time\n",
    "    # 2. Initialize lists to store metrics\n",
    "    # 3. Loop through epochs\n",
    "    # 4. Loop through batches\n",
    "    # 5. Forward pass\n",
    "    # 6. Calculate loss\n",
    "    # 7. Backward pass\n",
    "    # 8. Update weights\n",
    "    # 9. Record metrics\n",
    "    # 10. Calculate epoch statistics\n",
    "    pass\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion):\n",
    "    # TODO: Implement the validation function\n",
    "    # 1. Set model to evaluation mode\n",
    "    # 2. Disable gradient computation\n",
    "    # 3. Calculate validation loss and accuracy\n",
    "    pass\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the baseline model\n",
    "num_epochs = 5 # or more when trying out iterations\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, training_time = train_baseline(\n",
    "    model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Plot results\n",
    "# TODO: Plot training and validation losses and accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3c164",
   "metadata": {},
   "source": [
    "## üîç Practice\n",
    "1. Complete the `forward` method in the `SimpleCNN` class.\n",
    "2. Implement the `train_baseline` function to train the model for `num_epochs`:\n",
    "    - Use `model.train()` to set the training mode\n",
    "    - Utilize `torch.no_grad()` for validation\n",
    "    - Track time with `time.time()`\n",
    "    - Calculate accuracy with `torch.max(outputs, 1)[1].eq(labels).sum().item()`\n",
    "3. Implement the validate function to evaluate model performance.\n",
    "4. Add code to plot the training/validation loss and accuracy curves.\n",
    "    \n",
    "## ‚úÖ Success Checklist\n",
    "- Baseline model is trained without any optimizations.\n",
    "- Loss curve and training time are recorded.\n",
    "- Model achieves reasonable accuracy on the validation set.\n",
    "\n",
    "## üí° Key Points\n",
    "- Monitoring baseline performance allows clear comparisons when implementing optimizations.\n",
    "- Understanding default training dynamics is foundational for troubleshooting enhancements.\n",
    "- Pay attention to the learning rate, batch size, and number of epochs as they affect convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9275951d",
   "metadata": {},
   "source": [
    "## Task 2: Applying Gradient Clipping\n",
    "**Context:** Enhance training stability by incorporating gradient clipping into your model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_gradient_clipping(model, train_loader, criterion, optimizer, num_epochs=5, max_norm=1.0):\n",
    "    # TODO: Implement training with gradient clipping\n",
    "    # 1. Record start time\n",
    "    # 2. Initialize lists to store metrics\n",
    "    # 3. Loop through epochs\n",
    "    # 4. Loop through batches\n",
    "    # 5. Forward pass\n",
    "    # 6. Calculate loss\n",
    "    # 7. Backward pass\n",
    "    # 8. Apply gradient clipping with torch.nn.utils.clip_grad_norm_\n",
    "    # 9. Update weights\n",
    "    # 10. Record metrics\n",
    "    # 11. Calculate epoch statistics\n",
    "    pass\n",
    "\n",
    "# Initialize new model and optimizer for gradient clipping experiment\n",
    "model_gc = SimpleCNN().to(device)\n",
    "optimizer_gc = optim.SGD(model_gc.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train with gradient clipping\n",
    "max_norm = 1.0  # Try different values like 0.5, 1.0, 2.0\n",
    "train_losses_gc, val_losses_gc, train_accuracies_gc, val_accuracies_gc, training_time_gc = train_with_gradient_clipping(\n",
    "    model_gc, train_loader, criterion, optimizer_gc, num_epochs, max_norm)\n",
    "\n",
    "# Plot and compare results\n",
    "# TODO: Plot and compare results with baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f9913",
   "metadata": {},
   "source": [
    "## üîç Practice\n",
    "1. Implement the `train_with_gradient_clipping` function:\n",
    "    - Copy your baseline training loop\n",
    "    - Add `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)` after `loss.backward()`\n",
    "    - Experiment with different `max_norm `values (0.5, 1.0, 2.0)\n",
    "2. Compare the training stability and convergence with the baseline.\n",
    "3. Visualize the gradient norms during training using a histogram or line plot.\n",
    "\n",
    "## ‚úÖ Success Checklist\n",
    "- Model training includes gradient clipping.\n",
    "- Training is more stable with fewer spikes in the loss curve.\n",
    "- The effect of different `max_norm` values is documented.\n",
    "\n",
    "## ‚ùó Common Mistakes\n",
    "- Setting `max_norm` too small can slow down learning.\n",
    "- Setting `max_norm` too large might not provide enough constraint on exploding gradients.\n",
    "- Applying gradient clipping after optimizer step instead of before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d02ebd3",
   "metadata": {},
   "source": [
    "## Task 3: Implementing Learning Rate Scheduling\n",
    "**Context:** Use dynamic learning rates to potentially enhance convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr_scheduler(model, train_loader, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    # TODO: Implement training with learning rate scheduling\n",
    "    # 1. Record start time\n",
    "    # 2. Initialize lists to store metrics\n",
    "    # 3. Loop through epochs\n",
    "    # 4. Loop through batches\n",
    "    # 5. Forward pass\n",
    "    # 6. Calculate loss\n",
    "    # 7. Backward pass\n",
    "    # 8. Update weights\n",
    "    # 9. Record metrics\n",
    "    # 10. Step the scheduler (based on scheduler type)\n",
    "    # 11. Calculate epoch statistics\n",
    "    pass\n",
    "\n",
    "# Initialize model and optimizer for learning rate scheduling\n",
    "model_lr = SimpleCNN().to(device)\n",
    "optimizer_lr = optim.SGD(model_lr.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Create a learning rate scheduler\n",
    "# Try different schedulers:\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer_lr, step_size=2, gamma=0.5)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_lr, mode='min', factor=0.5, patience=1)\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer_lr, T_max=num_epochs)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_lr, step_size=2, gamma=0.5)\n",
    "\n",
    "# Train with learning rate scheduling\n",
    "train_losses_lr, val_losses_lr, train_accuracies_lr, val_accuracies_lr, training_time_lr, lr_history = train_with_lr_scheduler(\n",
    "    model_lr, train_loader, criterion, optimizer_lr, scheduler, num_epochs)\n",
    "\n",
    "# Plot and compare results\n",
    "# TODO: Plot learning rate changes and compare results with baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e8442e",
   "metadata": {},
   "source": [
    "## üîç Practice\n",
    "1. Implement the `train_with_lr_scheduler` function:\n",
    "    - Copy your baseline training loop\n",
    "    - Add scheduler stepping logic (`scheduler.step()`) at the appropriate point:\n",
    "        - For epoch-based schedulers like `StepLR` or `CosineAnnealingLR`: call `scheduler.step()` at the end of each epoch\n",
    "        - For metric-based schedulers like `ReduceLROnPlateau`: call `scheduler.step(val_loss)` after validation\n",
    "    - Track the learning rate history using `optimizer.param_groups[0]['lr']`\n",
    "2. Experiment with different scheduler types:\n",
    "    - `StepLR`: Decreases the learning rate by a factor gamma every step_size epochs\n",
    "    - `ReduceLROnPlateau`: Reduces learning rate when a metric stops improving\n",
    "    - `CosineAnnealingLR`: Uses a cosine annealing schedule\n",
    "3. Plot the learning rate over time and compare convergence with the baseline.\n",
    "\n",
    "## ‚úÖ Success Checklist\n",
    "- Learning rate scheduler is correctly implemented.\n",
    "- Learning rate changes are visualized throughout training.\n",
    "- Convergence improvements are observed compared to baseline.\n",
    "\n",
    "## üí° Key Points\n",
    "- Different schedulers work best for different problems.\n",
    "- Learning rate scheduling can help escape local minima and reach better solutions.\n",
    "- Monitoring the learning rate alongside loss helps understand training dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021fce1a",
   "metadata": {},
   "source": [
    "## Task 4: Experimenting with Mixed Precision \n",
    "**Context:** Speed up your training using mixed precision with PyTorch's `torch.cuda.amp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_with_mixed_precision(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    # TODO: Implement training with mixed precision\n",
    "    # 1. Create a GradScaler\n",
    "    # 2. Record start time\n",
    "    # 3. Initialize lists to store metrics\n",
    "    # 4. Loop through epochs\n",
    "    # 5. Loop through batches\n",
    "    # 6. Forward pass with autocast\n",
    "    # 7. Calculate loss\n",
    "    # 8. Scale loss and backpropagate\n",
    "    # 9. Unscale before gradient clipping (optional)\n",
    "    # 10. Scale optimizer's step\n",
    "    # 11. Update weights\n",
    "    # 12. Record metrics\n",
    "    # 13. Calculate epoch statistics\n",
    "    pass\n",
    "\n",
    "# Check if GPU supports mixed precision\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"CUDA not available. Mixed precision requires CUDA.\")\n",
    "else:\n",
    "    # Initialize model and optimizer for mixed precision\n",
    "    model_mp = SimpleCNN().to(device)\n",
    "    optimizer_mp = optim.SGD(model_mp.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    # Train with mixed precision\n",
    "    train_losses_mp, val_losses_mp, train_accuracies_mp, val_accuracies_mp, training_time_mp = train_with_mixed_precision(\n",
    "        model_mp, train_loader, criterion, optimizer_mp, num_epochs)\n",
    "    \n",
    "    # Plot and compare results\n",
    "    # TODO: Plot and compare results with baseline, especially training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d2a2c",
   "metadata": {},
   "source": [
    "## üîç Practice\n",
    "1. Implement the `train_with_mixed_precision` function:\n",
    "    - Create a `GradScaler` instance: `scaler = GradScaler()`\n",
    "    - Wrap the forward pass in `with autocast():`\n",
    "    - Scale the loss: `scaler.scale(loss).backward()`\n",
    "    - If using gradient clipping, unscale first: `scaler.unscale_(optimizer)`\n",
    "    - Update weights with scaled gradients: `scaler.step(optimizer)`\n",
    "    - Update scaler state: `scaler.update()`\n",
    "2. Track memory usage with `torch.cuda.max_memory_allocated() / (1024**2)` (in MB).\n",
    "3. Compare training time and resource usage with the baseline.\n",
    "\n",
    "## ‚úÖ Success Checklist\n",
    "- Mixed precision implementation is successful.\n",
    "- Training time is reduced compared to baseline.\n",
    "- Memory usage is reduced without degrading accuracy.\n",
    "\n",
    "## ‚ùó Common Mistakes\n",
    "- Not using `scaler.update()` after each optimizer step.\n",
    "- Using mixed precision on hardware that doesn't support it (older GPUs).\n",
    "- Not handling `inf/NaN` values that might occur more frequently with mixed precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b28d47",
   "metadata": {},
   "source": [
    "## Task 5: Analysis and Visualization\n",
    "**Context:** Visualize and compare outcomes of all optimizations for comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(metrics_dict, metric_name, title):\n",
    "    \"\"\"\n",
    "    Plot comparison of different training configurations.\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict: Dictionary with configuration names as keys and metric lists as values\n",
    "        metric_name: Name of the metric being plotted (for axis label)\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # TODO: Implement comparison plotting\n",
    "    # 1. Create figure and axis\n",
    "    # 2. Plot each configuration's metric\n",
    "    # 3. Add legend, labels, and title\n",
    "    # 4. Display or save the plot\n",
    "    pass\n",
    "\n",
    "# Prepare data for comparisons\n",
    "training_times = {\n",
    "    'Baseline': training_time,\n",
    "    'Gradient Clipping': training_time_gc,\n",
    "    'LR Scheduling': training_time_lr,\n",
    "    'Mixed Precision': training_time_mp\n",
    "}\n",
    "\n",
    "# Organize metrics for plotting\n",
    "train_losses_dict = {\n",
    "    'Baseline': train_losses,\n",
    "    'Gradient Clipping': train_losses_gc,\n",
    "    'LR Scheduling': train_losses_lr,\n",
    "    'Mixed Precision': train_losses_mp\n",
    "}\n",
    "\n",
    "val_accuracies_dict = {\n",
    "    'Baseline': val_accuracies,\n",
    "    'Gradient Clipping': val_accuracies_gc,\n",
    "    'LR Scheduling': val_accuracies_lr,\n",
    "    'Mixed Precision': val_accuracies_mp\n",
    "}\n",
    "\n",
    "# Plot comparisons\n",
    "# TODO: Create comparison plots for training time, loss curves, and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417f1c7",
   "metadata": {},
   "source": [
    "## üîç Practice\n",
    "1. Implement the `plot_comparison` function using `matplotlib`:\n",
    "    - Use different colors and markers for each configuration\n",
    "    - Include a legend to identify each configuration\n",
    "    - Add clear axis labels and an informative title\n",
    "2. Create comparison plots for:\n",
    "    - Training time (bar chart)\n",
    "    - Training loss curves (line chart)\n",
    "    - Validation accuracy (line chart)\n",
    "3. Analyze which optimization(s) provided the most benefits and why.\n",
    "\n",
    "## ‚úÖ Success Checklist\n",
    "- Visual comparisons clearly show differences between configurations.\n",
    "- Analysis identifies the most effective optimizations for this scenario.\n",
    "-  Time and resource trade-offs are discussed.\n",
    "\n",
    "## üí° Key Points\n",
    "- Different optimizations may affect different aspects of training.\n",
    "- The best approach often combines multiple techniques.\n",
    "- Consider the computational cost vs. performance improvement trade-off.\n",
    "\n",
    "## ‚ùó Common Mistakes to Avoid\n",
    "- Comparing implementations with different random seeds, causing unfair comparisons.\n",
    "- Overlooking differences in training dynamics between CPU/GPU setups.\n",
    "- Misconfiguring gradient clipping or schedulers leading to suboptimal results.\n",
    "- Not accounting for overhead when measuring training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19492e",
   "metadata": {},
   "source": [
    "## üíª Referance Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86a8c3",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><strong>Click HERE to see a reference solution</strong></summary>    \n",
    "    \n",
    "```python\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and validation datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "val_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Simple CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Validation function used by all training methods\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return val_loss / len(val_loader), 100 * correct / total\n",
    "\n",
    "# Training function for baseline\n",
    "def train_baseline(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    device = next(model.parameters()).device\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {epoch_loss:.4f}, '\n",
    "              f'Train Acc: {epoch_acc:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Training time: {training_time:.2f} seconds')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, training_time\n",
    "\n",
    "# Training function with gradient clipping\n",
    "def train_with_gradient_clipping(model, train_loader, criterion, optimizer, num_epochs=5, max_norm=1.0):\n",
    "    device = next(model.parameters()).device\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            \n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {epoch_loss:.4f}, '\n",
    "              f'Train Acc: {epoch_acc:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Training time with gradient clipping: {training_time:.2f} seconds')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, training_time\n",
    "\n",
    "# Training function with learning rate scheduling\n",
    "def train_with_lr_scheduler(model, train_loader, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    device = next(model.parameters()).device\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    lr_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Record current learning rate\n",
    "        lr_history.append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Step the scheduler\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}, '\n",
    "              f'Train Loss: {epoch_loss:.4f}, '\n",
    "              f'Train Acc: {epoch_acc:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Training time with LR scheduling: {training_time:.2f} seconds')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, training_time, lr_history\n",
    "\n",
    "# Training function with mixed precision\n",
    "def train_with_mixed_precision(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    device = next(model.parameters()).device\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Create GradScaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize with scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Optionally unscale gradients for clipping\n",
    "            # scaler.unscale_(optimizer)\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            \n",
    "            # Step with scaler\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Track statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        # Validate (no need for mixed precision in evaluation)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {epoch_loss:.4f}, '\n",
    "              f'Train Acc: {epoch_acc:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    max_memory = torch.cuda.max_memory_allocated() / (1024**2) if torch.cuda.is_available() else 0\n",
    "    print(f'Training time with mixed precision: {training_time:.2f} seconds')\n",
    "    print(f'Maximum GPU memory used: {max_memory:.2f} MB')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, training_time\n",
    "\n",
    "# Plotting function for comparison\n",
    "def plot_comparison(metrics_dict, metric_name, title, xlabel='Epoch', ylabel=None, figsize=(10, 6)):\n",
    "    if ylabel is None:\n",
    "        ylabel = metric_name\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    for name, metric in metrics_dict.items():\n",
    "        plt.plot(range(1, len(metric) + 1), metric, marker='o', label=name)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set parameters\n",
    "    num_epochs = 10 # Increased epochs for better observation\n",
    "    \n",
    "    # --- 1. Baseline model ---\n",
    "    print(\"\\n=== Training Baseline Model ===\")\n",
    "    model_baseline = SimpleCNN().to(device)\n",
    "    criterion_baseline = nn.CrossEntropyLoss()\n",
    "    optimizer_baseline = optim.SGD(model_baseline.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    (baseline_train_losses, baseline_val_losses,\n",
    "     baseline_train_accuracies, baseline_val_accuracies,\n",
    "     baseline_time) = train_baseline(\n",
    "        model_baseline, train_loader, criterion_baseline, optimizer_baseline, num_epochs)\n",
    "    \n",
    "    # --- 2. Model with gradient clipping ---\n",
    "    print(\"\\n=== Training Model with Gradient Clipping ===\")\n",
    "    model_gc = SimpleCNN().to(device)\n",
    "    criterion_gc = nn.CrossEntropyLoss()\n",
    "    optimizer_gc = optim.SGD(model_gc.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    (gc_train_losses, gc_val_losses,\n",
    "     gc_train_accuracies, gc_val_accuracies,\n",
    "     gc_time) = train_with_gradient_clipping(\n",
    "        model_gc, train_loader, criterion_gc, optimizer_gc, num_epochs, max_norm=1.0)\n",
    "    \n",
    "    # --- 3. Model with learning rate scheduling ---\n",
    "    print(\"\\n=== Training Model with Learning Rate Scheduling ===\")\n",
    "    model_lr = SimpleCNN().to(device)\n",
    "    criterion_lr = nn.CrossEntropyLoss()\n",
    "    optimizer_lr = optim.SGD(model_lr.parameters(), lr=0.01, momentum=0.9)\n",
    "    # Example: ReduceLROnPlateau scheduler\n",
    "    scheduler_lr = optim.lr_scheduler.ReduceLROnPlateau(optimizer_lr, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "    \n",
    "    (lr_train_losses, lr_val_losses,\n",
    "     lr_train_accuracies, lr_val_accuracies,\n",
    "     lr_time, lr_history) = train_with_lr_scheduler(\n",
    "        model_lr, train_loader, criterion_lr, optimizer_lr, scheduler_lr, num_epochs)\n",
    "\n",
    "    # --- 4. Model with Mixed Precision Training ---\n",
    "    print(\"\\n=== Training Model with Mixed Precision ===\")\n",
    "    # Reset CUDA memory stats for a cleaner measurement\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    model_mp = SimpleCNN().to(device)\n",
    "    criterion_mp = nn.CrossEntropyLoss()\n",
    "    optimizer_mp = optim.SGD(model_mp.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    (mp_train_losses, mp_val_losses,\n",
    "     mp_train_accuracies, mp_val_accuracies,\n",
    "     mp_time) = train_with_mixed_precision(\n",
    "        model_mp, train_loader, criterion_mp, optimizer_mp, num_epochs)\n",
    "\n",
    "    # --- Plotting Results ---\n",
    "    print(\"\\n=== Plotting Results ===\")\n",
    "\n",
    "    # Plot Training Loss\n",
    "    plot_comparison(\n",
    "        {'Baseline': baseline_train_losses,\n",
    "         'Gradient Clipping': gc_train_losses,\n",
    "         'LR Scheduling': lr_train_losses,\n",
    "         'Mixed Precision': mp_train_losses},\n",
    "        'Loss', 'Training Loss Comparison'\n",
    "    )\n",
    "\n",
    "    # Plot Validation Loss\n",
    "    plot_comparison(\n",
    "        {'Baseline': baseline_val_losses,\n",
    "         'Gradient Clipping': gc_val_losses,\n",
    "         'LR Scheduling': lr_val_losses,\n",
    "         'Mixed Precision': mp_val_losses},\n",
    "        'Loss', 'Validation Loss Comparison'\n",
    "    )\n",
    "\n",
    "    # Plot Training Accuracy\n",
    "    plot_comparison(\n",
    "        {'Baseline': baseline_train_accuracies,\n",
    "         'Gradient Clipping': gc_train_accuracies,\n",
    "         'LR Scheduling': lr_train_accuracies,\n",
    "         'Mixed Precision': mp_train_accuracies},\n",
    "        'Accuracy', 'Training Accuracy Comparison'\n",
    "    )\n",
    "\n",
    "    # Plot Validation Accuracy\n",
    "    plot_comparison(\n",
    "        {'Baseline': baseline_val_accuracies,\n",
    "         'Gradient Clipping': gc_val_accuracies,\n",
    "         'LR Scheduling': lr_val_accuracies,\n",
    "         'Mixed Precision': mp_val_accuracies},\n",
    "        'Accuracy', 'Validation Accuracy Comparison'\n",
    "    )\n",
    "\n",
    "    # Plot Learning Rate History for LR Scheduling\n",
    "    plot_comparison(\n",
    "        {'LR Scheduling': lr_history},\n",
    "        'Learning Rate', 'Learning Rate History (LR Scheduling)', ylabel='Learning Rate'\n",
    "    )\n",
    "\n",
    "    # Print summary of training times\n",
    "    print(\"\\n=== Summary of Training Times ===\")\n",
    "    print(f\"Baseline Training Time: {baseline_time:.2f} seconds\")\n",
    "    print(f\"Gradient Clipping Training Time: {gc_time:.2f} seconds\")\n",
    "    print(f\"LR Scheduling Training Time: {lr_time:.2f} seconds\")\n",
    "    print(f\"Mixed Precision Training Time: {mp_time:.2f} seconds\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Mixed Precision Max GPU Memory (Last Run): {torch.cuda.max_memory_allocated() / (1024**2):.2f} MB\")\n",
    "```    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
