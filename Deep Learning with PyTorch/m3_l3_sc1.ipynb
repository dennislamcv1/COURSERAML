{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation with LSTMs in PyTorch\n"
      ],
      "metadata": {
        "id": "HroxYVWN_RW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setting Up the Environment and Configuration\n"
      ],
      "metadata": {
        "id": "vHBg83np_RTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Gar1_PfEyvbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjNpcWTq_Lbp"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "TEXT_DATA = \"hello world this is a simple text for our lstm model\" # Example text data\n",
        "SEQUENCE_LENGTH = 5  # Number of characters the LSTM looks at to predict the next\n",
        "HIDDEN_SIZE = 128    # Number of features in the LSTM's hidden state\n",
        "NUM_LAYERS_LSTM = 1  # Number of stacked LSTM layers\n",
        "NUM_EPOCHS = 200     # Training iterations\n",
        "LEARNING_RATE = 0.001\n",
        "GENERATION_START_TEXT = \"hello\" # The initial text to kick off generation\n",
        "GENERATION_LENGTH = 3 # Number of characters to generate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Preparing Text Data\n"
      ],
      "metadata": {
        "id": "n-ydoKQtast8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sorted list of unique characters to ensure consistent mapping\n",
        "unique_chars = sorted(list(set(TEXT_DATA)))\n",
        "vocab_size = len(unique_chars) # Size of our character vocabulary\n",
        "\n",
        "# Create mappings between characters and their numerical indices\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(unique_chars)}\n",
        "idx_to_char = {idx: ch for idx, ch in enumerate(unique_chars)}\n",
        "\n",
        "# Prepare sequences for training: input sequences and their corresponding next-character targets\n",
        "sequences_idx = []\n",
        "targets_idx = []\n",
        "for i in range(len(TEXT_DATA) - SEQUENCE_LENGTH):\n",
        "    sequences_idx.append([char_to_idx[c] for c in TEXT_DATA[i:i+SEQUENCE_LENGTH]])\n",
        "    targets_idx.append(char_to_idx[TEXT_DATA[i+SEQUENCE_LENGTH]])\n",
        "\n",
        "# Convert sequences to one-hot encoded tensors for the LSTM input\n",
        "# Shape: (number_of_sequences, sequence_length, vocabulary_size)\n",
        "input_sequences_one_hot = torch.zeros(len(sequences_idx), SEQUENCE_LENGTH, vocab_size, dtype=torch.float32)\n",
        "for i, seq in enumerate(sequences_idx):\n",
        "    for j, char_idx in enumerate(seq):\n",
        "        input_sequences_one_hot[i, j, char_idx] = 1.0 # Set the appropriate character index to 1.0\n",
        "\n",
        "# Convert target indices to a tensor\n",
        "target_tensor = torch.tensor(targets_idx, dtype=torch.long)"
      ],
      "metadata": {
        "id": "nHqzYA2cau6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Building the LSTM Model\n"
      ],
      "metadata": {
        "id": "N27zLmhZa3JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGeneratorLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(TextGeneratorLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # The core LSTM layer.\n",
        "        # `input_size` (vocab_size): Dimension of each input character (one-hot vector).\n",
        "        # `hidden_size`: Size of the hidden state.\n",
        "        # `num_layers`: Number of stacked LSTM layers.\n",
        "        # `batch_first=True`: Ensures input/output tensors are (batch, seq, features).\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer to map LSTM's output hidden state to the vocabulary size,\n",
        "        # giving us probabilities for each possible next character.\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hc=None):\n",
        "        # Initialize hidden and cell states with zeros if not provided (for first sequence in a batch)\n",
        "        if hc is None:\n",
        "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "            hc_init = (h0, c0)\n",
        "        else:\n",
        "            hc_init = hc\n",
        "\n",
        "        # Forward pass through the LSTM layer.\n",
        "        # `out` contains the hidden states for each time step in the sequence from the last layer.\n",
        "        # `hidden_state` contains the final (h_n, c_n) for the entire batch.\n",
        "        out, hidden_state = self.lstm(x, hc_init)\n",
        "\n",
        "        # We take the hidden state output of the *last time step* (`out[:, -1, :]`)\n",
        "        # to predict the next character, as it summarizes the entire input sequence.\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out, hidden_state # Return output logits and final hidden state for sequential generation"
      ],
      "metadata": {
        "id": "L1pprnvVa5HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Training the LSTM Model\n"
      ],
      "metadata": {
        "id": "OBaWfms6a-gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = TextGeneratorLSTM(input_size=vocab_size,\n",
        "                          hidden_size=HIDDEN_SIZE,\n",
        "                          output_size=vocab_size, # Output size is vocabulary size for character prediction\n",
        "                          num_layers=NUM_LAYERS_LSTM)\n",
        "model.to(device)\n",
        "\n",
        "# Loss function for multi-class classification (predicting the next character)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Optimizer to update model weights\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train() # Set model to training mode (enables dropout, etc., if defined)\n",
        "\n",
        "    # Move data batch to the selected device\n",
        "    inputs_one_hot_batch = input_sequences_one_hot.to(device)\n",
        "    targets_batch = target_tensor.to(device)\n",
        "\n",
        "    optimizer.zero_grad() # Clear gradients from the previous iteration\n",
        "\n",
        "    # Forward pass: get predictions. For this basic example, we treat each sequence independently.\n",
        "    output_logits, _ = model(inputs_one_hot_batch)\n",
        "\n",
        "    loss = criterion(output_logits, targets_batch) # Calculate the loss\n",
        "    loss.backward() # Backpropagate to compute gradients\n",
        "    optimizer.step() # Update model weights\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.4f}')\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT51qzGma_9Z",
        "outputId": "6abfeae7-de5e-4edc-a68b-1f3e5cdaa3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Starting training for 200 epochs...\n",
            "Epoch [10/200], Loss: 2.7648\n",
            "Epoch [20/200], Loss: 2.6198\n",
            "Epoch [30/200], Loss: 2.5377\n",
            "Epoch [40/200], Loss: 2.4807\n",
            "Epoch [50/200], Loss: 2.3800\n",
            "Epoch [60/200], Loss: 2.1800\n",
            "Epoch [70/200], Loss: 1.8761\n",
            "Epoch [80/200], Loss: 1.4604\n",
            "Epoch [90/200], Loss: 1.0242\n",
            "Epoch [100/200], Loss: 0.6256\n",
            "Epoch [110/200], Loss: 0.3596\n",
            "Epoch [120/200], Loss: 0.2154\n",
            "Epoch [130/200], Loss: 0.1342\n",
            "Epoch [140/200], Loss: 0.0885\n",
            "Epoch [150/200], Loss: 0.0628\n",
            "Epoch [160/200], Loss: 0.0475\n",
            "Epoch [170/200], Loss: 0.0375\n",
            "Epoch [180/200], Loss: 0.0305\n",
            "Epoch [190/200], Loss: 0.0254\n",
            "Epoch [200/200], Loss: 0.0215\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Generating Text\n"
      ],
      "metadata": {
        "id": "thG8c-P2bDte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_text, length, char_to_idx, idx_to_char, sequence_length, vocab_size, device):\n",
        "    model.eval() # Set model to evaluation mode (disables dropout, batch norm, etc.)\n",
        "\n",
        "    # Prepare initial sequence for generation\n",
        "    current_indices = []\n",
        "    # Only use characters from start_text that are in our known vocabulary\n",
        "    for c in start_text:\n",
        "        if c in char_to_idx:\n",
        "            current_indices.append(char_to_idx[c])\n",
        "        else:\n",
        "            print(f\"Warning: Character '{c}' in start_text not found in vocabulary. Skipping.\")\n",
        "\n",
        "    # Handle cases where start_text might be empty or contain no known characters\n",
        "    if not current_indices:\n",
        "        print(f\"Warning: No valid characters in start_text '{start_text}'. Using a default seed.\")\n",
        "        # Fallback: use the first character from vocabulary as a seed\n",
        "        first_char_idx = 0\n",
        "        current_indices = [first_char_idx]\n",
        "        generated_text = idx_to_char[first_char_idx]\n",
        "    else:\n",
        "        generated_text = start_text\n",
        "\n",
        "    # Hidden state to be carried over for stateful generation\n",
        "    # Initialized to None for the first prediction, then updated.\n",
        "    hidden_cell_state = None\n",
        "\n",
        "    with torch.no_grad(): # Crucial for inference: disables gradient calculation to save memory and speed\n",
        "        for _ in range(length):\n",
        "            # Take the *last `sequence_length`* characters as input for the model.\n",
        "            # This simulates a sliding window approach for generation.\n",
        "            input_for_model_indices = current_indices[-sequence_length:]\n",
        "\n",
        "            # Create a one-hot encoded tensor for the current input characters\n",
        "            current_input_one_hot = torch.zeros(1, len(input_for_model_indices), vocab_size, dtype=torch.float32).to(device)\n",
        "            for i, char_idx in enumerate(input_for_model_indices):\n",
        "                current_input_one_hot[0, i, char_idx] = 1.0\n",
        "\n",
        "            # Pass the current input and the *previous* hidden state through the model\n",
        "            # This is key for stateful generation, allowing the model to remember context.\n",
        "            output_logits, hidden_cell_state = model(current_input_one_hot, hidden_cell_state)\n",
        "\n",
        "            # Get the predicted character index (greedy approach: pick the one with highest probability)\n",
        "            _, predicted_idx = torch.max(output_logits, dim=1)\n",
        "            predicted_char_idx = predicted_idx.item()\n",
        "            predicted_char = idx_to_char[predicted_char_idx]\n",
        "\n",
        "            generated_text += predicted_char # Append the new character\n",
        "            current_indices.append(predicted_char_idx) # Add its index to the history for the next iteration\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "print(\"\\nStarting text generation...\")\n",
        "generated_output = generate_text(model=model,\n",
        "                                 start_text=GENERATION_START_TEXT,\n",
        "                                 length=GENERATION_LENGTH,\n",
        "                                 char_to_idx=char_to_idx,\n",
        "                                 idx_to_char=idx_to_char,\n",
        "                                 sequence_length=SEQUENCE_LENGTH,\n",
        "                                 vocab_size=vocab_size,\n",
        "                                 device=device)\n",
        "\n",
        "print(f\"Seed Text: '{GENERATION_START_TEXT}'\")\n",
        "print(f\"Generated Text: '{generated_output}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgKHyIu3bE8-",
        "outputId": "03f0c98f-59ea-4585-f0f4-706cd47d5e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting text generation...\n",
            "Seed Text: 'hello'\n",
            "Generated Text: 'hello hi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oBBNbAzwzXxU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}