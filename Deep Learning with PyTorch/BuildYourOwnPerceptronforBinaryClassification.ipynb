{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03bdebfb",
   "metadata": {},
   "source": [
    "# 👩‍💻 Build Your Own Perceptron for Binary Classification\n",
    "\n",
    "## 📋 Overview\n",
    "In this lab, you will implement a perceptron model from scratch for binary classification using PyTorch tensors. You'll generate synthetic data, train your model using the perceptron learning rule, visualize the decision boundary, and experiment with different learning rates. By building this fundamental neural network component yourself, you'll gain deeper insights into how neural networks learn and make predictions.\n",
    "\n",
    "## 🎯 Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Implement a perceptron model from scratch using PyTorch tensors\n",
    "- Apply the perceptron learning rule to train a binary classifier\n",
    "- Visualize decision boundaries and observe their evolution during training\n",
    "- Analyze how different learning rates affect model convergence and performance\n",
    "\n",
    "## 🚀 Starting Point\n",
    "Access the starter code:\n",
    "\n",
    "-  Use the provided Jupyter notebook\n",
    "\n",
    "Required tools/setup:\n",
    "\n",
    "- Python 3.6+\n",
    "- PyTorch\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- Scikit-learn\n",
    "\n",
    "Make sure you understand:\n",
    "\n",
    "- Basic tensor operations in PyTorch\n",
    "- The perceptron model architecture discussed in previous lessons\n",
    "- How to plot data using matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be0bb2d",
   "metadata": {},
   "source": [
    "## Task 1: Dataset Setup\n",
    "**Context:** Before building a classifier, you first need data. In real-world scenarios, data scientists often start with exploratory data analysis and visualization to understand the classification problem better.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Import the necessary libraries\n",
    "    - Use PyTorch (`torch`) for tensor operations\n",
    "    - Use Scikit-learn's `make_classification` to generate synthetic data\n",
    "    - Import matplotlib for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5320a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bcd4bf",
   "metadata": {},
   "source": [
    "2. Generate a synthetic binary classification dataset\n",
    "    - Create 200 samples with 2 features using `make_classification`\n",
    "    - Set `n_clusters_per_class=1` and `class_sep=2` to ensure good separation\n",
    "    - Convert the NumPy arrays to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate synthetic dataset and convert to PyTorch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff2c710",
   "metadata": {},
   "source": [
    "3. Visualize the dataset\n",
    "    - Create a scatter plot showing the two classes in different colors\n",
    "    - Add a legend and appropriate labels to the axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize dataset with different colors for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3607145",
   "metadata": {},
   "source": [
    "**💡 Tip:** When visualizing the data, consider using different markers or colors to make the class separation clear. This will help you verify if your perceptron can learn to separate these classes.\n",
    "\n",
    "**⚙️ Test Your Work:**\n",
    "\n",
    "- You should see a scatter plot with two clearly distinguishable groups of points\n",
    "- Verify that you have 200 data points with 2 features (X shape should be [200, 2])\n",
    "- Make sure your tensors have the correct data type (typically `torch.float32`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4379dd6",
   "metadata": {},
   "source": [
    "## Task 2: Implementing the Perceptron\n",
    "**Context:** In industry, understanding how models work \"under the hood\" is crucial for effective debugging and customization. Building a perceptron from scratch helps cement your understanding of the fundamental building block of neural networks.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a Perceptron class\n",
    "    - Initialize weights and bias as PyTorch tensors\n",
    "    - Include parameters for input dimension and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define Perceptron class with appropriate initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a597a0",
   "metadata": {},
   "source": [
    "2. Implement the forward pass method\n",
    "    - Calculate the weighted sum of inputs\n",
    "    - Apply an activation function (e.g., sigmoid) to the sum\n",
    "    - Return the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement forward method for making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26c597",
   "metadata": {},
   "source": [
    "3. Implement a method to update weights manually\n",
    "    - Accept inputs and corresponding target values\n",
    "    - Calculate prediction error\n",
    "    - Update weights according to the perceptron learning rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e70e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement weight update method based on perceptron learning rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fcec29",
   "metadata": {},
   "source": [
    "**💡 Tip:** Remember that the perceptron learning rule updates weights proportionally to the error: Δw = learning_rate * error * input, where error = target - prediction.\n",
    "\n",
    "**⚙️ Test Your Work:**\n",
    "\n",
    "- Initialize your perceptron with random weights and verify they match the expected dimensions\n",
    "- Pass a single input through the forward method and confirm you get a prediction between 0 and 1\n",
    "- Test your weight update with a simple example to ensure weights change in the expected direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f000f",
   "metadata": {},
   "source": [
    "## Task 3: Training the Perceptron\n",
    "**Context:** Training neural networks is an iterative process where the model learns from examples over multiple passes through the dataset (epochs). Monitoring this process helps ensure the model is learning effectively.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Implement the training loop\n",
    "    - Loop through a specified number of epochs\n",
    "    - For each epoch, iterate through all training examples\n",
    "    - Call your weight update method for each example\n",
    "    - Track metrics like accuracy or loss after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bccc537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement training loop with epoch iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5542d55",
   "metadata": {},
   "source": [
    "2. Create a function to calculate accuracy\n",
    "    - Convert continuous predictions to binary classes\n",
    "    - Compare predictions with true labels\n",
    "    - Calculate and return the accuracy percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement function to calculate classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c633086",
   "metadata": {},
   "source": [
    "3. Train your perceptron\n",
    "    - Initialize your model with appropriate parameters\n",
    "    - Call the training function with your dataset\n",
    "    - Print progress updates during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75577e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the model and track progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df161b40",
   "metadata": {},
   "source": [
    "**💡 Tip:** Consider shuffling your data before each epoch to prevent the model from learning patterns related to the order of examples.\n",
    "\n",
    "**⚙️ Test Your Work:**\n",
    "\n",
    "- You should see accuracy improving over epochs\n",
    "- The final accuracy should be reasonably high (>90%) for this linearly separable dataset\n",
    "- Verify weights are changing during training by printing them occasionally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc25d9",
   "metadata": {},
   "source": [
    "## Task 4: Decision Boundary Visualization\n",
    "**Context:** Visualizing machine learning models helps communicate results to stakeholders and provides insights into how the model makes decisions. For binary classifiers, the decision boundary is particularly informative.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a function to plot the decision boundary\n",
    "    - Generate a mesh grid covering the feature space\n",
    "    - Make predictions for each point in the grid\n",
    "    - Use `contourf()` or similar to plot the decision regions\n",
    "    - Overlay the original data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement function to visualize decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e16a5e",
   "metadata": {},
   "source": [
    "2. Visualize the trained model's decision boundary\n",
    "    - Call your plotting function with the trained perceptron\n",
    "    - Add appropriate labels and a title to the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3bca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Call visualization function with trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592952d",
   "metadata": {},
   "source": [
    "3. Implement a function to visualize weights during training\n",
    "    - Store weights at specific intervals during training\n",
    "    - Plot how the weight values change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab5cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement function to track and visualize weight evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b523b",
   "metadata": {},
   "source": [
    "**💡 Tip:** For 2D data, the decision boundary is a line with the equation w₁x₁ + w₂x₂ + b = 0, where w₁ and w₂ are the weights and b is the bias.\n",
    "\n",
    "**⚙️ Test Your Work:**\n",
    "\n",
    "- The decision boundary should clearly separate the two classes\n",
    "- The boundary visualization should match your intuition based on the data scatter plot\n",
    "- The weight evolution plot should show convergence (stabilization) over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488fd7a",
   "metadata": {},
   "source": [
    "## Task 5: Experimentation with Learning Rates\n",
    "**Context:** Learning rate is a critical hyperparameter that affects how quickly and effectively a model learns. In production environments, finding the optimal learning rate can significantly impact model performance.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Modify your training code to experiment with different learning rates\n",
    "    - Create a list of learning rates to test (e.g., [0.001, 0.01, 0.1, 1.0])\n",
    "    - Train separate models with each learning rate\n",
    "    - Track and store training metrics for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement learning rate experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55514217",
   "metadata": {},
   "source": [
    "2. Visualize the impact of learning rates\n",
    "    - Create a plot comparing accuracy over epochs for different learning rates\n",
    "    - Plot decision boundaries for models trained with different rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82915d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create visualization comparing learning rate effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e57723",
   "metadata": {},
   "source": [
    "3. Analyze and document your findings\n",
    "    - Compare convergence speed and final accuracy\n",
    "    - Identify the optimal learning rate for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fdb9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add code to print analysis of learning rate effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedeb74a",
   "metadata": {},
   "source": [
    "**💡 Tip:** Too small a learning rate might cause slow convergence, while too large a learning rate might cause oscillation or divergence.\n",
    "\n",
    "**⚙️ Test Your Work:**\n",
    "\n",
    "- You should be able to clearly see differences in learning curves for different rates\n",
    "- At least one learning rate should lead to good convergence\n",
    "- Extreme learning rates (very small or very large) should show suboptimal behavior\n",
    "\n",
    "## ✅ Success Checklist\n",
    "- Successfully generated and visualized a synthetic dataset\n",
    "- Implemented a perceptron model from scratch using PyTorch tensors\n",
    "- Created a functional training loop using the perceptron learning rule\n",
    "- Visualized the decision boundary and how it evolves during training\n",
    "- Experimented with different learning rates and analyzed their impact\n",
    "- Achieved high accuracy (>90%) on the synthetic dataset\n",
    "- Documented findings and insights from the experiments\n",
    "\n",
    "## 🔍 Common Issues & Solutions\n",
    "**Problem:** Model accuracy doesn't improve during training **Solution:** Check if your weight updates are being applied correctly. Make sure your learning rate isn't too small, and verify that your synthetic dataset is indeed linearly separable.\n",
    "\n",
    "**Problem:** Visualizations are blank or incorrect **Solution:** Ensure your data ranges are appropriate for plotting. Check that your tensor-to-numpy conversions are handling data types correctly.\n",
    "\n",
    "**Problem:** Training seems unstable or diverges **Solution:** Your learning rate might be too high. Try reducing it by an order of magnitude. Also verify that you're not overshooting the minimum during weight updates.\n",
    "\n",
    "**Problem:** Your model achieves high training accuracy but decision boundary looks wrong **Solution:** Double-check your decision boundary visualization code. Make sure you're using the correct weights and covering an appropriate range in your mesh grid.\n",
    "\n",
    "## 🔑 Key Points\n",
    "- The perceptron is the fundamental building block of neural networks, capable of learning linear decision boundaries\n",
    "- The perceptron learning rule adjusts weights based on prediction errors to minimize misclassifications\n",
    "- Learning rate selection significantly impacts training speed and final model performance\n",
    "- Visualization is a powerful tool for understanding model behavior and debugging issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2458a",
   "metadata": {},
   "source": [
    "## 💻 Reference Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0fde66",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><strong>Click HERE to see a reference solution</strong></summary>    \n",
    "    \n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Task 1: Dataset Setup\n",
    "# Generate synthetic dataset\n",
    "X_np, y_np = make_classification(\n",
    "    n_samples=200, \n",
    "    n_features=2, \n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1, \n",
    "    class_sep=2,\n",
    "    random_state=88\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X_np, dtype=torch.float32)\n",
    "y = torch.tensor(y_np, dtype=torch.float32)\n",
    "\n",
    "# Visualize dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "for class_value in np.unique(y_np):\n",
    "    plt.scatter(\n",
    "        X_np[y_np == class_value, 0],\n",
    "        X_np[y_np == class_value, 1],\n",
    "        label=f'Class {class_value}',\n",
    "        edgecolors='k'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Binary Classification Dataset')\n",
    "plt.legend() \n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "    \n",
    "# Task 2: Implementing the Perceptron\n",
    "class Perceptron:\n",
    "    def __init__(self, input_dim, learning_rate=0.01):\n",
    "        # Initialize weights and bias\n",
    "        self.weights = torch.randn(input_dim, requires_grad=False) * 0.01\n",
    "        self.bias = torch.zeros(1, requires_grad=False)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate the weighted sum and apply activation\n",
    "        weighted_sum = torch.dot(self.weights, x) + self.bias\n",
    "        return torch.sigmoid(weighted_sum)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Make binary prediction\n",
    "        return 1 if self.forward(x) >= 0.5 else 0\n",
    "    \n",
    "    def update(self, x, target):\n",
    "        # Calculate prediction and error\n",
    "        prediction = self.forward(x)\n",
    "        error = target - prediction\n",
    "        \n",
    "        # Update weights and bias using perceptron learning rule\n",
    "        self.weights += self.learning_rate * error * x\n",
    "        self.bias += self.learning_rate * error\n",
    "\n",
    "# Task 3: Training the Perceptron\n",
    "def train_perceptron(model, X, y, epochs=20):\n",
    "    accuracies = []\n",
    "    stored_weights = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Create a permutation for shuffling\n",
    "        indices = torch.randperm(len(X))\n",
    "        \n",
    "        # Training loop\n",
    "        for i in indices:\n",
    "            model.update(X[i], y[i])\n",
    "        \n",
    "        # Calculate and store accuracy\n",
    "        accuracy = calculate_accuracy(model, X, y)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        # Store weights every 10 epochs\n",
    "        if epoch % 2 == 0:\n",
    "            stored_weights.append((epoch, model.weights.clone(), model.bias.clone()))\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"Epoch {epoch}: Accuracy = {accuracy:.2f}%\")\n",
    "    \n",
    "    return accuracies, stored_weights\n",
    "    \n",
    "def calculate_accuracy(model, X, y):\n",
    "    correct = 0\n",
    "    for i in range(len(X)):\n",
    "        pred = model.predict(X[i])\n",
    "        if pred == y[i]:\n",
    "            correct += 1\n",
    "    return correct / len(X) * 100\n",
    "\n",
    "# Initialize and train perceptron\n",
    "perceptron = Perceptron(input_dim=2, learning_rate=0.05)\n",
    "accuracies, stored_weights = train_perceptron(perceptron, X, y)\n",
    "\n",
    "# Plot accuracy over epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(accuracies)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Perceptron Training Progress')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Task 4: Decision Boundary Visualization\n",
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Get predictions for each point in the mesh\n",
    "    Z = np.zeros(xx.shape)\n",
    "    for i in range(xx.shape[0]):\n",
    "        for j in range(xx.shape[1]):\n",
    "            point = torch.tensor([xx[i, j], yy[i, j]], dtype=torch.float32)\n",
    "            Z[i, j] = model.forward(point).item()\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "# Plot final decision boundary\n",
    "plot_decision_boundary(perceptron, X_np, y_np)\n",
    "\n",
    "# Visualize weight evolution\n",
    "def plot_weight_evolution(stored_weights):\n",
    "    epochs = [item[0] for item in stored_weights]\n",
    "    w1_values = [item[1][0].item() for item in stored_weights]\n",
    "    w2_values = [item[1][1].item() for item in stored_weights]\n",
    "    bias_values = [item[2].item() for item in stored_weights]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(epochs, w1_values, 'o-', label='Weight 1')\n",
    "    plt.plot(epochs, w2_values, 's-', label='Weight 2')\n",
    "    plt.plot(epochs, bias_values, '^-', label='Bias')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Weight Evolution During Training')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_weight_evolution(stored_weights)\n",
    "\n",
    "# Task 5: Experimentation with Learning Rates\n",
    "def experiment_with_learning_rates(X, y, learning_rates):\n",
    "    results = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTraining with learning rate: {lr}\")\n",
    "        model = Perceptron(input_dim=2, learning_rate=lr)\n",
    "        accuracies, _ = train_perceptron(model, X, y, epochs=20)\n",
    "        results[lr] = {\n",
    "            'model': model,\n",
    "            'accuracies': accuracies\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run experiments with different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
    "experiment_results = experiment_with_learning_rates(X, y, learning_rates)\n",
    "\n",
    "# Plot comparison of learning rates\n",
    "plt.figure(figsize=(12, 8))\n",
    "for lr, result in experiment_results.items():\n",
    "    plt.plot(result['accuracies'], label=f'LR = {lr}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Effect of Learning Rate on Training')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot decision boundaries for different learning rates\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    model = experiment_results[lr]['model']\n",
    "    \n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Get predictions for each point in the mesh\n",
    "    Z = np.zeros(xx.shape)\n",
    "    for j in range(xx.shape[0]):\n",
    "        for k in range(xx.shape[1]):\n",
    "            point = torch.tensor([xx[j, k], yy[j, k]], dtype=torch.float32)\n",
    "            Z[j, k] = model.forward(point).item()\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    axes[i].contourf(xx, yy, Z, alpha=0.8, cmap='viridis')\n",
    "    axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')\n",
    "    axes[i].set_title(f'Learning Rate = {lr}')\n",
    "    axes[i].set_xlabel('Feature 1')\n",
    "    axes[i].set_ylabel('Feature 2')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis of results\n",
    "print(\"\\nAnalysis of Learning Rate Effects:\")\n",
    "for lr, result in experiment_results.items():\n",
    "    final_accuracy = result['accuracies'][-1]\n",
    "    convergence_epoch = next((i for i, acc in enumerate(result['accuracies']) \n",
    "                              if acc > 95), len(result['accuracies']))\n",
    "    \n",
    "    print(f\"Learning Rate {lr}:\")\n",
    "    print(f\"  Final Accuracy: {final_accuracy:.2f}%\")\n",
    "    if convergence_epoch < len(result['accuracies']):\n",
    "        print(f\"  Converged at epoch {convergence_epoch}\")\n",
    "    else:\n",
    "        print(\" Did not converge to >95% accuracy\")\n",
    "    print()   \n",
    "```    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
