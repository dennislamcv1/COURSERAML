{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4d17bb",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Fine-Tune a Pre-Trained Model on a New Dataset\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you will explore the powerful concept of transfer learning by fine-tuning a pre-trained Convolutional Neural Network (CNN) on a new dataset. By leveraging models such as ResNet18 or VGG16 that are pre-trained on ImageNet, you'll efficiently adapt them to a new image classification task with limited data. This technique is widely used in industry to build powerful image classifiers without requiring extensive computational resources or massive datasets.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Load and modify pre-trained CNN models using PyTorch\n",
    "- Adapt pre-trained models for new tasks through strategic layer freezing and classifier replacement\n",
    "- Implement a complete training pipeline for transfer learning\n",
    "- Evaluate fine-tuned model performance using appropriate metrics\n",
    "\n",
    "## üöÄ Starting Point\n",
    "Access the starter code by creating a new Python file or Jupyter notebook.\n",
    "\n",
    "Required tools/setup:\n",
    "\n",
    "- Python 3.x\n",
    "- PyTorch (1.7+)\n",
    "- torchvision\n",
    "- matplotlib\n",
    "- numpy\n",
    "\n",
    "Make sure to reference:\n",
    "\n",
    "- Previous lab on CNN architecture and training basics\n",
    "- Understanding of model architecture components (feature extraction vs. classification layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811cb090",
   "metadata": {},
   "source": [
    "## Task 1: Set up the Environment and Load the Pre-trained Model\n",
    "**Context:** When working with transfer learning in industry, the first step is always to import the correct libraries and load the pre-trained model that will serve as your starting point.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Import the necessary libraries for working with PyTorch, data loading, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1166fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the necessary libraries\n",
    "# Hint: You'll need torch, torchvision, matplotlib, and other utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f90cc",
   "metadata": {},
   "source": [
    "2. Load a pre-trained ResNet18 model from torchvision.models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the pre-trained ResNet18 model\n",
    "# Use torchvision.models.resnet18() with pretrained=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859bc32",
   "metadata": {},
   "source": [
    "3. Examine the model architecture to understand which parts are feature extractors and which perform classification.\n",
    "- What is the structure of the final layer (fc)?\n",
    "- How many output classes does the original model have?\n",
    "\n",
    "**üí° Tip:** Use `print(model)` to view the entire architecture, and specifically examine `model.fc` to understand the final classification layer.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Print the model structure and verify it has loaded correctly\n",
    "- Expected output: A complete model architecture with layers showing input/output dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af56ee",
   "metadata": {},
   "source": [
    "## Task 2: Freeze Features and Modify the Classifier\n",
    "**Context:** In real-world transfer learning, we want to preserve the feature extraction knowledge from ImageNet while adapting the classification head to our specific problem.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Freeze all parameters in the pre-trained layers of the model by setting their requires_grad attribute to False. This ensures that only the newly added classifier layers will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d925ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Freeze all parameters in the feature extraction layers\n",
    "# Hint: Use requires_grad = False for parameters that should not be updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fe939e",
   "metadata": {},
   "source": [
    "2. Replace the final fully connected layer (classifier) with a new one that matches your target dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace the final fully connected layer\n",
    "# The new layer should match the number of classes in your target dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a69dc",
   "metadata": {},
   "source": [
    "3. Verify that only the parameters in the new classifier layer are trainable.\n",
    "    - How many parameters are trainable vs. frozen?\n",
    "    - Why is this approach efficient for transfer learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9587bc03",
   "metadata": {},
   "source": [
    "**üí° Tip:** For ResNet18, access the fully connected layer with `model.fc`. Replace it with a new `nn.Linear` layer matching your output dimensions.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Print the model's parameters showing which are trainable\n",
    "- Expected output: Most parameters should have requires_grad=False, except for the final layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1282546",
   "metadata": {},
   "source": [
    "## Task 3: Prepare Your Dataset\n",
    "**Context:** To fine-tune our model, we need a properly formatted dataset that matches the input requirements of the pre-trained model.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define data transformations for training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define data transforms for training and validation\n",
    "# Remember to include resizing, normalization (with ImageNet statistics), and conversion to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d081b1",
   "metadata": {},
   "source": [
    "2. Load the CIFAR-10 dataset and specifically filter it to use a subset of its classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34151dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset using torchvision.datasets and apply the transformations\n",
    "# Create appropriate DataLoader objects for training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d39e31f",
   "metadata": {},
   "source": [
    "3. Inspect a few sample images and verify transformations are correctly applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da399eec",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `torchvision.transforms.Compose` to chain multiple transformations, and ensure normalization uses the same mean and std values as the original ImageNet training.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Display a batch of images to verify they look as expected\n",
    "- Print the dataset size and batch dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0fb6c8",
   "metadata": {},
   "source": [
    "## Task 4: Implement the Training Loop\n",
    "**Context:** Fine-tuning requires a careful training approach with appropriate hyperparameters to adapt the model without losing the pre-trained knowledge.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define an appropriate loss function (e.g., nn.CrossEntropyLoss) and an optimizer (e.g., optim.Adam focusing only on the trainable parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up the loss function and optimizer\n",
    "# Consider using a smaller learning rate for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862898ea",
   "metadata": {},
   "source": [
    "2. Implement a training loop with validation after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the training loop\n",
    "# Include tracking for loss and accuracy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea6246",
   "metadata": {},
   "source": [
    "3. Save the best model checkpoint based on validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f48840",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use an optimizer like optim.Adam with a small learning rate (around 0.001) and ensure it's configured to optimize only the parameters of your newly added classification layer (model.fc.parameters() in ResNet18's case) to avoid drastic changes to the pre-learned features.\"\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Track and print training loss and accuracy for each epoch\n",
    "- Expected output: Decreasing training loss and increasing accuracy over epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177bb0c",
   "metadata": {},
   "source": [
    "## Task 5: Evaluate the Fine-tuned Model\n",
    "**Context:** After fine-tuning, we need to properly evaluate the model to determine if it has successfully adapted to the new task.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement model evaluation on the validation set\n",
    "# Calculate overall accuracy and generate a confusion matrix to understand per-class performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a44547",
   "metadata": {},
   "source": [
    "2. Visualize sample predictions alongside ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize model predictions vs true labels\n",
    "# Show both correct and incorrect predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9700cb7",
   "metadata": {},
   "source": [
    "3. Compare the fine-tuned model performance with a baseline (e.g., random guessing).\n",
    "\n",
    "**üí° Tip:** Use a confusion matrix to identify which classes the model struggles with most.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Generate a report with overall accuracy and visualize the confusion matrix\n",
    "- Create visualization showing correctly and incorrectly classified images\n",
    "\n",
    "## ‚úÖ Success Checklist\n",
    "- Pre-trained model successfully loaded and inspected\n",
    "- Feature extraction layers properly frozen (requires_grad = False)\n",
    "- Classifier layer replaced to match the target dataset classes\n",
    "- Dataset correctly loaded with appropriate transformations\n",
    "- Training loop implementation shows decreasing loss and increasing accuracy\n",
    "- Model evaluation demonstrates significant improvement over random chance\n",
    "- Sample predictions successfully visualized and analyzed\n",
    "\n",
    "## üîç Common Issues & Solutions\n",
    "**Problem:** Model doesn't seem to be learning (accuracy stays low) **Solution:** Check that only the classifier is trainable and verify the learning rate isn't too small.\n",
    "\n",
    "**Problem:** Runtime errors related to tensor dimensions **Solution:** Ensure input image dimensions match what the pre-trained model expects (typically 224√ó224 for ImageNet models).\n",
    "\n",
    "**Problem:** Out of memory errors during training **Solution:** Reduce batch size or use a lighter pre-trained model (like MobileNet instead of ResNet).\n",
    "\n",
    "**Problem:** Overfitting occurs quickly **Solution:** Implement data augmentation in training transforms and consider adding dropout to the classifier.\n",
    "\n",
    "## üîë Key Points\n",
    "- Transfer learning significantly reduces training time and data requirements\n",
    "- Freezing feature extraction layers preserves valuable pre-trained knowledge\n",
    "- The classifier layer needs to be replaced to match your specific task\n",
    "- Fine-tuning requires careful parameter selection (learning rate, epochs) to be effective\n",
    "- ImageNet pre-trained models expect specific normalization values and input sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0bbcef",
   "metadata": {},
   "source": [
    "## üíª Reference Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60e5e0",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><strong>Click HERE to see a referance solution</strong></summary>    \n",
    "    \n",
    "```python\n",
    "    \n",
    "# Task 1: Set up the Environment and Load the Pre-trained Model\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import math # Import math for ceil\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "    \n",
    "# Use torchvision.models.resnet18() with pretrained=True\n",
    "model = models.resnet18(pretrained=True)\n",
    "print(model)\n",
    "print(model.fc)\n",
    "\n",
    "# Task 2: Freeze Features and Modify the Classifier\n",
    "   \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "    \n",
    "# Task 3: Prepare Your Dataset\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=data_transforms['train'])\n",
    "val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=data_transforms['val'])\n",
    "\n",
    "train_idx = torch.tensor(train_dataset.targets) < 2\n",
    "val_idx = torch.tensor(val_dataset.targets) < 2\n",
    "\n",
    "train_dataset.data = train_dataset.data[train_idx]\n",
    "train_dataset.targets = torch.tensor(train_dataset.targets)[train_idx].tolist()\n",
    "\n",
    "val_dataset.data = val_dataset.data[val_idx]\n",
    "val_dataset.targets = torch.tensor(val_dataset.targets)[val_idx].tolist()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# Task 4: Implement the Training Loop\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(val_dataset)\n",
    "        epoch_acc = running_corrects.double() / len(val_dataset)\n",
    "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        print()\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model, criterion, optimizer, num_epochs=10)\n",
    "    \n",
    "# Task 5: Evaluate the Fine-tuned Model\n",
    "    \n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return accuracy, cm, all_preds, all_labels\n",
    "accuracy, cm, predictions, true_labels = evaluate_model(model, val_loader)\n",
    "print(f'Final Test Accuracy: {accuracy:.4f}')\n",
    "# Define class names (if not already in scope)\n",
    "classes = ('plane', 'car')\n",
    "\n",
    "# Now plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def visualize_predictions(model, dataloader, classes, num_images=5):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    cols = 2\n",
    "    rows = math.ceil(num_images / cols)\n",
    "    \n",
    "    fig = plt.figure(figsize=(cols * 5, rows * 4))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            for j in range(inputs.size(0)):\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    return\n",
    "                \n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(rows, cols, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'Predicted: {classes[preds[j]]}\\nTrue: {classes[labels[j]]}')\n",
    "                \n",
    "                img = inputs.cpu().data[j].numpy().transpose((1, 2, 0))\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "\n",
    "                plt.imshow(img)\n",
    "    \n",
    "    model.train(mode=was_training)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize model predictions\n",
    "visualize_predictions(model, val_loader, classes)\n",
    "```    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
