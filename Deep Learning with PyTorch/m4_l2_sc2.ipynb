{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing and Switching Optimizers in PyTorch"
      ],
      "metadata": {
        "id": "mjwTYki_galN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setting Up the PyTorch Environment\n",
        "\n"
      ],
      "metadata": {
        "id": "WEnR9w2Fgac4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70DUTSFbfvuU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# For creating a dummy DataLoader\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Defining a Simple Neural Network\n"
      ],
      "metadata": {
        "id": "7s8hyaKZgxkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, input_size=784, num_classes=10):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x) # Output raw logits\n",
        "        return x\n",
        "\n",
        "# Define model parameters based on common use-case (e.g., MNIST-like)\n",
        "INPUT_SIZE = 784\n",
        "NUM_CLASSES = 10\n",
        "model = SimpleNet(input_size=INPUT_SIZE, num_classes=NUM_CLASSES)"
      ],
      "metadata": {
        "id": "7C_xOrPNg0Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Choosing and Implementing Optimizers\n"
      ],
      "metadata": {
        "id": "fYf20qpBg3jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the neural network model\n",
        "if 'model' not in locals():  # Check if model is undefined to handle non-sequential cell execution\n",
        "    INPUT_SIZE = 784\n",
        "    NUM_CLASSES = 10\n",
        "    model = SimpleNet(input_size=INPUT_SIZE, num_classes=NUM_CLASSES)\n",
        "\n",
        "# Configure optimizers\n",
        "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # SGD with momentum\n",
        "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.01)  # RMSprop optimizer\n",
        "\n",
        "print(\"Optimizers initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZohMSuqqg_Zq",
        "outputId": "9272c2c8-0e30-4d13-97fb-a860eed890c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizers initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Training Loops: Switching Optimizers\n"
      ],
      "metadata": {
        "id": "GeVePah9hBtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure model is defined (e.g., if running this cell standalone)\n",
        "if 'model' not in locals():\n",
        "    INPUT_SIZE = 784\n",
        "    NUM_CLASSES = 10\n",
        "    model = SimpleNet(input_size=INPUT_SIZE, num_classes=NUM_CLASSES)\n",
        "    # Re-initialize optimizers if model was re-initialized\n",
        "    optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
        "    optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "# Define a loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create dummy data and DataLoader for demonstration\n",
        "NUM_EPOCHS = 6  # Reduced for quicker demonstration\n",
        "BATCH_SIZE = 32\n",
        "NUM_SAMPLES_PER_EPOCH = 128 # Number of samples for the dummy dataset\n",
        "\n",
        "# Generate dummy data\n",
        "dummy_inputs_tensor = torch.randn(NUM_SAMPLES_PER_EPOCH, INPUT_SIZE)\n",
        "dummy_labels_tensor = torch.randint(0, NUM_CLASSES, (NUM_SAMPLES_PER_EPOCH,)) # Target for CrossEntropyLoss\n",
        "\n",
        "# Create a TensorDataset and DataLoader\n",
        "dummy_dataset = TensorDataset(dummy_inputs_tensor, dummy_labels_tensor)\n",
        "train_loader = DataLoader(dataset=dummy_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print(f\"Starting training for {NUM_EPOCHS} epochs.\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Switch optimizer halfway through training\n",
        "    # The optimizers (optimizer_adam, optimizer_rmsprop) were initialized once with model.parameters().\n",
        "    # They maintain their own internal states. When we switch, the selected optimizer\n",
        "    # continues with its state from where it left off (or starts fresh if never used).\n",
        "    if epoch < NUM_EPOCHS // 2:\n",
        "        optimizer = optimizer_adam\n",
        "        optimizer_name = \"Adam\"\n",
        "    else:\n",
        "        optimizer = optimizer_rmsprop\n",
        "        optimizer_name = \"RMSprop\"\n",
        "\n",
        "    model.train() # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        inputs, labels = batch\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = running_loss / num_batches\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} completed with {optimizer_name}. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEe_FFUxhCIZ",
        "outputId": "f72bc537-881a-45f1-cc1f-9022d16018b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs.\n",
            "Epoch 1/6 completed with Adam. Average Loss: 2.3116\n",
            "Epoch 2/6 completed with Adam. Average Loss: 2.0963\n",
            "Epoch 3/6 completed with Adam. Average Loss: 1.9239\n",
            "Epoch 4/6 completed with RMSprop. Average Loss: 42.0097\n",
            "Epoch 5/6 completed with RMSprop. Average Loss: 36.8680\n",
            "Epoch 6/6 completed with RMSprop. Average Loss: 1.4027\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C67G65987IVj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}