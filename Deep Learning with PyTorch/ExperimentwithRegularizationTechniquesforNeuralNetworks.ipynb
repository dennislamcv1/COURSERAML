{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0140443e",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Experiment with Regularization Techniques for Neural Networks\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you'll apply and evaluate various regularization techniques to improve a neural network's performance on image data. You will work with a baseline model and progressively enhance it using dropout, L2 weight decay, and batch normalization. These techniques are essential for preventing overfitting and ensuring your models generalize well to unseen data - critical skills for any machine learning practitioner.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Implement and evaluate a baseline neural network on an image classification task\n",
    "- Apply dropout layers to reduce overfitting in neural networks\n",
    "- Implement weight decay (L2 regularization) to enhance model generalization\n",
    "- Integrate batch normalization to improve training stability and convergence speed\n",
    "- Compare and analyze the effects of different regularization techniques on model performance\n",
    "\n",
    "## üöÄ Starting Point\n",
    "Access the starter code by creating a new Python notebook in your preferred environment (Colab, Jupyter, etc.).\n",
    "\n",
    "Required tools/setup:\n",
    "\n",
    "- Python 3.6+\n",
    "- PyTorch\n",
    "- Matplotlib\n",
    "- Numpy\n",
    "\n",
    "Make sure you've completed previous labs on neural networks and are familiar with basic model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321752e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Device Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Data Loading and Preprocessing ---\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Standard normalization for MNIST\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca5609",
   "metadata": {},
   "source": [
    "## Task 1: Create and Train a Baseline Model\n",
    "**Context:** As a first step in understanding the effects of regularization, you need to establish a baseline model to serve as a reference point for comparison.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define a simple CNN model class with convolutional layers followed by fully connected layers\n",
    "\n",
    "    - Use `nn.Conv2d` for convolutional layers with appropriate kernel sizes\n",
    "    - Add `nn.ReLU` activations after each layer\n",
    "    - Include fully connected layers using `nn.Linear`\n",
    "    - What architecture complexity is appropriate for this task?\n",
    "\n",
    "2. Instantiate the baseline model, loss function, and optimizer\n",
    "\n",
    "    - Use `CrossEntropyLoss()` for classification tasks\n",
    "    - Consider using `optim.SGD` or `ptim.Adam` for optimization\n",
    "    - What learning rate would be appropriate?\n",
    "\n",
    "3. Train the model and evaluate performance\n",
    "\n",
    "    - Use the provided `train()` and `evaluate()` functions\n",
    "    - Save the training and testing metrics for comparison\n",
    "\n",
    "4. Plot the training and validation loss curves\n",
    "\n",
    "    - How can you determine if the model is overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your baseline model implementation\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        # TODO: Define layers for your baseline model\n",
    "        # Convolutional layers\n",
    "        \n",
    "        # Fully connected layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        \n",
    "        return x\n",
    "\n",
    "# TODO: Instantiate model, define loss function and optimizer\n",
    "\n",
    "# TODO: Train the model and track metrics\n",
    "\n",
    "# TODO: Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42fa13",
   "metadata": {},
   "source": [
    "**üí° Tip:** Keep your baseline model relatively simple (2-3 convolutional layers followed by 1-2 fully connected layers) to better observe the effects of regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6051b",
   "metadata": {},
   "source": [
    "## Task 2: Implement Dropout\n",
    "**Context:** Dropout is a powerful technique that prevents co-adaptation of neurons by randomly setting a fraction of inputs to zero during training, which helps reduce overfitting.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a new model class that incorporates dropout layers\n",
    "\n",
    "    - Add `nn.Dropout` layers after fully connected layers\n",
    "    - Try a moderate dropout rate (0.2-0.5)\n",
    "    - How might placement of dropout layers affect model performance?\n",
    "\n",
    "2. Train the model with dropout and evaluate\n",
    "    \n",
    "    - Use the same training parameters as the baseline for fair comparison\n",
    "    - Pay attention to differences in training time and convergence\n",
    "\n",
    "3. Compare the dropout model's performance with the baseline\n",
    "\n",
    "    - Are there notable differences in train vs test loss curves?\n",
    "    - How does dropout affect the gap between training and validation performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947953b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your dropout model implementation\n",
    "class DropoutModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(DropoutModel, self).__init__()\n",
    "        # TODO: Define layers including dropout\n",
    "        # Convolutional layers\n",
    "        \n",
    "        # Dropout + Fully connected layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass with dropout\n",
    "        \n",
    "        return x\n",
    "\n",
    "# TODO: Instantiate dropout model, define loss function and optimizer\n",
    "\n",
    "# TODO: Train the model and track metrics\n",
    "\n",
    "# TODO: Evaluate the model on test data\n",
    "\n",
    "# TODO: Compare with baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b6068",
   "metadata": {},
   "source": [
    "**üí° Tip:** Experiment with different dropout rates to find the optimal balance between regularization and model capacity. Remember that dropout is only active during training, not during evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94816e95",
   "metadata": {},
   "source": [
    "## Task 3: Apply Weight Decay (L2 Regularization)\n",
    "**Context:** Weight decay adds a penalty to the loss function proportional to the squared magnitude of the weights, encouraging the model to learn smaller weights and potentially generalize better.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a new model with the same architecture as the baseline\n",
    "\n",
    "2. Configure the optimizer with weight decay\n",
    "\n",
    "    - Add the `weight_decay` parameter to your optimizer (try values like 1e-4 or 1e-5)\n",
    "    - How does the choice of weight decay value relate to the learning rate?\n",
    "\n",
    "3. Train the model with weight decay and evaluate\n",
    "\n",
    "    - What differences do you observe in the weights distributions?\n",
    "    - How does convergence speed compare to the baseline?\n",
    "\n",
    "4. Compare performance with previous models\n",
    "\n",
    "    - Does weight decay effectively reduce overfitting?\n",
    "    - How does it compare to dropout in terms of generalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb86ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your weight decay implementation uses the same model architecture as baseline\n",
    "# but with weight decay in the optimizer\n",
    "\n",
    "# TODO: Instantiate model with same architecture as baseline\n",
    "\n",
    "# TODO: Define loss function and optimizer with weight decay\n",
    "\n",
    "# TODO: Train the model and track metrics\n",
    "\n",
    "# TODO: Evaluate the model on test data\n",
    "\n",
    "# TODO: Compare with baseline and dropout models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04dc0a5",
   "metadata": {},
   "source": [
    "**üí° Tip:** Start with a small weight decay value (1e-5) and gradually increase it if needed. Too large a value can impede learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ea12d",
   "metadata": {},
   "source": [
    "## Task 4: Implement Batch Normalization\n",
    "**Context:** Batch normalization normalizes the inputs of each layer, which can stabilize and accelerate training by reducing internal covariate shift.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a new model that incorporates batch normalization\n",
    "\n",
    "    - Add `nn.BatchNorm2d` after convolutional layers but before activations\n",
    "    - Add `nn.BatchNorm1d` for fully connected layers\n",
    "    - Why might the order of batch norm and activation functions matter?\n",
    "\n",
    "2. Train the model with batch normalization and evaluate\n",
    "\n",
    "    - Try using a slightly higher learning rate than before\n",
    "    - How does batch normalization affect training stability?\n",
    "\n",
    "3. Compare training speed and final performance with previous models\n",
    "\n",
    "    - Does batch normalization accelerate convergence?\n",
    "    - How does it affect the model's generalization ability?\n",
    "\n",
    "4. Visualize and compare learning curves for all models\n",
    "\n",
    "    - Which regularization technique had the most significant impact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a06844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your batch normalization model implementation\n",
    "class BatchNormModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BatchNormModel, self).__init__()\n",
    "        # TODO: Define layers with batch normalization\n",
    "        # Conv > BatchNorm > ReLU sequences\n",
    "        \n",
    "        # Fully connected layers with batch norm\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass with batch normalization\n",
    "        \n",
    "        return x\n",
    "\n",
    "# TODO: Instantiate batch norm model, define loss function and optimizer\n",
    "\n",
    "# TODO: Train the model and track metrics\n",
    "\n",
    "# TODO: Evaluate the model on test data\n",
    "\n",
    "# TODO: Compare with all previous models\n",
    "\n",
    "# TODO: Plot comparative learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4131ff",
   "metadata": {},
   "source": [
    "**üí° Tip:** Batch normalization often allows for higher learning rates and can significantly accelerate training. Pay attention to how the model behaves in early training epochs.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "For each model, run the following tests:\n",
    "\n",
    "- Evaluate on the test dataset and record accuracy\n",
    "\n",
    "    - Baseline model should achieve at least 95% accuracy\n",
    "    - Regularized models should maintain or improve this accuracy\n",
    "\n",
    "- For each model, calculate and compare:\n",
    "\n",
    "    - Final training loss\n",
    "    - Final test loss\n",
    "    - Gap between training and test loss (indicator of overfitting)\n",
    "    - Training time per epoch\n",
    "\n",
    "## ‚úÖ Success Checklist\n",
    "- Successfully implemented and trained a baseline CNN model\n",
    "- Applied dropout regularization and analyzed its impact\n",
    "- Implemented weight decay and evaluated its effect on generalization\n",
    "- Integrated batch normalization and assessed training stability\n",
    "- Compared all approaches using well-formatted learning curves\n",
    "- Identified which regularization technique worked best for this task\n",
    "- Model runs without errors and achieves reasonable accuracy (>95% on MNIST)\n",
    "\n",
    "## üîç Common Issues & Solutions\n",
    "**Problem:** Model shows signs of severe overfitting (large gap between training and test loss) **Solution:** Increase regularization strength (higher dropout rate or weight decay parameter).\n",
    "\n",
    "**Problem:** Model convergence is too slow **Solution:** Check learning rate, consider using batch normalization, or implement learning rate scheduling.\n",
    "\n",
    "**Problem:** Loss becomes unstable or NaN **Solution:** Reduce learning rate, check for proper initialization, or review batch normalization implementation.\n",
    "\n",
    "## üîë Key Points\n",
    "- Dropout randomly deactivates neurons during training, forcing the network to learn redundant representations\n",
    "- Weight decay penalizes large weights, encouraging the model to find simpler solutions\n",
    "- Batch normalization stabilizes training by normalizing layer inputs, often allowing faster convergence\n",
    "- The choice of regularization technique depends on the specific dataset and model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd1ab9",
   "metadata": {},
   "source": [
    "## üíª Reference Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd2ff93",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><strong>Click HERE to see a reference solution</strong></summary>    \n",
    "    \n",
    "```python\n",
    "# Task 1: Create and Train a Baseline Model ---\n",
    "    \n",
    "# Define a simple CNN model class\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define training and evaluation functions\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "    return train_losses\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    return test_loss, accuracy\n",
    "\n",
    "# Train and evaluate the baseline model\n",
    "baseline_model = BaselineModel().to(device)\n",
    "baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n--- Training Baseline Model ---\")\n",
    "baseline_train_losses = train(baseline_model, train_loader, baseline_optimizer, criterion, epochs=10)\n",
    "baseline_test_loss, baseline_accuracy = evaluate(baseline_model, test_loader, criterion)\n",
    "\n",
    "# Plot learning curve for the baseline model\n",
    "def plot_learning_curves(train_losses, test_loss, model_name):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=f'{model_name} Train Loss')\n",
    "    plt.plot([test_loss] * len(train_losses), linestyle='--', label=f'{model_name} Test Loss')\n",
    "    plt.title(f'{model_name} Learning Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(baseline_train_losses, baseline_test_loss, \"Baseline\")\n",
    "\n",
    "\n",
    "# Task 2: Implement Dropout \n",
    "\n",
    "# Define CNN model with dropout\n",
    "class DropoutModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(DropoutModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train and evaluate dropout model\n",
    "dropout_model = DropoutModel(dropout_rate=0.5).to(device)\n",
    "dropout_optimizer = optim.Adam(dropout_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\n--- Training Dropout Model ---\")\n",
    "dropout_train_losses = train(dropout_model, train_loader, dropout_optimizer, criterion, epochs=10)\n",
    "dropout_test_loss, dropout_accuracy = evaluate(dropout_model, test_loader, criterion)\n",
    "\n",
    "plot_learning_curves(dropout_train_losses, dropout_test_loss, \"Dropout\")\n",
    "\n",
    "\n",
    "# Task 3: Apply Weight Decay \n",
    "\n",
    "# Instantiate baseline model architecture again\n",
    "weightdecay_model = BaselineModel().to(device)\n",
    "# Define optimizer with weight decay\n",
    "weightdecay_optimizer = optim.Adam(weightdecay_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Train and evaluate model with weight decay\n",
    "print(\"\\n--- Training Weight Decay Model ---\")\n",
    "weightdecay_train_losses = train(weightdecay_model, train_loader, weightdecay_optimizer, criterion, epochs=10)\n",
    "weightdecay_test_loss, weightdecay_accuracy = evaluate(weightdecay_model, test_loader, criterion)\n",
    "\n",
    "plot_learning_curves(weightdecay_train_losses, weightdecay_test_loss, \"Weight Decay\")\n",
    "\n",
    "\n",
    "# --- Task 4: Implement Batch Normalization ---\n",
    "\n",
    "# Define CNN model with batch normalization\n",
    "class BatchNormModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BatchNormModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train and evaluate batch normalization model\n",
    "batchnorm_model = BatchNormModel().to(device)\n",
    "batchnorm_optimizer = optim.Adam(batchnorm_model.parameters(), lr=0.0015)\n",
    "\n",
    "print(\"\\n--- Training BatchNorm Model ---\")\n",
    "batchnorm_train_losses = train(batchnorm_model, train_loader, batchnorm_optimizer, criterion, epochs=10)\n",
    "batchnorm_test_loss, batchnorm_accuracy = evaluate(batchnorm_model, test_loader, criterion)\n",
    "\n",
    "plot_learning_curves(batchnorm_train_losses, batchnorm_test_loss, \"BatchNorm\")\n",
    "    \n",
    "# --- Final Comparison of All Models ---\n",
    "print(\"\\n--- Final Test Accuracies ---\")\n",
    "print(f\"Baseline: {baseline_accuracy:.2f}%\")\n",
    "print(f\"Dropout: {dropout_accuracy:.2f}%\")\n",
    "print(f\"Weight Decay: {weightdecay_accuracy:.2f}%\")\n",
    "print(f\"BatchNorm: {batchnorm_accuracy:.2f}%\")\n",
    "```    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
