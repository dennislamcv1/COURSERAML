{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Improving Stability: Gradient Clipping and Learning Rate Scheduling\n"
      ],
      "metadata": {
        "id": "RdsvUjfFjXHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Initial Setup\n"
      ],
      "metadata": {
        "id": "F3Bi2kQgjW_Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdvFpXJqjPPD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for our dummy data\n",
        "num_samples = 100  # Total number of data points\n",
        "num_features = 784  # Represents 28x28 pixels for an image\n",
        "num_classes = 10   # Number of possible output categories (e.g., digits 0-9)\n",
        "batch_size = 10    # Number of samples processed in one go\n",
        "\n",
        "# Generate random input data (features)\n",
        "dummy_inputs = torch.randn(num_samples, num_features)\n",
        "# Generate random integer labels for classification\n",
        "dummy_labels = torch.randint(0, num_classes, (num_samples,))\n",
        "\n",
        "# Create a TensorDataset from our inputs and labels\n",
        "dummy_dataset = TensorDataset(dummy_inputs, dummy_labels)\n",
        "\n",
        "# Set up DataLoaders:\n",
        "# train_loader shuffles the data each epoch, val_loader does not.\n",
        "train_loader = DataLoader(dummy_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dummy_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "mIbP38Z3kMm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(num_features, 128), # Input layer (784 features) to a hidden layer of 128 neurons\n",
        "    nn.ReLU(),                   # ReLU activation function for non-linearity\n",
        "    nn.Linear(128, 64),          # First hidden layer to a second hidden layer of 64 neurons\n",
        "    nn.ReLU(),                   # Another ReLU activation\n",
        "    nn.Linear(64, num_classes)   # Second hidden layer to the output layer (10 neurons for 10 classes)\n",
        ")"
      ],
      "metadata": {
        "id": "xzhU0M8yk2hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Gradient Clipping in Action\n"
      ],
      "metadata": {
        "id": "cKTuiEWsjkrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer: Adam is a popular choice for its adaptive learning rates.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
        "\n",
        "# Loss Function: CrossEntropyLoss is standard for multi-class classification problems.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def training_loop_with_clipping(loader, model_instance, criterion_func, optimizer_instance, max_clip_norm=1.0):\n",
        "    \"\"\"\n",
        "    Performs one epoch of training, including gradient clipping.\n",
        "    Args:\n",
        "        loader (DataLoader): The data loader for the training set.\n",
        "        model_instance (nn.Module): The neural network model.\n",
        "        criterion_func (callable): The loss function.\n",
        "        optimizer_instance (optim.Optimizer): The optimizer.\n",
        "        max_clip_norm (float): The maximum norm for gradient clipping.\n",
        "    Returns:\n",
        "        float: The average training loss for the epoch.\n",
        "    \"\"\"\n",
        "    model_instance.train() # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    for inputs, labels in loader:\n",
        "        optimizer_instance.zero_grad() # Clear previous gradients\n",
        "        outputs = model_instance(inputs) # Forward pass\n",
        "        loss = criterion_func(outputs, labels) # Calculate loss\n",
        "        loss.backward() # Backpropagation: compute gradients\n",
        "\n",
        "        # Gradient Clipping: prevents exploding gradients by capping their magnitude\n",
        "        torch.nn.utils.clip_grad_norm_(model_instance.parameters(), max_norm=max_clip_norm)\n",
        "\n",
        "        optimizer_instance.step() # Update model parameters\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader) if len(loader) > 0 else 0.0\n"
      ],
      "metadata": {
        "id": "GvH663CHjk9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Adapting Learning Rates with Schedulers\n"
      ],
      "metadata": {
        "id": "HflNKFYQjsMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_validation_metrics(loader, model_instance, criterion_func):\n",
        "    \"\"\"\n",
        "    Calculates the average loss and accuracy over a dataset (e.g., validation set).\n",
        "    Args:\n",
        "        loader (DataLoader): The data loader for the dataset.\n",
        "        model_instance (nn.Module): The neural network model.\n",
        "        criterion_func (callable): The loss function.\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy_percentage)\n",
        "    \"\"\"\n",
        "    model_instance.eval() # Set the model to evaluation mode (disables dropout, BatchNorm updates, etc.)\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad(): # Disable gradient calculations to save memory and speed up\n",
        "        for inputs_val, labels_val in loader:\n",
        "            outputs_val = model_instance(inputs_val)\n",
        "            loss_val = criterion_func(outputs_val, labels_val)\n",
        "            total_loss += loss_val.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs_val.data, 1) # Get the index of the max log-probability\n",
        "            total_samples += labels_val.size(0)\n",
        "            correct_predictions += (predicted == labels_val).sum().item()\n",
        "\n",
        "    model_instance.train() # Set the model back to training mode\n",
        "    avg_loss = total_loss / len(loader) if len(loader) > 0 else 0.0\n",
        "    accuracy = (correct_predictions / total_samples) * 100 if total_samples > 0 else 0.0\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "3g8rG7-yjt8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Integrating Techniques for Better Training\n"
      ],
      "metadata": {
        "id": "Lqi8w1AHjwVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Demonstrating Gradient Clipping in a Single Epoch ---\")\n",
        "# Run one epoch to see initial behavior with clipping\n",
        "initial_train_loss = training_loop_with_clipping(train_loader, model, criterion, optimizer, max_clip_norm=1.0)\n",
        "print(f\"One epoch training with clipping complete. Initial Training Loss: {initial_train_loss:.4f}\\n\")\n",
        "\n",
        "# Learning Rate Scheduler:\n",
        "# StepLR decays the learning rate by 'gamma' every 'step_size' epochs.\n",
        "# We've adjusted step_size and gamma to make the LR changes more visible in a shorter demonstration.\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5) # Decay every 5 epochs by 50%\n",
        "\n",
        "print(\"--- Full Training Run with Gradient Clipping and Learning Rate Scheduling ---\")\n",
        "num_epochs_full_run = 5 # Total epochs for this demonstration\n",
        "\n",
        "for epoch in range(num_epochs_full_run):\n",
        "    # Train for one epoch (clipping is handled inside this function)\n",
        "    train_loss = training_loop_with_clipping(train_loader, model, criterion, optimizer, max_clip_norm=1.0)\n",
        "\n",
        "    # Get the current learning rate *before* the scheduler steps\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    # Update the learning rate for the next epoch\n",
        "    scheduler.step()\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    val_loss, val_accuracy = calculate_validation_metrics(val_loader, model, criterion)\n",
        "\n",
        "    # Print epoch results\n",
        "    print(f\"Epoch {epoch+1:2d}: LR: {current_lr:.6f}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "# Report final validation metrics after the training run\n",
        "final_val_loss, final_val_accuracy = calculate_validation_metrics(val_loader, model, criterion)\n",
        "print(f\"Final Validation Loss: {final_val_loss:.4f}, Final Validation Accuracy: {final_val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3w3yt_BjxsL",
        "outputId": "dc6bc168-d544-4f85-f129-9ca64d32893a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Demonstrating Gradient Clipping in a Single Epoch ---\n",
            "One epoch training with clipping complete. Initial Training Loss: 2.4618\n",
            "\n",
            "--- Full Training Run with Gradient Clipping and Learning Rate Scheduling ---\n",
            "Epoch  1: LR: 0.020000, Train Loss: 1.9440, Val Loss: 0.6252, Val Acc: 79.00%\n",
            "Epoch  2: LR: 0.020000, Train Loss: 0.4794, Val Loss: 0.0837, Val Acc: 98.00%\n",
            "Epoch  3: LR: 0.020000, Train Loss: 0.1606, Val Loss: 0.1785, Val Acc: 96.00%\n",
            "Epoch  4: LR: 0.020000, Train Loss: 0.5755, Val Loss: 0.0063, Val Acc: 100.00%\n",
            "Epoch  5: LR: 0.020000, Train Loss: 0.2132, Val Loss: 0.0073, Val Acc: 100.00%\n",
            "\n",
            "--- Training Complete ---\n",
            "Final Validation Loss: 0.0073, Final Validation Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vx9RISa3k5fA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}