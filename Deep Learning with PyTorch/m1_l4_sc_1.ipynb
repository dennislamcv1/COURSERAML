{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a Multi-Layer Perceptron with nn.Module and nn.Sequential"
      ],
      "metadata": {
        "id": "fBIGWtq5sZOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setting Up the Environment"
      ],
      "metadata": {
        "id": "sR9HOHb8sdC3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuAC1pD4sMt7",
        "outputId": "5b817071-c5cb-4596-dbc8-817d7aa32ad8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a45f4214d30>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Optional: for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Defining an MLP with `nn.Module`\n"
      ],
      "metadata": {
        "id": "Se1MbjyPsgst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPUsingModule(nn.Module):\n",
        "    \"\"\"\n",
        "    A Multi-Layer Perceptron defined using nn.Module.\n",
        "    This approach offers maximum flexibility in designing the network architecture\n",
        "    and defining the forward pass.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()  # Updated to modern Python 3 super() call\n",
        "        # Define the layers\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.activation_fn = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Defines the forward pass of the model.\"\"\"\n",
        "        x = self.hidden_layer(x)\n",
        "        x = self.activation_fn(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iP8kyApeskLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Building an MLP with `nn.Sequential`\n"
      ],
      "metadata": {
        "id": "QjjSvyi_sqZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mlp_via_sequential(input_size, hidden_size, output_size):\n",
        "    \"\"\"\n",
        "    Creates an MLP using nn.Sequential.\n",
        "    This is convenient for models where layers are applied in a simple linear sequence.\n",
        "    \"\"\"\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, output_size)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Example of creating an instance and printing its structure\n",
        "# This demonstrates how an nn.Sequential model looks when printed.\n",
        "mlp_sequential_example = create_mlp_via_sequential(4, 5, 3)\n",
        "print(\"MLP Sequential Example Architecture (from Step 3):\")\n",
        "print(mlp_sequential_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPo0FAnxsrVs",
        "outputId": "c5aecdbd-018e-4547-9854-44cd931cc3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Sequential Example Architecture (from Step 3):\n",
            "Sequential(\n",
            "  (0): Linear(in_features=4, out_features=5, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=5, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Practical Application: Training the MLP\n"
      ],
      "metadata": {
        "id": "Zybx2zsXsw43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mlp(model, data, targets, learning_rate=0.001, epochs=100):\n",
        "    \"\"\"\n",
        "    A basic training loop for an MLP model.\n",
        "    It uses Stochastic Gradient Descent (SGD) optimizer and Mean Squared Error (MSE) loss.\n",
        "    \"\"\"\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    # Define the loss function and pass variable name\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(f\"\\nTraining model: {model.__class__.__name__ if not isinstance(model, nn.Sequential) else 'SequentialMLP'}\")\n",
        "    if isinstance(model, nn.Sequential): # nn.Sequential doesn't have a custom __name__ reflecting the MLP\n",
        "        print(\"Model type: nn.Sequential based\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward pass: compute model predictions\n",
        "        predictions = model(data)\n",
        "\n",
        "        # 3. Compute loss\n",
        "        loss = criterion(predictions, targets)\n",
        "\n",
        "        # 4. Backward pass: compute gradients of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step: update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress (e.g., every 10% of epochs and the final epoch)\n",
        "        if (epoch + 1) % (epochs // 10 or 1) == 0 or epoch == epochs - 1:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# -- Define common parameters for the models and data --\n",
        "input_size = 4\n",
        "hidden_size = 5\n",
        "output_size = 3\n",
        "num_samples = 10\n",
        "num_epochs_demo = 50\n",
        "\n",
        "# Generate some random data for demonstration\n",
        "# In a real scenario, this would be your actual dataset.\n",
        "# For reproducibility of data, you might use torch.manual_seed() before this.\n",
        "input_data = torch.randn(num_samples, input_size)\n",
        "# Target data should match the output_size of the MLP\n",
        "target_data = torch.randn(num_samples, output_size)\n",
        "\n",
        "# --- Train MLP defined with nn.Module ---\n",
        "print(\"--- Training MLP defined with nn.Module ---\")\n",
        "mlp_module_instance = MLPUsingModule(input_size, hidden_size, output_size)\n",
        "print(\"MLPUsingModule Architecture:\")\n",
        "print(mlp_module_instance)\n",
        "train_mlp(mlp_module_instance, input_data, target_data, epochs=num_epochs_demo)\n",
        "\n",
        "# --- Train MLP defined with nn.Sequential ---\n",
        "print(\"\\n--- Training MLP defined with nn.Sequential ---\")\n",
        "mlp_sequential_instance = create_mlp_via_sequential(input_size, hidden_size, output_size)\n",
        "print(\"Sequential MLP Architecture:\")\n",
        "print(mlp_sequential_instance) # The architecture is inherently shown by nn.Sequential's print\n",
        "train_mlp(mlp_sequential_instance, input_data, target_data, epochs=num_epochs_demo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iBMx28Lsykb",
        "outputId": "9947695b-270c-4e5d-df3e-e54ebd4026e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training MLP defined with nn.Module ---\n",
            "MLPUsingModule Architecture:\n",
            "MLPUsingModule(\n",
            "  (hidden_layer): Linear(in_features=4, out_features=5, bias=True)\n",
            "  (activation_fn): ReLU()\n",
            "  (output_layer): Linear(in_features=5, out_features=3, bias=True)\n",
            ")\n",
            "\n",
            "Training model: MLPUsingModule\n",
            "Epoch 5/50, Loss: 0.7200\n",
            "Epoch 10/50, Loss: 0.7184\n",
            "Epoch 15/50, Loss: 0.7168\n",
            "Epoch 20/50, Loss: 0.7152\n",
            "Epoch 25/50, Loss: 0.7136\n",
            "Epoch 30/50, Loss: 0.7120\n",
            "Epoch 35/50, Loss: 0.7105\n",
            "Epoch 40/50, Loss: 0.7090\n",
            "Epoch 45/50, Loss: 0.7075\n",
            "Epoch 50/50, Loss: 0.7060\n",
            "\n",
            "--- Training MLP defined with nn.Sequential ---\n",
            "Sequential MLP Architecture:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=4, out_features=5, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=5, out_features=3, bias=True)\n",
            ")\n",
            "\n",
            "Training model: SequentialMLP\n",
            "Model type: nn.Sequential based\n",
            "Epoch 5/50, Loss: 0.7927\n",
            "Epoch 10/50, Loss: 0.7908\n",
            "Epoch 15/50, Loss: 0.7888\n",
            "Epoch 20/50, Loss: 0.7869\n",
            "Epoch 25/50, Loss: 0.7850\n",
            "Epoch 30/50, Loss: 0.7832\n",
            "Epoch 35/50, Loss: 0.7813\n",
            "Epoch 40/50, Loss: 0.7795\n",
            "Epoch 45/50, Loss: 0.7776\n",
            "Epoch 50/50, Loss: 0.7758\n"
          ]
        }
      ]
    }
  ]
}