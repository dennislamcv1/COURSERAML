{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d70594b1",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Build and Visualize a Perceptron from Scratch\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you will implement a perceptron from scratch using PyTorch tensors. You'll visualize the training process to understand how neural networks learn, including weight updates and decision boundary evolution. This hands-on experience provides fundamental insight into how neural networks operate before diving into more complex architectures.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "- Implement a perceptron from scratch using PyTorch tensors\n",
    "- Perform forward and backward passes manually with custom activation functions\n",
    "- Visualize and interpret decision boundary evolution during training\n",
    "- Experiment with learning rates and initial weights to understand their impact\n",
    "\n",
    "## üöÄ Starting Point\n",
    "Access the starter code below.\n",
    "\n",
    "Required tools/setup:\n",
    "- Python 3.x\n",
    "- PyTorch\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6496b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# You'll be generating data and building your perceptron from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87da022",
   "metadata": {},
   "source": [
    "## Task 1: Create and Visualize a Synthetic Dataset\n",
    "**Context:** As a machine learning engineer, you need a clear, linearly separable dataset to demonstrate how perceptrons learn. This synthetic data will help your team understand classification boundaries.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Use scikit-learn's `make_classification` to generate a binary classification dataset with 2 features\n",
    "2. Convert the data to NumPy arrays for visualization\n",
    "3. Create a scatter plot colored by class to visualize the data distribution\n",
    "4. Label axes appropriately and add a title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05db626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Generate and visualize the dataset\n",
    "# Use make_classification with n_samples=200, n_features=2, n_redundant=0, \n",
    "# n_informative=2, n_clusters_per_class=1, class_sep=1.0\n",
    "# Set class_sep parameter high enough to ensure linear separability\n",
    "\n",
    "# Visualize the generated data using plt.scatter()\n",
    "# Add proper labels and title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724a5a9",
   "metadata": {},
   "source": [
    "**üí° Tip:** Set `class_sep` to at least 1.0 to ensure good linear separation in your dataset.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Run your code to generate the scatter plot\n",
    "- Verify that you can see a clear separation between the two classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448908ed",
   "metadata": {},
   "source": [
    "## Task 2: Build a Perceptron Class\n",
    "**Context:** Understanding the basic building block of neural networks is essential. By constructing a perceptron manually, you'll gain insight into how weights contribute to decision-making.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a `Perceptron` class that initializes weights as PyTorch tensors\n",
    "2. Implement a `forward` method using either sigmoid or ReLU activation\n",
    "3. Add a method to calculate loss between predictions and actual values\n",
    "4. Ensure weights are initialized properly with `requires_grad=True` for optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Implement the Perceptron class\n",
    "# Initialize weights and bias as trainable PyTorch tensors\n",
    "# Implement forward pass with activation function\n",
    "# Add loss calculation method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1999bc",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `torch.rand()` with a small multiplier for better initial weights than all zeros.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Create an instance of your Perceptron with input dimension 2\n",
    "- Test a forward pass with a sample input tensor\n",
    "- Verify the output is between 0 and 1 for sigmoid activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449d2a6",
   "metadata": {},
   "source": [
    "## Task 3: Train the Perceptron\n",
    "**Context:** Training algorithms are the heart of machine learning. Implementing the perceptron training rule manually helps understand gradient-based optimization before using automatic frameworks.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a `train` method that takes input data, labels, learning rate, and epochs\n",
    "2. Implement the perceptron learning algorithm for weight updates\n",
    "3. Store weights and losses at regular intervals for visualization\n",
    "4. Print progress updates at specified intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df753f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Implement the training method\n",
    "# Loop through epochs and training samples\n",
    "# Calculate predictions and errors\n",
    "# Update weights using the perceptron learning rule\n",
    "# Store training history for visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e24bb84",
   "metadata": {},
   "source": [
    "**üí° Tip:** Try different learning rates (0.001 to 0.1) to observe how they affect convergence speed.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Train your perceptron on the synthetic dataset\n",
    "- Check if weights are updating each epoch\n",
    "- Observe if the loss decreases over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ce061",
   "metadata": {},
   "source": [
    "## Task 4: Visualize Decision Boundaries\n",
    "**Context:** Visualization is crucial for understanding model behavior. Creating decision boundary plots helps team members see how the model separates classes throughout training.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a function to plot the decision boundary at different epochs\n",
    "2. Visualize how the decision boundary evolves during training\n",
    "3. Plot the loss and accuracy curves over epochs\n",
    "4. Create an animation (optional) showing boundary changes over training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Implement visualization functions\n",
    "# Create a meshgrid for plotting decision boundaries\n",
    "# Use your perceptron to make predictions across the grid\n",
    "# Plot contour lines showing the decision boundary\n",
    "# Add data points colored by their class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60e05e",
   "metadata": {},
   "source": [
    "**üí° Tip:** Save the perceptron weights at different epochs to visualize the evolution of the decision boundary.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Generate the decision boundary plot\n",
    "- Verify that the boundary correctly separates the two classes\n",
    "- Ensure all plots have proper labels and titles\n",
    "\n",
    "## ‚úÖ Success Checklist\n",
    "- Dataset is created and properly visualized with clear class separation\n",
    "- Perceptron class is implemented with proper initialization and forward method\n",
    "- Training algorithm successfully updates weights and reduces loss\n",
    "- Decision boundary is visualized and correctly separates the classes\n",
    "- Training progress (loss/accuracy) is properly visualized\n",
    "- Code runs without errors\n",
    "\n",
    "## üîç Common Issues & Solutions\n",
    "- Problem: Perceptron doesn't converge to separate classes. \n",
    "    - Solution: Ensure your dataset is linearly separable and try a smaller learning rate.\n",
    "\n",
    "- Problem: Loss fluctuates wildly during training. \n",
    "    - Solution: Reduce your learning rate and check for proper weight initialization.\n",
    "\n",
    "- Problem: Decision boundary visualization is blank or incorrect. \n",
    "    - Solution: Verify your meshgrid covers the data range and check prediction function logic.\n",
    "\n",
    "- Problem: Training is extremely slow. \n",
    "    - Solution: Reduce the grid density for visualization and consider using NumPy vectorization.\n",
    "\n",
    "## üîë Key Points\n",
    "- The perceptron is the simplest form of a neural network, capable of learning linear boundaries.\n",
    "- Learning rate significantly impacts training stability and convergence speed.\n",
    "- Visualization of the decision boundary provides key insights into the learning process.\n",
    "- Understanding this foundational model helps build intuition for more complex neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24274b9",
   "metadata": {},
   "source": [
    "## Exemplar Solution\n",
    "\n",
    "After completing this activity (or if you get stuck!), take a moment to review the exemplar solution. This sample solution can offer insights into different techniques and approaches. \n",
    "\n",
    "Reflect on what you can learn from the exemplar solution to improve your coding skills.\n",
    "\n",
    "Remember, multiple solutions can exist for some problems; the goal is to learn and grow as a programmer by exploring various approaches.\n",
    "\n",
    "Use the exemplar solution as a learning tool to enhance your understanding and refine your approach to coding challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49db583",
   "metadata": {},
   "source": [
    "<details>\n",
    "    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>    \n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Task 1: Create and visualize synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=200,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    random_state=88,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=1.0\n",
    ")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Visualize dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolor='k')\n",
    "plt.title('Synthetic Classification Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Task 2: Build a Perceptron class\n",
    "class Perceptron:\n",
    "    def __init__(self, input_dim):\n",
    "        # Initialize weights with small random values\n",
    "        self.weights = torch.randn(input_dim, dtype=torch.float32, requires_grad=True) * 0.01\n",
    "        self.bias = torch.randn(1, dtype=torch.float32, requires_grad=True) * 0.01\n",
    "        \n",
    "        # Store training history\n",
    "        self.weight_history = []\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear combination of inputs and weights\n",
    "        linear_output = torch.dot(self.weights, x) + self.bias\n",
    "        \n",
    "        # Apply sigmoid activation\n",
    "        return torch.sigmoid(linear_output)\n",
    "    \n",
    "    def calculate_loss(self, y_pred, y_true):\n",
    "        # Binary cross-entropy loss\n",
    "        epsilon = 1e-10  # Small value to avoid log(0)\n",
    "        return -((y_true * torch.log(y_pred + epsilon)) + \n",
    "                ((1 - y_true) * torch.log(1 - y_pred + epsilon)))\n",
    "\n",
    "    # Task 3: Train the Perceptron\n",
    "    def train(self, X, y, epochs=10, lr=0.01, verbose=True):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            \n",
    "            # Store current weights for visualization\n",
    "            if epoch % 2 == 0:\n",
    "                self.weight_history.append((self.weights.clone().detach(), \n",
    "                                           self.bias.clone().detach()))\n",
    "            \n",
    "            for i in range(X_tensor.shape[0]):\n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_tensor[i])\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = self.calculate_loss(y_pred, y_tensor[i])\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                prediction = 1 if y_pred >= 0.5 else 0\n",
    "                if prediction == y_tensor[i]:\n",
    "                    correct += 1\n",
    "                \n",
    "                # Calculate error/gradient\n",
    "                error = y_tensor[i] - y_pred\n",
    "                \n",
    "                # Update weights and bias\n",
    "                self.weights = self.weights + lr * error * X_tensor[i]\n",
    "                self.bias = self.bias + lr * error\n",
    "            \n",
    "            # Calculate epoch statistics\n",
    "            avg_loss = total_loss / len(X)\n",
    "            accuracy = correct / len(X)\n",
    "            \n",
    "            # Store history\n",
    "            self.loss_history.append(avg_loss)\n",
    "            self.accuracy_history.append(accuracy)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch % 2 == 0 or epoch == epochs-1):\n",
    "                print(f'Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Task 4: Visualize Decision Boundaries\n",
    "def plot_decision_boundary(perceptron, X, y, weights=None, bias=None):\n",
    "    # Define the plot boundaries\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    # Create a mesh grid\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # Use provided weights or current perceptron weights\n",
    "    w = weights if weights is not None else perceptron.weights.detach().numpy()\n",
    "    b = bias if bias is not None else perceptron.bias.item()\n",
    "    \n",
    "    # Create predictions for all grid points\n",
    "    Z = np.zeros(xx.shape)\n",
    "    for i in range(xx.shape[0]):\n",
    "        for j in range(xx.shape[1]):\n",
    "            features = np.array([xx[i, j], yy[i, j]])\n",
    "            # Convert to tensor for perceptron\n",
    "            features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "            Z[i, j] = 1 if perceptron.forward(features_tensor).item() >= 0.5 else 0\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolor='k')\n",
    "    plt.title('Decision Boundary')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    return plt\n",
    "\n",
    "# Create and train the perceptron\n",
    "perceptron = Perceptron(input_dim=2)\n",
    "perceptron.train(X, y, epochs=10, lr=0.05)\n",
    "\n",
    "# Plot final decision boundary\n",
    "plot_decision_boundary(perceptron, X, y)\n",
    "plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(perceptron.loss_history)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(perceptron.accuracy_history)\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize decision boundary evolution\n",
    "if len(perceptron.weight_history) > 0:\n",
    "    for i, (weights, bias) in enumerate(perceptron.weight_history):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plot_decision_boundary(perceptron, X, y, weights.numpy(), bias.item())\n",
    "        plt.title(f'Decision Boundary at Epoch {i*2}')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
