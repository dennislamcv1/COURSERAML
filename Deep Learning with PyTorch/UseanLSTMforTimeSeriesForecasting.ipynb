{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9611eb59",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Use an LSTM for Time Series Forecasting\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you'll build and implement a Long Short-Term Memory (LSTM) neural network for time series forecasting. Working with synthetic sine wave data, you'll learn how to prepare sequential data, construct an LSTM model in PyTorch, train it effectively, and evaluate its predictions. By the end of this lab, you'll have created a model that can predict future values in a time series pattern - a technique widely used in stock market prediction, weather forecasting, and demand planning.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Transform raw data into sequence format for LSTM processing\n",
    "- Implement and configure an LSTM model architecture in PyTorch\n",
    "- Train an LSTM model with appropriate hyperparameters\n",
    "- Evaluate and visualize time series predictions against actual values\n",
    "\n",
    "## üöÄ Starting Point\n",
    "Access the starter code by creating a new Python file or using a Jupyter notebook.\n",
    "\n",
    "Required tools/setup:\n",
    "\n",
    "- PyTorch\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- Python 3.6 or later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695490bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code: Imports and data generation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Generate synthetic time series data (sine wave)\n",
    "np.random.seed(42)\n",
    "time_steps = np.linspace(0, 100, 1000)\n",
    "data = np.sin(time_steps) + np.random.normal(0, 0.1, size=len(time_steps))\n",
    "\n",
    "# Plot the data to visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data)\n",
    "plt.title(\"Synthetic Time Series Data (Sine Wave with Noise)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3621a",
   "metadata": {},
   "source": [
    "## Task 1: Prepare Sequential Data\n",
    "**Context:** Time series forecasting requires data to be formatted as sequences with input windows and target values. For LSTMs, we need to structure our data as sequences of past observations to predict future values.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a function to transform the time series data into sequences\n",
    "\n",
    "    - Use the `np.array` function to store your sequences\n",
    "    - Consider how a sliding window approach works with parameters like `seq_length`\n",
    "    - What's the appropriate target for each sequence? (Think about forecasting one step ahead)\n",
    "\n",
    "2. Split the data into training and testing sets\n",
    "\n",
    "    - Use `torch.tensor()` to convert numpy arrays to PyTorch tensors\n",
    "    - Remember to set the correct data type (`dtype=torch.float32`)\n",
    "\n",
    "3. Create a custom dataset class for better data handling\n",
    "\n",
    "    - Implement the `__len__` and `__getitem__` methods required by PyTorch's Dataset\n",
    "    - Format each sample as a sequence-target pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Prepare Sequential Data\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Transform a time series into sequences for LSTM processing\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    \n",
    "# Set sequence length (window size for prediction)\n",
    "seq_length = 20\n",
    "\n",
    "# Create sequences from time series data\n",
    "# Your code here\n",
    "\n",
    "# Split data into training and testing sets (80/20 split)\n",
    "# Your code here\n",
    "\n",
    "# Create PyTorch dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    # Your code here\n",
    "    \n",
    "# Create data loaders\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bb37b5",
   "metadata": {},
   "source": [
    "**üí° Tip:** When creating sequences, make sure your target value is positioned correctly relative to each input sequence (e.g., the next value after the sequence ends for one-step-ahead prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c895e1e",
   "metadata": {},
   "source": [
    "## Task 2: Design the LSTM Model\n",
    "**Context:** LSTMs are particularly effective for time-series because they can remember patterns over long sequences. You'll build a model that processes sequence data and outputs predictions.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create an LSTM class inheriting from `nn.Module`\n",
    "\n",
    "    - Consider the appropriate input size, hidden size, and number of layers\n",
    "    - Use `nn.LSTM` for the recurrent layers\n",
    "    - Use `nn.Linear` for the output layer\n",
    "\n",
    "2. Initialize the model parameters\n",
    "\n",
    "    - Set the size of hidden states\n",
    "    - Configure the number of LSTM layers\n",
    "    - Set the output dimension based on your prediction task\n",
    "\n",
    "3. Implement the forward pass\n",
    "\n",
    "    - Initialize hidden states using torch.zeros\n",
    "    - Pass input through the LSTM layer with proper hidden state handling\n",
    "    - Extract the relevant output and pass through final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c68bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Design the LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        \"\"\"\n",
    "        Initialize LSTM model architecture\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        \n",
    "# Initialize the model with appropriate parameters\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dfa56b",
   "metadata": {},
   "source": [
    "**üí° Tip:** For time series prediction, your input_size is typically 1 (for univariate data) and output_size is 1 (for single-step prediction), but hidden_size can be larger (e.g., 50-100) to capture complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaab860",
   "metadata": {},
   "source": [
    "## Task 3: Train the LSTM Model\n",
    "**Context:** Training an LSTM involves feeding sequences, calculating loss, and updating weights through backpropagation. The process requires careful monitoring to ensure the model is learning effectively.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define loss function and optimizer\n",
    "\n",
    "    - Use `nn.MSELoss()` for regression tasks\n",
    "    - Configure `optim.Adam` with an appropriate learning rate\n",
    "\n",
    "2. Create a training loop\n",
    "\n",
    "    - Iterate through your training data in batches\n",
    "    - Zero gradients with `optimizer.zero_grad()`\n",
    "    - Compute model outputs and loss\n",
    "    - Call `loss.backward()` and `optimizer.step()` to update weights\n",
    "\n",
    "3. Track and display training progress\n",
    "\n",
    "    - Store loss values for plotting\n",
    "    - Print progress at regular intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Train the LSTM Model\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# Your code here\n",
    "\n",
    "# Set training parameters\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "\n",
    "# Training loop\n",
    "# Your code here\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce2d416",
   "metadata": {},
   "source": [
    "**üí° Tip:** Consider implementing early stopping by monitoring validation loss to prevent overfitting, especially with longer training runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b86998",
   "metadata": {},
   "source": [
    "## Task 4: Evaluate and Visualize Predictions\n",
    "**Context:** After training, you need to evaluate how well your model can predict future values and visualize the results to understand its performance.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Set the model to evaluation mode\n",
    "\n",
    "    - Use `model.eval()` to disable dropout and other training-specific behaviors\n",
    "    - Use `torch.no_grad()` context manager to disable gradient calculation\n",
    "\n",
    "2. Generate predictions on test data\n",
    "\n",
    "    - Feed test sequences through the model\n",
    "    - Convert predictions back to numpy arrays for visualization\n",
    "\n",
    "3. Create visualization comparing predictions with actual values\n",
    "\n",
    "    - Use `matplotlib.pyplot` to plot actual vs. predicted values\n",
    "    - Add appropriate labels and title to clarify the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d1f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Evaluate and Visualize Predictions\n",
    "\n",
    "# Set model to evaluation mode\n",
    "# Your code here\n",
    "\n",
    "# Generate predictions\n",
    "# Your code here\n",
    "\n",
    "# Convert tensors to numpy arrays\n",
    "# Your code here\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Your code here\n",
    "plt.title('LSTM Time Series Prediction')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate error metrics\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2d0ff",
   "metadata": {},
   "source": [
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Run a forward pass with a sample sequence and verify that:\n",
    "\n",
    "    - The output shape matches your expected prediction shape\n",
    "    - The predictions follow a similar pattern to your input data\n",
    "    - The error metrics show reasonable performance\n",
    "    \n",
    "## ‚úÖ Success Checklist\n",
    "- Data successfully transformed into sequence format\n",
    "- LSTM model implemented with correct architecture\n",
    "- Model trained with decreasing loss over time\n",
    "- Predictions generated and visualized against actual values\n",
    "- Program runs without errors\n",
    "\n",
    "## üîç Common Issues & Solutions\n",
    "**Problem:** Loss doesn't decrease during training **Solution:** Check learning rate (try a smaller value like 0.001), ensure data is normalized, and verify that your sequence preparation is correct.\n",
    "\n",
    "**Problem:** Model outputs same value regardless of input **Solution:** Your model might be underfitting. Try increasing model complexity (hidden size, layers) or training for more epochs.\n",
    "\n",
    "**Problem:** \"Expected hidden[0] size...\" error **Solution:** Ensure hidden state dimensions match batch size and other LSTM parameters. Check that your model's forward method correctly initializes hidden states.\n",
    "\n",
    "## üîë Key Points\n",
    "- LSTMs are particularly effective for time series because they can learn and remember long-term dependencies\n",
    "- Data preparation is crucial - sequence length affects prediction quality\n",
    "- Proper model evaluation helps verify that your LSTM has learned meaningful patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822be1ba",
   "metadata": {},
   "source": [
    "## üíª Reference Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd9b41",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><strong>Click HERE to see a reference solution</strong></summary>    \n",
    "    \n",
    "```python\n",
    "# Task 1: Prepare Sequential Data\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Transform a time series into sequences for LSTM processing\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        target = data[i+seq_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Set sequence length (window size for prediction)\n",
    "seq_length = 20\n",
    "\n",
    "# Create sequences from time series data\n",
    "X, y = create_sequences(data, seq_length)\n",
    "\n",
    "# Split data into training and testing sets (80/20 split)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(2)  # Add feature dimension\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).unsqueeze(2)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create PyTorch dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Task 2: Design the LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        \"\"\"\n",
    "        Initialize LSTM model architecture\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \"\"\"\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Get output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model with appropriate parameters\n",
    "input_size = 1  # Single feature (univariate time series)\n",
    "hidden_size = 64  # Size of LSTM hidden units\n",
    "num_layers = 2  # Number of stacked LSTM layers\n",
    "output_size = 1  # Single output (next value prediction)\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Task 3: Train the LSTM Model\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set training parameters\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Track training progress\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Task 4: Evaluate and Visualize Predictions\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    test_predictions = []\n",
    "    for inputs, _ in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        test_predictions.append(outputs.numpy())\n",
    "    \n",
    "    # Flatten the predictions list\n",
    "    test_predictions = np.concatenate(test_predictions).flatten()\n",
    "\n",
    "# Convert tensors to numpy arrays\n",
    "actual_values = y_test.numpy()\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actual_values, label='Actual')\n",
    "plt.plot(test_predictions, label='Predicted', linestyle='--')\n",
    "plt.title('LSTM Time Series Prediction')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate error metrics\n",
    "mse = np.mean((test_predictions - actual_values) ** 2)\n",
    "mae = np.mean(np.abs(test_predictions - actual_values))\n",
    "print(f'Test MSE: {mse:.4f}')\n",
    "print(f'Test MAE: {mae:.4f}')\n",
    "```    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
