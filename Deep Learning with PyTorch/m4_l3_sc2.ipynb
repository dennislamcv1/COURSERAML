{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Faster: Mixed Precision with torch.cuda.amp"
      ],
      "metadata": {
        "id": "1a61wrQhkf1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setting Up the PyTorch Environment for Mixed Precision\n"
      ],
      "metadata": {
        "id": "9E00ouCikfrM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwmTdVjijP7y",
        "outputId": "2a01e5b1-119c-4f79-99a2-aee5041e2bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Training on GPU.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import time\n",
        "\n",
        "# Configuration\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 64 # For dummy data\n",
        "NUM_CLASSES = 10 # Example number of classes\n",
        "INPUT_CHANNELS = 3\n",
        "IMAGE_SIZE = 32 # Example image size (height and width)\n",
        "NUM_SAMPLES = BATCH_SIZE * 10 # Number of dummy samples\n",
        "\n",
        "# Check for CUDA availability and set device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA is available. Training on GPU.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA not available. Training on CPU. Mixed precision (AMP) is a CUDA feature and will be disabled.\")\n",
        "\n",
        "# AMP is only available on CUDA\n",
        "# This flag will control whether AMP is used.\n",
        "# For the demonstration, we'll run once with AMP and once without.\n",
        "# use_amp_globally = torch.cuda.is_available() # Global flag, will be set per run in evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Defining the Model and Preparing Data\n"
      ],
      "metadata": {
        "id": "0BzGSQ_kks15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3, image_size=32):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layer\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calculate flattened feature size after conv and pool\n",
        "        pooled_size = image_size // 2  # Pooling halves the spatial dimensions\n",
        "        self.fc1_input_features = 64 * pooled_size * pooled_size\n",
        "        self.fc1 = nn.Linear(self.fc1_input_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, self.fc1_input_features)  # Flatten tensor\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Create dummy data and DataLoader\n",
        "dummy_images = torch.randn(NUM_SAMPLES, INPUT_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)  # (N, C, H, W)\n",
        "dummy_labels = torch.randint(0, NUM_CLASSES, (NUM_SAMPLES,))\n",
        "train_dataset = TensorDataset(dummy_images, dummy_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "M-9mJg-7kubK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Training Loop with and without Mixed Precision\n"
      ],
      "metadata": {
        "id": "CFwTF68JkybL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, optimizer, criterion, device, num_epochs, use_amp):\n",
        "    \"\"\"\n",
        "    A generic training function.\n",
        "    If use_amp is True and device is CUDA, mixed precision will be used.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize GradScaler only if AMP is used and on CUDA\n",
        "    # Pass enabled=False to effectively disable scaler if use_amp is False or not on CUDA\n",
        "    scaler = GradScaler(enabled=use_amp and device.type == 'cuda')\n",
        "\n",
        "    epoch_times = []\n",
        "\n",
        "    print(f\"\\nTraining with AMP: {'Enabled' if use_amp and device.type == 'cuda' else 'Disabled'}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time_epoch = time.time()\n",
        "        total_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Autocast context manager\n",
        "            # enabled=False effectively makes autocast a no-op\n",
        "            with autocast(enabled=use_amp and device.type == 'cuda'):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            if use_amp and device.type == 'cuda':\n",
        "                # Scale the gradients and perform backward pass\n",
        "                scaler.scale(loss).backward()\n",
        "                # Unscale gradients and step optimizer\n",
        "                scaler.step(optimizer)\n",
        "                # Update the scale for next iteration\n",
        "                scaler.update()\n",
        "            else:\n",
        "                # Standard backward pass and optimizer step if not using AMP\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        end_time_epoch = time.time()\n",
        "        epoch_duration = end_time_epoch - start_time_epoch\n",
        "        epoch_times.append(epoch_duration)\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Duration: {epoch_duration:.2f}s\")\n",
        "\n",
        "    total_training_time = sum(epoch_times)\n",
        "    print(f\"Total training time: {total_training_time:.2f}s\")\n",
        "    if len(epoch_times) > 1: # Print average excluding potential first epoch overhead\n",
        "        avg_epoch_time = sum(epoch_times[1:]) / len(epoch_times[1:]) if len(epoch_times) > 1 else epoch_times[0]\n",
        "        print(f\"Average epoch time (excluding first): {avg_epoch_time:.2f}s\")\n",
        "    elif epoch_times:\n",
        "        print(f\"Average epoch time: {epoch_times[0]:.2f}s\")\n",
        "\n",
        "    return total_training_time\n",
        "\n",
        "# --- Training without Mixed Precision ---\n",
        "print(\"--- Simulating training WITHOUT mixed precision ---\")\n",
        "model_no_amp = SimpleCNN(num_classes=NUM_CLASSES, input_channels=INPUT_CHANNELS, image_size=IMAGE_SIZE)\n",
        "optimizer_no_amp = optim.Adam(model_no_amp.parameters(), lr=0.001)\n",
        "criterion_no_amp = nn.CrossEntropyLoss()\n",
        "\n",
        "# Ensure use_amp is False for this run if on CUDA, otherwise it's already effectively disabled\n",
        "time_without_mixed_precision = train_model(\n",
        "    model_no_amp, train_loader, optimizer_no_amp, criterion_no_amp,\n",
        "    device, NUM_EPOCHS, use_amp=False\n",
        ")\n",
        "\n",
        "# --- Training with Mixed Precision (if CUDA is available) ---\n",
        "if device.type == 'cuda':\n",
        "    print(\"\\n--- Simulating training WITH mixed precision ---\")\n",
        "    model_amp = SimpleCNN(num_classes=NUM_CLASSES, input_channels=INPUT_CHANNELS, image_size=IMAGE_SIZE)\n",
        "    optimizer_amp = optim.Adam(model_amp.parameters(), lr=0.001)\n",
        "    criterion_amp = nn.CrossEntropyLoss()\n",
        "\n",
        "    time_with_mixed_precision = train_model(\n",
        "        model_amp, train_loader, optimizer_amp, criterion_amp,\n",
        "        device, NUM_EPOCHS, use_amp=True\n",
        "    )\n",
        "else:\n",
        "    print(\"\\nSkipping mixed precision training simulation as CUDA is not available.\")\n",
        "    time_with_mixed_precision = float('inf') # Indicate AMP wasn't run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okUdq8kkkzpT",
        "outputId": "636d75b8-6043-4474-d5f3-d29c82657d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Simulating training WITHOUT mixed precision ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-3506230078>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=use_amp and device.type == 'cuda')\n",
            "<ipython-input-3-3506230078>:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp and device.type == 'cuda'):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with AMP: Disabled\n",
            "Epoch [1/8], Loss: 10.2376, Duration: 1.24s\n",
            "Epoch [2/8], Loss: 7.1103, Duration: 0.04s\n",
            "Epoch [3/8], Loss: 4.4700, Duration: 0.04s\n",
            "Epoch [4/8], Loss: 2.5604, Duration: 0.04s\n",
            "Epoch [5/8], Loss: 1.8413, Duration: 0.04s\n",
            "Epoch [6/8], Loss: 1.5037, Duration: 0.04s\n",
            "Epoch [7/8], Loss: 1.2639, Duration: 0.04s\n",
            "Epoch [8/8], Loss: 1.0519, Duration: 0.04s\n",
            "Total training time: 1.51s\n",
            "Average epoch time (excluding first): 0.04s\n",
            "\n",
            "--- Simulating training WITH mixed precision ---\n",
            "\n",
            "Training with AMP: Enabled\n",
            "Epoch [1/8], Loss: 8.6714, Duration: 0.41s\n",
            "Epoch [2/8], Loss: 5.5980, Duration: 0.04s\n",
            "Epoch [3/8], Loss: 3.0717, Duration: 0.04s\n",
            "Epoch [4/8], Loss: 2.0788, Duration: 0.04s\n",
            "Epoch [5/8], Loss: 1.5985, Duration: 0.04s\n",
            "Epoch [6/8], Loss: 1.3114, Duration: 0.04s\n",
            "Epoch [7/8], Loss: 1.0572, Duration: 0.04s\n",
            "Epoch [8/8], Loss: 0.8335, Duration: 0.04s\n",
            "Total training time: 0.70s\n",
            "Average epoch time (excluding first): 0.04s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Evaluating Results with Mixed Precision\n"
      ],
      "metadata": {
        "id": "Z31YeJhsk4AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Evaluation Summary ---\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"Time taken without mixed precision: {time_without_mixed_precision:.2f} seconds\")\n",
        "    print(f\"Time taken with mixed precision: {time_with_mixed_precision:.2f} seconds\")\n",
        "    if time_with_mixed_precision < time_without_mixed_precision:\n",
        "        speedup = (time_without_mixed_precision / time_with_mixed_precision)\n",
        "        print(f\"Mixed precision training was {speedup:.2f}x faster.\")\n",
        "        print(\"Training completed more efficiently with mixed precision (if GPU supports it well and model/batch size are large enough).\")\n",
        "    elif time_with_mixed_precision > time_without_mixed_precision :\n",
        "        print(\"Mixed precision training was slower. This can happen for very small models/batches or certain GPU architectures.\")\n",
        "    else:\n",
        "        print(\"No significant speed difference observed.\")\n",
        "else:\n",
        "    print(f\"Time taken (on CPU, no mixed precision): {time_without_mixed_precision:.2f} seconds\")\n",
        "    print(\"Mixed precision is a CUDA feature and was not run.\")\n",
        "\n",
        "print(\"\\nNote: Actual speedup from mixed precision depends on the GPU architecture, model complexity, and batch size.\")\n",
        "print(\"For small models or small batches like in this demo, the overhead of AMP might sometimes outweigh benefits,\")\n",
        "print(\"or the difference might not be substantial. Benefits are more pronounced in larger, more complex scenarios.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQcbfygyk5D6",
        "outputId": "17a67e1e-37a4-4122-86a9-32a843f31d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation Summary ---\n",
            "Time taken without mixed precision: 1.51 seconds\n",
            "Time taken with mixed precision: 0.70 seconds\n",
            "Mixed precision training was 2.17x faster.\n",
            "Training completed more efficiently with mixed precision (if GPU supports it well and model/batch size are large enough).\n",
            "\n",
            "Note: Actual speedup from mixed precision depends on the GPU architecture, model complexity, and batch size.\n",
            "For small models or small batches like in this demo, the overhead of AMP might sometimes outweigh benefits,\n",
            "or the difference might not be substantial. Benefits are more pronounced in larger, more complex scenarios.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "obLbJ8F6oxW9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}