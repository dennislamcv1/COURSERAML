{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c6c4b6",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Tensor Operations, Gradients, and GPU Practice\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you will gain hands-on experience with fundamental PyTorch components that form the backbone of deep learning workflows. You'll create and manipulate tensors, compute gradients automatically, and leverage GPU acceleration to speed up your computations. By the end of this lab, you'll understand how these elements work together to enable efficient deep learning model development and training.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Create and manipulate tensors using various PyTorch operations\n",
    "- Compute gradients automatically using PyTorch's autograd functionality\n",
    "- Move computations between CPU and GPU to observe performance differences\n",
    "- Implement basic tensor reshaping and broadcasting techniques\n",
    "\n",
    "## üöÄ Starting Point\n",
    "Access the starter code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f207c16",
   "metadata": {},
   "source": [
    "Required tools/setup:\n",
    "\n",
    "- Python 3.6 or later\n",
    "- PyTorch installed (1.7 or later recommended)\n",
    "- Access to a GPU is beneficial but not required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef14fd9",
   "metadata": {},
   "source": [
    "## Task 1: Create Tensors and Perform Matrix Operations\n",
    "**Context:** In deep learning, you frequently need to create matrices and vectors to represent data and model parameters. Matrix operations form the foundation of neural network computations.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a 3x3 matrix tensor using `torch.tensor()`\n",
    "    - Use a nested list structure to define your matrix values\n",
    "    - Consider using floating point values for better compatibility\n",
    "\n",
    "2. Create a 3x1 vector tensor\n",
    "\n",
    "    - Make sure the dimensions are compatible for matrix multiplication\n",
    "\n",
    "3. Perform matrix multiplication using `torch.mm()`\n",
    "\n",
    "    - Why does `torch.mm()` require specific dimension compatibility? What would happen if dimensions don't match?\n",
    "    - How is this different from element-wise multiplication?\n",
    "\n",
    "4. Print the result and verify the shape is as expected (should be 3x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c29d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for creating matrices and performing operations\n",
    "# [CODE GOES HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550c4fa",
   "metadata": {},
   "source": [
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Verify the shape of your result matches what you expect\n",
    "- Try multiplying your matrices in reverse order - what happens and why?\n",
    "\n",
    "**üí° Tip:** Remember that matrix multiplication is not commutative - AB ‚â† BA in most cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef847f7",
   "metadata": {},
   "source": [
    "## Task 2: Reshape and Broadcast Tensors\n",
    "**Context:** When working with neural networks, you'll often need to reshape data or perform operations between tensors of different shapes. PyTorch's broadcasting makes this easier.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a 1D tensor with 6 elements using `torch.arange()`\n",
    "\n",
    "    - What does `torch.arange()` do and how is it different from `torch.tensor()`?\n",
    "\n",
    "2. Reshape this 1D tensor into a 2x3 2D tensor using `.view()`\n",
    "\n",
    "    - How does `.view()` compare to `.reshape()`? When might you use one over the other?\n",
    "\n",
    "3. Create another 2x1 tensor with values [[1], [2]]\n",
    "\n",
    "4. Add these tensors together using broadcasting\n",
    "\n",
    "    - Think about: How does PyTorch determine which dimensions to broadcast?\n",
    "    - Try to predict the output shape before running your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25def8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for reshaping and broadcasting tensors\n",
    "# [CODE GOES HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d2011b",
   "metadata": {},
   "source": [
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Verify that the resulting tensor has the shape 2x3\n",
    "- Confirm the values match what you expected from broadcasting\n",
    "\n",
    "**üí° Tip:** Broadcasting automatically expands smaller tensors to match the shape of larger ones without making copies of data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ef717",
   "metadata": {},
   "source": [
    "## Task 3: Move Tensors Between CPU and GPU\n",
    "**Context:** Modern deep learning relies on GPU acceleration to process large datasets efficiently. Understanding how to move data between devices is essential for optimizing performance.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a large tensor (1000x1000) on the CPU using `torch.rand()`\n",
    "\n",
    "    - How does `torch.rand()` differ from `torch.randn()`?\n",
    "\n",
    "2. Time how long it takes to square all values in this tensor on the CPU\n",
    "\n",
    "    - Use the `time` module to measure execution time\n",
    "    - Why use element-wise operations for testing performance differences?\n",
    "\n",
    "3. Move your tensor to the GPU using `.to(device)`\n",
    "\n",
    "    - What happens if you try to move to GPU on a system without one?\n",
    "\n",
    "4. Time the same square operation on the GPU\n",
    "\n",
    "    - Remember to include the time.time() calls before and after the operation\n",
    "\n",
    "5. Compare and print the execution times\n",
    "\n",
    "    - What factors might affect the performance difference you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca92300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for comparing CPU and GPU performance\n",
    "# [CODE GOES HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bab6ae",
   "metadata": {},
   "source": [
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- If GPU is available, the GPU operation should be faster\n",
    "- Try with different tensor sizes to see how the performance gap changes\n",
    "\n",
    "**üí° Tip:** For small tensors, the overhead of moving data to the GPU might outweigh the performance benefit!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3ac9f",
   "metadata": {},
   "source": [
    "## Task 4: Define Custom Functions and Utilize Autograd\n",
    "**Context:** The ability to automatically compute gradients is essential for training neural networks. PyTorch's autograd system handles this complex task for you.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a tensor with `requires_grad=True` to enable gradient tracking\n",
    "\n",
    "    - Why do we need to set this flag for gradient computation?\n",
    "\n",
    "2. Define a custom mathematical function (x¬≥ + 2x¬≤)\n",
    "\n",
    "    - Consider how PyTorch builds a computation graph for this operation\n",
    "\n",
    "3. Apply this function to your tensor\n",
    "\n",
    "4. Call `.backward()` on the result to compute gradients\n",
    "\n",
    "    - What does calling backward() do in PyTorch?\n",
    "    - How is this related to backpropagation in neural networks?\n",
    "\n",
    "5. Print the gradient values stored in `.grad` attribute of your input tensor\n",
    "\n",
    "    - Check if the values match what you would expect from manual differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d33ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for gradient computation\n",
    "# [CODE GOES HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e54bc2",
   "metadata": {},
   "source": [
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Verify that the gradient values match the derivative of our function (3x¬≤ + 4x)\n",
    "- Try with different input values to ensure the gradient calculation is correct\n",
    "\n",
    "**üí° Tip:** Understanding autograd is crucial for debugging training issues in deep learning!\n",
    "\n",
    "## ‚úÖ Success Checklist\n",
    "- Successfully created tensors and performed matrix multiplication\n",
    "- Correctly reshaped tensors and utilized broadcasting\n",
    "- Moved computations between CPU and GPU and observed performance differences\n",
    "- Defined a custom function and computed gradients using autograd\n",
    "- Program runs without errors\n",
    "\n",
    "## üîç Common Issues & Solutions\n",
    "**Problem:** Dimension mismatch in matrix operations\n",
    "**Solution:** Double-check tensor shapes before operations and use `.shape` to debug\n",
    "\n",
    "**Problem:** CUDA out of memory error\n",
    "**Solution:** Reduce tensor size or batch size when working with GPU\n",
    "\n",
    "**Problem:** Gradients aren't being computed\n",
    "**Solution:** Ensure you've set `requires_grad=True` and that you're calling `.backward()` on a scalar tensor\n",
    "\n",
    "## üîë Key Points\n",
    "- Tensors are the fundamental data structure in PyTorch, similar to NumPy arrays but with GPU support\n",
    "- Broadcasting eliminates the need for explicit tensor resizing in many operations\n",
    "- GPUs can significantly accelerate tensor operations, especially for large tensors\n",
    "- Autograd automatically computes gradients by tracking operations in a dynamic computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee68423",
   "metadata": {},
   "source": [
    "## üíª Reference Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e8498",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see a reference solution</strong></summary>    \n",
    "\n",
    "```python\n",
    "# Task 1: Create Tensors and Perform Matrix Operations\n",
    "matrix_3x3 = torch.tensor([[1.0, 2.0, 3.0], \n",
    "                           [4.0, 5.0, 6.0], \n",
    "                           [7.0, 8.0, 9.0]])\n",
    "vector_3x1 = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "\n",
    "# Perform matrix multiplication\n",
    "result = torch.mm(matrix_3x3, vector_3x1)\n",
    "print(f\"Matrix multiplication result:\\n{result}\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "\n",
    "# Try reverse multiplication\n",
    "try:\n",
    "    reverse_result = torch.mm(vector_3x1, matrix_3x3)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error in reverse multiplication: {e}\")\n",
    "\n",
    "# Task 2: Reshape and Broadcast Tensors\n",
    "tensor1D = torch.arange(6)\n",
    "print(f\"Original 1D tensor: {tensor1D}\")\n",
    "\n",
    "reshaped_tensor = tensor1D.view(2, 3)\n",
    "print(f\"Reshaped 2x3 tensor:\\n{reshaped_tensor}\")\n",
    "\n",
    "tensor2D = torch.tensor([[1], [2]])\n",
    "print(f\"Second tensor shape: {tensor2D.shape}\")\n",
    "\n",
    "# Broadcasting example\n",
    "broadcasted_sum = reshaped_tensor + tensor2D\n",
    "print(f\"Result after broadcasting:\\n{broadcasted_sum}\")\n",
    "\n",
    "# Task 3: Move Tensors Between CPU and GPU\n",
    "# Create a large tensor\n",
    "large_tensor = torch.rand((1000, 1000))\n",
    "\n",
    "# Time the operation on CPU\n",
    "start_cpu = time.time()\n",
    "result_cpu = large_tensor ** 2\n",
    "end_cpu = time.time()\n",
    "cpu_time = end_cpu - start_cpu\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    large_tensor_gpu = large_tensor.to(device)\n",
    "    \n",
    "    # Time the operation on GPU\n",
    "    start_gpu = time.time()\n",
    "    result_gpu = large_tensor_gpu ** 2\n",
    "    # Ensure operation is complete before timing\n",
    "    torch.cuda.synchronize()\n",
    "    end_gpu = time.time()\n",
    "    gpu_time = end_gpu - start_gpu\n",
    "    \n",
    "    print(f\"CPU time: {cpu_time:.6f} seconds\")\n",
    "    print(f\"GPU time: {gpu_time:.6f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "else:\n",
    "    print(f\"CPU time: {cpu_time:.6f} seconds\")\n",
    "    print(\"GPU not available for comparison\")\n",
    "\n",
    "# Task 4: Define Custom Functions and Utilize Autograd\n",
    "# Create tensor with gradient tracking\n",
    "x_tensor = torch.tensor([2.0, 3.0, 4.0], requires_grad=True)\n",
    "\n",
    "# Define and apply custom function\n",
    "def custom_function(x):\n",
    "    return x**3 + 2*x**2\n",
    "\n",
    "y_tensor = custom_function(x_tensor)\n",
    "print(f\"Function output: {y_tensor}\")\n",
    "\n",
    "# Sum to get scalar for backward function\n",
    "y_sum = y_tensor.sum()\n",
    "\n",
    "# Compute gradients\n",
    "y_sum.backward()\n",
    "\n",
    "# Print gradients\n",
    "print(f\"Input values: {x_tensor}\")\n",
    "print(f\"Computed gradients: {x_tensor.grad}\")\n",
    "print(f\"Expected gradients (3x¬≤ + 4x): {3*x_tensor**2 + 4*x_tensor}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
