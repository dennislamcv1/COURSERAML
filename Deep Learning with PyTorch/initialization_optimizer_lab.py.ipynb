{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bebe3e",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Experiment with Initialization and Optimizer Combinations\n",
    "\n",
    "## üìã Overview\n",
    "In this hands-on lab, you'll explore how different weight initialization techniques and optimization algorithms affect neural network training. You'll implement He and Xavier initialization methods and experiment with popular optimizers like SGD with momentum, Adam, and RMSprop. By comparing their convergence patterns and performance metrics, you'll gain practical insights into selecting the right combination for efficient model training‚Äîa crucial skill for deep learning practitioners working on real-world applications.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Implement and compare different weight initialization techniques (He and Xavier) in PyTorch\n",
    "- Configure and evaluate multiple optimizers (SGD with momentum, Adam, RMSprop) for neural network training\n",
    "- Analyze convergence speed, stability, and accuracy across initialization-optimizer combinations\n",
    "- Visualize training metrics to identify optimal training configurations\n",
    "\n",
    "## üöÄ Starting Point\n",
    "Access the starter code in the provided `initialization_optimizer_lab.py`file.\n",
    "\n",
    "Required tools/setup:\n",
    "\n",
    "- PyTorch (1.7.0 or higher)\n",
    "- torchvision\n",
    "- matplotlib\n",
    "- numpy\n",
    "\n",
    "Make sure you have completed the previous labs on neural network fundamentals and are familiar with PyTorch basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c7b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data loading and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Define the CNN model architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31491e54",
   "metadata": {},
   "source": [
    "## Task 1: Implement Weight Initialization Techniques\n",
    "**Context:** Weight initialization is critical for neural network training. Poor initialization can lead to vanishing/exploding gradients, while proper initialization helps achieve faster convergence.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a function to initialize the CNN model with He initialization\n",
    "\n",
    "    - Use `nn.init.kaiming_normal_` function to initialize convolutional and linear layers\n",
    "    - Remember that He initialization is designed for ReLU activations\n",
    "\n",
    "2. Create a function to initialize the CNN model with Xavier/Glorot initialization\n",
    "\n",
    "    - Use `nn.init.xavier_normal_` function for weight initialization\n",
    "    - Xavier initialization is better suited for tanh and sigmoid activations, but we'll compare its performance with ReLU\n",
    "\n",
    "3. Test both initialization methods by creating two separate model instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededb83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for implementing weight initialization techniques\n",
    "# Initialize models with different weight initialization strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0352e98",
   "metadata": {},
   "source": [
    "**üí° Tip:** When applying initialization, make sure to only initialize weights, not biases. Apply initialization only to convolutional and linear layers.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Print the mean and standard deviation of weights for both initialization methods\n",
    "- Expected output should show different distributions: He initialization typically has larger values than Xavier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf552ee",
   "metadata": {},
   "source": [
    "## Task 2: Configure Optimizers for Training\n",
    "**Context:** Different optimizers use different strategies to update model weights, which can significantly impact training dynamics, convergence speed, and final model performance.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a function to set up SGD with momentum optimizer\n",
    "\n",
    "    - Use `torch.optim.SGD` with appropriate learning rate and momentum parameters\n",
    "    - Set momentum to 0.9 for stability\n",
    "\n",
    "2. Create a function to configure Adam optimizer\n",
    "\n",
    "    - Use `torch.optim.Adam` with appropriate learning rate\n",
    "    - Use default beta values unless you want to experiment with them\n",
    "\n",
    "3. Create a function to configure RMSprop optimizer\n",
    "\n",
    "    - Use `torch.optim.RMSprop` with appropriate learning rate\n",
    "    - Set alpha (smoothing constant) to 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab0fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for configuring different optimizers\n",
    "# Set up different optimizers with appropriate parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3da21",
   "metadata": {},
   "source": [
    "**üí° Tip:** Different optimizers may require different learning rates. Adam typically works well with learning rates around 0.001, while SGD might need higher learning rates like 0.01.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Create a simple test case applying each optimizer to update weights\n",
    "- Verify that each optimizer is correctly configured with the specified parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f942ef",
   "metadata": {},
   "source": [
    "## Task 3: Implement Training Loop and Experiments\n",
    "**Context:** A systematic approach to experimentation helps isolate the effects of initialization and optimizer choices on model training.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a training function that takes a model, optimizer, and number of epochs\n",
    "\n",
    "    - Loop through batches of training data\n",
    "    - Calculate loss using `nn.CrossEntropyLoss()`\n",
    "    - Track and return training loss history, validation accuracy per epoch, and training time\n",
    "\n",
    "2. Run experiments combining different initialization methods and optimizers\n",
    "\n",
    "    - Create a total of 6 combinations: 2 initializations √ó 3 optimizers\n",
    "    - Train each combination for the same number of epochs (e.g., 10)\n",
    "\n",
    "3. Store results in a dictionary or list for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for implementing the training loop and experiments\n",
    "# Run experiments with different initialization-optimizer combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e361a7ef",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `time.time()` to measure training duration for each experiment. This will help you compare computational efficiency across different configurations.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- After each epoch, evaluate the model on the validation set\n",
    "- Make sure loss is decreasing and accuracy is improving over time\n",
    "- Expected behavior: Some combinations should train faster or achieve better accuracy than others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfae9a5",
   "metadata": {},
   "source": [
    "## Task 4: Visualize and Analyze Results\n",
    "**Context:** Visualization is essential for understanding complex training dynamics and identifying the best-performing configurations.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create plots for training loss across epochs for each configuration\n",
    "\n",
    "    - Use matplotlib to create line plots\n",
    "    - Use different colors or line styles for different configurations\n",
    "    - Include a legend to identify each configuration\n",
    "\n",
    "2. Create plots for validation accuracy across epochs\n",
    "\n",
    "    - Plot accuracy curves for all configurations on the same graph\n",
    "    - Add appropriate labels, title, and legend\n",
    "\n",
    "3. Create a bar chart comparing final accuracy and training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda48452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for visualizing and analyzing results\n",
    "# Create plots for training dynamics and final performance comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2474700",
   "metadata": {},
   "source": [
    "**üí° Tip:** Use `plt.subplots()` to create multiple plots in a grid layout for better comparison between different metrics.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Ensure plots clearly show differences between configurations\n",
    "- Check that axis labels and legends are readable and informative\n",
    "- Expected output: Visual differences in convergence speed and final metrics across different configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5dc79",
   "metadata": {},
   "source": [
    "## Task 5: Analyze and Document Findings\n",
    "**Context:** Drawing correct conclusions from experimental results is a critical skill in machine learning research and practice.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Identify the best-performing initialization-optimizer combination based on:\n",
    "\n",
    "    - Final validation accuracy\n",
    "    - Convergence speed (epochs to reach a target accuracy)\n",
    "    - Training time efficiency\n",
    "\n",
    "2. Analyze the relationship between initialization methods and optimizers\n",
    "\n",
    "    - Identify which combinations work well together\n",
    "    - Note any initialization-optimizer combinations that performed poorly\n",
    "\n",
    "3. Document your findings with specific evidence from the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a750f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for calculating and comparing key performance metrics\n",
    "# Extract and present final results in a structured format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e06bc",
   "metadata": {},
   "source": [
    "**üí° Tip:** When comparing configurations, consider both absolute performance and efficiency. Some setups might reach slightly better accuracy but take significantly longer to train.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "\n",
    "- Create a summary table with key metrics for all configurations\n",
    "- Verify your conclusions are supported by the data in your plots and tables\n",
    "\n",
    "## ‚úÖ Success Checklist\n",
    "- Both He and Xavier initialization methods are correctly implemented\n",
    "- All three optimizers (SGD with momentum, Adam, RMSprop) are properly configured\n",
    "- Training loop successfully tracks and reports relevant metrics\n",
    "- Visualizations clearly demonstrate differences between configurations\n",
    "- Analysis identifies the best-performing configuration with supporting evidence\n",
    "- Code runs without errors and produces consistent results\n",
    "\n",
    "## üîç Common Issues & Solutions\n",
    "**Problem:** Loss becomes NaN during training **Solution:** Reduce the learning rate or check for improper initialization causing gradient explosion.\n",
    "\n",
    "**Problem:** Some optimizer-initialization combinations show no learning **Solution:** Try adjusting the learning rate for that specific combination; different optimizers often require different learning rates.\n",
    "\n",
    "**Problem:** Training is extremely slow on CPU **Solution:** Reduce batch size or number of epochs; consider using Google Colab with GPU acceleration if available.\n",
    "\n",
    "**Problem:** Memory errors when running all experiments **Solution:** Run experiments sequentially instead of storing all models in memory at once.\n",
    "\n",
    "## üîë Key Points\n",
    "- Initialization methods significantly impact early training dynamics and can determine whether a model converges at all\n",
    "- Adaptive optimizers like Adam often converge faster than SGD but may generalize differently\n",
    "- The best initialization-optimizer combination depends on your specific architecture and dataset\n",
    "- Systematic experimentation and visualization are essential tools for understanding deep learning behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c516407",
   "metadata": {},
   "source": [
    "## üíª Reference Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d94ee3",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><strong>Click HERE to see a reference solution</strong></summary>    \n",
    "    \n",
    "```python\n",
    "# Task 1: Implement Weight Initialization Techniques\n",
    "def initialize_he(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    return model\n",
    "\n",
    "def initialize_xavier(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    return model\n",
    "\n",
    "# Print weight statistics to verify initialization\n",
    "def print_weight_stats(model, name):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} - {param.mean().item():.4f} ¬± {param.std().item():.4f}\")\n",
    "\n",
    "# Create models with different initializations\n",
    "model_he = CNN().to(device)\n",
    "model_he = initialize_he(model_he)\n",
    "print(\"He initialization weight statistics:\")\n",
    "print_weight_stats(model_he, \"He\")\n",
    "\n",
    "model_xavier = CNN().to(device)\n",
    "model_xavier = initialize_xavier(model_xavier)\n",
    "print(\"\\nXavier initialization weight statistics:\")\n",
    "print_weight_stats(model_xavier, \"Xavier\")\n",
    "\n",
    "# Task 2: Configure Optimizers for Training\n",
    "def get_sgd_optimizer(model, lr=0.01):\n",
    "    return optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "def get_adam_optimizer(model, lr=0.001):\n",
    "    return optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "def get_rmsprop_optimizer(model, lr=0.001):\n",
    "    return optim.RMSprop(model.parameters(), lr=lr, alpha=0.99)\n",
    "\n",
    "# Helper functions for model evaluation\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Task 3: Implement Training Loop and Experiments\n",
    "def train_model(model, optimizer, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        # Calculate average loss for the epoch\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_accuracy = evaluate_model(model, test_loader)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Training completed in {training_time:.2f} seconds')\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'training_time': training_time,\n",
    "        'final_accuracy': val_accuracies[-1]\n",
    "    }\n",
    "\n",
    "# Run experiments with different combinations\n",
    "experiments = {}\n",
    "epochs = 10\n",
    "\n",
    "# He + SGD\n",
    "model = initialize_he(CNN().to(device))\n",
    "optimizer = get_sgd_optimizer(model)\n",
    "print(\"\\nTraining He initialization with SGD:\")\n",
    "experiments['He + SGD'] = train_model(model, optimizer, epochs)\n",
    "\n",
    "# He + Adam\n",
    "model = initialize_he(CNN().to(device))\n",
    "optimizer = get_adam_optimizer(model)\n",
    "print(\"\\nTraining He initialization with Adam:\")\n",
    "experiments['He + Adam'] = train_model(model, optimizer, epochs)\n",
    "\n",
    "# He + RMSprop\n",
    "model = initialize_he(CNN().to(device))\n",
    "optimizer = get_rmsprop_optimizer(model)\n",
    "print(\"\\nTraining He initialization with RMSprop:\")\n",
    "experiments['He + RMSprop'] = train_model(model, optimizer, epochs)\n",
    "\n",
    "# Xavier + SGD\n",
    "model = initialize_xavier(CNN().to(device))\n",
    "optimizer = get_sgd_optimizer(model)\n",
    "print(\"\\nTraining Xavier initialization with SGD:\")\n",
    "experiments['Xavier + SGD'] = train_model(model, optimizer, epochs)\n",
    "\n",
    "# Xavier + Adam\n",
    "model = initialize_xavier(CNN().to(device))\n",
    "optimizer = get_adam_optimizer(model)\n",
    "print(\"\\nTraining Xavier initialization with Adam:\")\n",
    "experiments['Xavier + Adam'] = train_model(model, optimizer, epochs)\n",
    "\n",
    "# Xavier + RMSprop\n",
    "model = initialize_xavier(CNN().to(device))\n",
    "optimizer = get_rmsprop_optimizer(model)\n",
    "print(\"\\nTraining Xavier initialization with RMSprop:\")\n",
    "experiments['Xavier + RMSprop'] = train_model(model, optimizer, epochs)\n",
    "\n",
    "# Task 4: Visualize and Analyze Results\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for name, exp in experiments.items():\n",
    "    plt.plot(range(1, epochs+1), exp['train_losses'], label=name)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "for name, exp in experiments.items():\n",
    "    plt.plot(range(1, epochs+1), exp['val_accuracies'], label=name)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png')\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for final accuracy and training time\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Final accuracy comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "names = list(experiments.keys())\n",
    "accuracies = [exp['final_accuracy'] for exp in experiments.values()]\n",
    "plt.bar(names, accuracies)\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Final Validation Accuracy')\n",
    "plt.title('Final Validation Accuracy by Configuration')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0.8, 0.95)  # Adjust based on your results\n",
    "\n",
    "# Training time comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "times = [exp['training_time'] for exp in experiments.values()]\n",
    "plt.bar(names, times)\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Training Time (s)')\n",
    "plt.title('Training Time by Configuration')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_metrics.png')\n",
    "plt.show()\n",
    "\n",
    "# Task 5: Analyze and Document Findings\n",
    "# Create a summary table of results\n",
    "summary = {\n",
    "    'Configuration': names,\n",
    "    'Final Accuracy': [f\"{acc:.4f}\" for acc in accuracies],\n",
    "    'Training Time (s)': [f\"{t:.2f}\" for t in times],\n",
    "    'Avg Epochs to 85%': []\n",
    "}\n",
    "\n",
    "for name, exp in experiments.items():\n",
    "    # Calculate epochs to reach 85% accuracy\n",
    "    epochs_to_85 = next((i+1 for i, acc in enumerate(exp['val_accuracies']) if acc >= 0.85), epochs)\n",
    "    summary['Avg Epochs to 85%'].append(epochs_to_85)\n",
    "\n",
    "# Find best configuration\n",
    "best_acc_idx = np.argmax(accuracies)\n",
    "best_time_idx = np.argmin(times)\n",
    "fastest_convergence_idx = np.argmin(summary['Avg Epochs to 85%'])\n",
    "\n",
    "print(\"\\n=== Performance Summary ===\")\n",
    "print(f\"Best accuracy: {names[best_acc_idx]} ({accuracies[best_acc_idx]:.4f})\")\n",
    "print(f\"Fastest training: {names[best_time_idx]} ({times[best_time_idx]:.2f}s)\")\n",
    "print(f\"Fastest convergence: {names[fastest_convergence_idx]} ({summary['Avg Epochs to 85%'][fastest_convergence_idx]} epochs to 85%)\")\n",
    "\n",
    "print(\"\\nConclusions:\")\n",
    "# Draw conclusions based on actual experiment results\n",
    "print(\"1. Adam optimizer consistently performs well regardless of initialization method\")\n",
    "print(\"2. He initialization generally leads to faster convergence with ReLU activations\")\n",
    "print(\"3. Training time differences between initializations are minimal compared to optimizer choices\")\n",
    "print(\"4. The optimal combination for this specific CNN architecture is [best combination based on results]\")\n",
    "```    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
