{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª **Transcribe and Generate Audio with Whisper + TTS**\n",
    "\n",
    "**Time Estimate:** 60 minutes\n",
    "\n",
    "## üìã **Overview**\n",
    "\n",
    "In this activity, you will work with two cutting-edge models to bridge the gap between text and audio‚ÄîWhisper for transcribing speech to text and a Text-to-Speech (TTS) model for converting text back to speech. These skills are crucial for roles in automated transcription services, accessible media creation, and multilingual communication tools.\n",
    "\n",
    "By gaining hands-on experience with Whisper and TTS technologies, you will solidify your understanding of their applications and challenges, enabling you to provide state-of-the-art solutions in projects requiring audio-visual integration.\n",
    "\n",
    "## üéØ **Learning Outcomes**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Transcribe audio files with diverse characteristics using the Whisper model.\n",
    "- Generate natural-sounding audio from text using a TTS model.\n",
    "- Analyze the performance of speech-to-text and text-to-speech processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Transcribe Audio with Whisper [30 minutes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torchaudio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will explore the capabilities of the Whisper model to accurately transcribe audio with varying languages, accents, and background noises.\n",
    "\n",
    "1. Initialize Whisper model and processor.\n",
    "2. Load and preprocess audio files.\n",
    "3. Generate transcriptions and evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç **Practice**\n",
    "\n",
    "Examine the Whisper transcription results. Consider:\n",
    "\n",
    "- How well the model handles different audio qualities and accents.\n",
    "- The accuracy of transcriptions across various languages or dialects.\n",
    "- Challenges with background noise or unclear speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Successfully load and process audio files.\n",
    "- Achieve accurate transcriptions.\n",
    "- Reflect on transcription challenges.\n",
    "\n",
    "üí° **Key Points**\n",
    "\n",
    "- Whisper is versatile but may struggle with low-quality audio or unfamiliar accents.\n",
    "- Preprocessing audio correctly is crucial for optimal transcriptions.\n",
    "- The model performs well across multiple languages without fine-tuning.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Not preprocessing audio adequately, leading to inaccurate transcriptions.\n",
    "- Using incorrect sampling rates, resulting in distorted audio input.\n",
    "- Overlooking audio quality issues that affect model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Generate Speech with TTS [25 minutes]\n",
    "Convert text to natural-sounding speech using TTS models.\n",
    "1. Initialize a Text-to-Speech model and processor.\n",
    "2. Experiment with different text prompts and voice settings.\n",
    "3. Generate audio files and assess speech quality.\n",
    "4. Evaluate naturalness and clarity of generated speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç **Practice**\n",
    "\n",
    "Reflect on the TTS generation results:\n",
    "\n",
    "- How natural and human-like the generated speech sounds.\n",
    "- The clarity and pronunciation accuracy of the output.\n",
    "- How well the model handles different text styles and content types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Generate audio files from text.\n",
    "- Ensure the generated speech aligns with the intended text tone.\n",
    "- Evaluate speech quality and naturalness.\n",
    "\n",
    "üí° **Key Points**\n",
    "\n",
    "- Modern TTS models can produce highly natural-sounding speech.\n",
    "- Text preprocessing and formatting can significantly impact output quality.\n",
    "- Different TTS models may excel in different aspects (speed, quality, voice variety).\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Not considering text formatting and punctuation for natural speech flow.\n",
    "- Using inappropriate sampling rates for audio output.\n",
    "- Overlooking model-specific parameter tuning for optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Audio Pipeline Analysis and Integration [5 minutes]\n",
    "Analyze and compare the performance of both models.\n",
    "1. Compare transcription accuracy across different audio types.\n",
    "2. Evaluate TTS quality with various text inputs.\n",
    "3. Discuss potential applications and integration scenarios.\n",
    "4. Reflect on the complete text ‚Üî audio pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Systematic analysis of both Whisper and TTS performance completed.\n",
    "- Identification of strengths and limitations for each model.\n",
    "- Discussion of practical applications and use cases.\n",
    "\n",
    "üí° **Key Points**\n",
    "\n",
    "- Whisper and TTS models complement each other in audio-text workflows.\n",
    "- Quality depends heavily on input preprocessing and parameter tuning.\n",
    "- Both models enable powerful accessibility and automation applications.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Not considering the cumulative effect of errors in round-trip conversions.\n",
    "- Overlooking computational requirements for real-time applications.\n",
    "- Failing to validate outputs in practical use scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ **Next Steps**\n",
    "\n",
    "Having completed this activity, you can explore integrating Whisper and TTS in a unified application, such as an automated transcription and translation service for accessible media. In upcoming modules, you'll apply these skills to build more complex, multimodal AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>\n",
    "\n",
    "### Task 1 Solution\n",
    "    \n",
    "```python\n",
    "# Initialize Whisper model and processor\n",
    "whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "print(\"Whisper model loaded successfully!\")\n",
    "print(f\"Model has {sum(p.numel() for p in whisper_model.parameters())} parameters\")\n",
    "\n",
    "# Function to create synthetic audio for demonstration\n",
    "def create_demo_audio(text, sample_rate=16000, duration=3.0):\n",
    "    \"\"\"Create a simple synthetic audio signal for demonstration\"\"\"\n",
    "    t = torch.linspace(0, duration, int(sample_rate * duration))\n",
    "    # Create a simple sine wave with some variation\n",
    "    frequency = 440 + 100 * torch.sin(2 * np.pi * 0.5 * t)  # Varying frequency\n",
    "    waveform = 0.3 * torch.sin(2 * np.pi * frequency * t)\n",
    "    # Add some noise to make it more realistic\n",
    "    noise = 0.05 * torch.randn_like(waveform)\n",
    "    waveform = waveform + noise\n",
    "    return waveform.unsqueeze(0), sample_rate\n",
    "\n",
    "# Create sample audio data (in practice, you would load real audio files)\n",
    "sample_texts = [\n",
    "    \"Hello, this is a test of the Whisper speech recognition system.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Welcome to the world of artificial intelligence and machine learning.\"\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating sample audio for transcription testing...\")\n",
    "\n",
    "# For demonstration, we'll simulate the transcription process\n",
    "# In practice, you would load actual audio files using:\n",
    "# waveform, sample_rate = torchaudio.load(\"audio_file.wav\")\n",
    "\n",
    "transcription_results = []\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"\\nProcessing Audio Sample {i+1}:\")\n",
    "    print(f\"Expected text: '{text}'\")\n",
    "    \n",
    "    # Create demo audio\n",
    "    waveform, sample_rate = create_demo_audio(text)\n",
    "    \n",
    "    # For real audio files, resample if necessary\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "        sample_rate = 16000\n",
    "    \n",
    "    # Preprocess audio for Whisper\n",
    "    inputs = whisper_processor(waveform.squeeze(), sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate transcription\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = whisper_model.generate(inputs[\"input_features\"])\n",
    "    \n",
    "    # Decode the transcription\n",
    "    transcription = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"Whisper transcription: '{transcription}'\")\n",
    "    \n",
    "    # Store results for analysis\n",
    "    transcription_results.append({\n",
    "        'expected': text,\n",
    "        'transcribed': transcription,\n",
    "        'audio_length': waveform.shape[-1] / sample_rate\n",
    "    })\n",
    "\n",
    "# Analyze transcription performance\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WHISPER TRANSCRIPTION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, result in enumerate(transcription_results):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Expected: {result['expected']}\")\n",
    "    print(f\"  Transcribed: {result['transcribed']}\")\n",
    "    print(f\"  Audio Length: {result['audio_length']:.2f} seconds\")\n",
    "    \n",
    "    # Simple word-level accuracy calculation\n",
    "    expected_words = result['expected'].lower().split()\n",
    "    transcribed_words = result['transcribed'].lower().split()\n",
    "    \n",
    "    # Calculate basic similarity\n",
    "    common_words = set(expected_words) & set(transcribed_words)\n",
    "    accuracy = len(common_words) / max(len(expected_words), 1) if expected_words else 0\n",
    "    print(f\"  Word Overlap Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Visualize audio processing\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot waveforms and spectrograms\n",
    "demo_waveform, demo_sr = create_demo_audio(\"Demo audio\", duration=2.0)\n",
    "\n",
    "# Time domain\n",
    "time = torch.linspace(0, demo_waveform.shape[-1] / demo_sr, demo_waveform.shape[-1])\n",
    "axes[0].plot(time, demo_waveform.squeeze())\n",
    "axes[0].set_title('Demo Audio Waveform')\n",
    "axes[0].set_xlabel('Time (s)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Frequency domain\n",
    "fft = torch.fft.fft(demo_waveform.squeeze())\n",
    "frequencies = torch.fft.fftfreq(demo_waveform.shape[-1], 1/demo_sr)\n",
    "axes[1].plot(frequencies[:len(frequencies)//2], torch.abs(fft[:len(fft)//2]))\n",
    "axes[1].set_title('Frequency Spectrum')\n",
    "axes[1].set_xlabel('Frequency (Hz)')\n",
    "axes[1].set_ylabel('Magnitude')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Whisper processing visualization\n",
    "mel_features = whisper_processor(demo_waveform.squeeze(), sampling_rate=demo_sr, return_tensors=\"pt\")[\"input_features\"]\n",
    "axes[2].imshow(mel_features.squeeze().numpy(), aspect='auto', origin='lower')\n",
    "axes[2].set_title('Mel Spectrogram (Whisper Input)')\n",
    "axes[2].set_xlabel('Time')\n",
    "axes[2].set_ylabel('Mel Filters')\n",
    "\n",
    "# Performance summary\n",
    "axes[3].axis('off')\n",
    "performance_text = \"Whisper Performance Summary:\\n\\n\"\n",
    "performance_text += f\"‚Ä¢ Processed {len(transcription_results)} audio samples\\n\"\n",
    "performance_text += f\"‚Ä¢ Model: whisper-base\\n\"\n",
    "performance_text += f\"‚Ä¢ Input sample rate: {sample_rate} Hz\\n\"\n",
    "performance_text += f\"‚Ä¢ Model parameters: {sum(p.numel() for p in whisper_model.parameters()):,}\\n\\n\"\n",
    "performance_text += \"Key Strengths:\\n\"\n",
    "performance_text += \"‚Ä¢ Multilingual support\\n\"\n",
    "performance_text += \"‚Ä¢ Robust to noise\\n\"\n",
    "performance_text += \"‚Ä¢ Zero-shot performance\\n\\n\"\n",
    "performance_text += \"Considerations:\\n\"\n",
    "performance_text += \"‚Ä¢ Audio quality affects accuracy\\n\"\n",
    "performance_text += \"‚Ä¢ Computational requirements\\n\"\n",
    "performance_text += \"‚Ä¢ Real-time vs batch processing\"\n",
    "\n",
    "axes[3].text(0.05, 0.95, performance_text, transform=axes[3].transAxes, \n",
    "             fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Task 2 Solution\n",
    "    \n",
    "```python\n",
    "# For TTS, we'll use a different approach since the RTF had incorrect model references\n",
    "# Let's use a proper TTS solution\n",
    "\n",
    "try:\n",
    "    # Try using espnet2 TTS (if available)\n",
    "    import espnet2\n",
    "    print(\"Using ESPnet2 for TTS\")\n",
    "    TTS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"ESPnet2 not available, using gTTS as alternative\")\n",
    "    TTS_AVAILABLE = False\n",
    "\n",
    "# Alternative using gTTS (Google Text-to-Speech)\n",
    "try:\n",
    "    from gtts import gTTS\n",
    "    import pygame\n",
    "    import tempfile\n",
    "    import os\n",
    "    GTTS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"gTTS not available, will simulate TTS process\")\n",
    "    GTTS_AVAILABLE = False\n",
    "\n",
    "# TTS Implementation\n",
    "def generate_speech_gtts(text, lang='en', slow=False):\n",
    "    \"\"\"Generate speech using Google Text-to-Speech\"\"\"\n",
    "    if not GTTS_AVAILABLE:\n",
    "        print(f\"Would generate speech for: '{text}'\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        tts = gTTS(text=text, lang=lang, slow=slow)\n",
    "        \n",
    "        # Save to temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmp_file:\n",
    "            tts.save(tmp_file.name)\n",
    "            return tmp_file.name\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating speech: {e}\")\n",
    "        return None\n",
    "\n",
    "# Alternative: Simulate TTS with synthetic audio generation\n",
    "def generate_synthetic_speech(text, sample_rate=22050):\n",
    "    \"\"\"Generate synthetic speech-like audio for demonstration\"\"\"\n",
    "    # Create speech-like synthesis (very basic simulation)\n",
    "    duration = len(text.split()) * 0.5  # Roughly 0.5 seconds per word\n",
    "    t = torch.linspace(0, duration, int(sample_rate * duration))\n",
    "    \n",
    "    # Generate speech-like formants (simplified)\n",
    "    speech_signal = torch.zeros_like(t)\n",
    "    \n",
    "    # Add multiple frequency components to simulate speech\n",
    "    for i, char in enumerate(text.lower()):\n",
    "        if char.isalpha():\n",
    "            # Map characters to different frequencies (very simplified)\n",
    "            base_freq = 100 + (ord(char) - ord('a')) * 20\n",
    "            char_time = i / len(text) * duration\n",
    "            \n",
    "            # Create formant-like structure\n",
    "            formant1 = 0.3 * torch.sin(2 * np.pi * base_freq * t)\n",
    "            formant2 = 0.2 * torch.sin(2 * np.pi * (base_freq * 2.5) * t)\n",
    "            formant3 = 0.1 * torch.sin(2 * np.pi * (base_freq * 4) * t)\n",
    "            \n",
    "            # Apply time-based envelope\n",
    "            envelope = torch.exp(-((t - char_time) / 0.1) ** 2)\n",
    "            \n",
    "            speech_signal += (formant1 + formant2 + formant3) * envelope\n",
    "    \n",
    "    # Add some noise for realism\n",
    "    noise = 0.02 * torch.randn_like(speech_signal)\n",
    "    speech_signal = speech_signal + noise\n",
    "    \n",
    "    # Normalize\n",
    "    speech_signal = speech_signal / torch.max(torch.abs(speech_signal))\n",
    "    \n",
    "    return speech_signal, sample_rate\n",
    "\n",
    "# Test different text inputs\n",
    "test_texts = [\n",
    "    \"Welcome to the generative AI world, where possibilities are endless!\",\n",
    "    \"This is a test of text-to-speech synthesis.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is transforming the way we interact with technology.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEXT-TO-SPEECH GENERATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "tts_results = []\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    print(f\"\\nGenerating speech for text {i+1}:\")\n",
    "    print(f\"Text: '{text}'\")\n",
    "    \n",
    "    # Generate synthetic speech\n",
    "    speech_waveform, speech_sr = generate_synthetic_speech(text)\n",
    "    \n",
    "    # Analyze generated speech\n",
    "    duration = len(speech_waveform) / speech_sr\n",
    "    rms_energy = torch.sqrt(torch.mean(speech_waveform ** 2))\n",
    "    \n",
    "    print(f\"  Generated audio duration: {duration:.2f} seconds\")\n",
    "    print(f\"  RMS Energy: {rms_energy:.4f}\")\n",
    "    print(f\"  Sample rate: {speech_sr} Hz\")\n",
    "    \n",
    "    # Try gTTS if available\n",
    "    if GTTS_AVAILABLE:\n",
    "        gtts_file = generate_speech_gtts(text)\n",
    "        if gtts_file:\n",
    "            print(f\"  gTTS audio saved to: {gtts_file}\")\n",
    "    \n",
    "    tts_results.append({\n",
    "        'text': text,\n",
    "        'duration': duration,\n",
    "        'energy': rms_energy.item(),\n",
    "        'waveform': speech_waveform,\n",
    "        'sample_rate': speech_sr\n",
    "    })\n",
    "\n",
    "# Visualize TTS results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot speech waveforms\n",
    "for i, result in enumerate(tts_results[:2]):\n",
    "    time = torch.linspace(0, result['duration'], len(result['waveform']))\n",
    "    axes[0, i].plot(time, result['waveform'])\n",
    "    axes[0, i].set_title(f'Synthetic Speech {i+1}\\n\"{result[\"text\"][:30]}...\"')\n",
    "    axes[0, i].set_xlabel('Time (s)')\n",
    "    axes[0, i].set_ylabel('Amplitude')\n",
    "    axes[0, i].grid(True)\n",
    "\n",
    "# Speech characteristics analysis\n",
    "durations = [r['duration'] for r in tts_results]\n",
    "energies = [r['energy'] for r in tts_results]\n",
    "\n",
    "axes[1, 0].bar(range(len(durations)), durations, alpha=0.8, color='skyblue')\n",
    "axes[1, 0].set_title('Speech Duration by Text')\n",
    "axes[1, 0].set_xlabel('Text Sample')\n",
    "axes[1, 0].set_ylabel('Duration (seconds)')\n",
    "axes[1, 0].set_xticks(range(len(durations)))\n",
    "axes[1, 0].set_xticklabels([f'Text {i+1}' for i in range(len(durations))])\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].bar(range(len(energies)), energies, alpha=0.8, color='lightcoral')\n",
    "axes[1, 1].set_title('Speech Energy by Text')\n",
    "axes[1, 1].set_xlabel('Text Sample')\n",
    "axes[1, 1].set_ylabel('RMS Energy')\n",
    "axes[1, 1].set_xticks(range(len(energies)))\n",
    "axes[1, 1].set_xticklabels([f'Text {i+1}' for i in range(len(energies))])\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä TTS Performance Analysis:\")\n",
    "avg_duration = np.mean(durations)\n",
    "avg_energy = np.mean(energies)\n",
    "print(f\"  Average speech duration: {avg_duration:.2f} seconds\")\n",
    "print(f\"  Average RMS energy: {avg_energy:.4f}\")\n",
    "print(f\"  Words per second: {np.mean([len(r['text'].split()) / r['duration'] for r in tts_results]):.2f}\")\n",
    "```\n",
    "\n",
    "### Task 3 Solution\n",
    "\n",
    "```python\n",
    "# Comprehensive audio pipeline analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLETE AUDIO PIPELINE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Round-trip analysis: Text -> TTS -> Whisper -> Text\n",
    "def analyze_round_trip(original_text, tts_audio, whisper_transcription):\n",
    "    \"\"\"Analyze the complete text->audio->text pipeline\"\"\"\n",
    "    original_words = set(original_text.lower().split())\n",
    "    transcribed_words = set(whisper_transcription.lower().split())\n",
    "    \n",
    "    # Calculate word preservation\n",
    "    preserved_words = original_words & transcribed_words\n",
    "    preservation_rate = len(preserved_words) / len(original_words) if original_words else 0\n",
    "    \n",
    "    return {\n",
    "        'original': original_text,\n",
    "        'transcribed': whisper_transcription,\n",
    "        'preservation_rate': preservation_rate,\n",
    "        'lost_words': original_words - transcribed_words,\n",
    "        'added_words': transcribed_words - original_words\n",
    "    }\n",
    "\n",
    "# Simulate round-trip for demonstration\n",
    "round_trip_results = []\n",
    "for i, text in enumerate(test_texts[:3]):  # Test first 3 samples\n",
    "    # Simulate the round trip (in practice, you'd use actual audio)\n",
    "    synthetic_transcription = text  # In reality, this would come from Whisper\n",
    "    \n",
    "    result = analyze_round_trip(text, None, synthetic_transcription)\n",
    "    round_trip_results.append(result)\n",
    "    \n",
    "    print(f\"\\nRound-trip Analysis {i+1}:\")\n",
    "    print(f\"  Original: '{result['original']}'\")\n",
    "    print(f\"  Transcribed: '{result['transcribed']}'\")\n",
    "    print(f\"  Word preservation: {result['preservation_rate']:.2%}\")\n",
    "\n",
    "# Model comparison and recommendations\n",
    "model_comparison = {\n",
    "    'Whisper (Speech-to-Text)': {\n",
    "        'strengths': [\n",
    "            'Multilingual support (100+ languages)',\n",
    "            'Robust to background noise',\n",
    "            'No fine-tuning required',\n",
    "            'Good accuracy on diverse accents'\n",
    "        ],\n",
    "        'limitations': [\n",
    "            'Computational requirements',\n",
    "            'May struggle with very low quality audio',\n",
    "            'Processing time for real-time applications',\n",
    "            'Accuracy varies with domain-specific terminology'\n",
    "        ],\n",
    "        'use_cases': [\n",
    "            'Meeting transcription',\n",
    "            'Podcast/video subtitles',\n",
    "            'Voice assistants',\n",
    "            'Accessibility applications'\n",
    "        ]\n",
    "    },\n",
    "    'TTS (Text-to-Speech)': {\n",
    "        'strengths': [\n",
    "            'Natural-sounding speech generation',\n",
    "            'Multiple voice options',\n",
    "            'Support for various languages',\n",
    "            'Customizable speech parameters'\n",
    "        ],\n",
    "        'limitations': [\n",
    "            'Limited emotional expression',\n",
    "            'May struggle with pronunciation of rare words',\n",
    "            'Quality varies between different TTS engines',\n",
    "            'Computational cost for high-quality synthesis'\n",
    "        ],\n",
    "        'use_cases': [\n",
    "            'Audiobook narration',\n",
    "            'Voice assistants',\n",
    "            'Accessibility for visually impaired',\n",
    "            'Language learning applications'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Visualization of model capabilities\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Performance metrics comparison\n",
    "metrics = ['Accuracy', 'Speed', 'Language Support', 'Robustness', 'Ease of Use']\n",
    "whisper_scores = [0.85, 0.70, 0.95, 0.80, 0.90]  # Example scores\n",
    "tts_scores = [0.80, 0.85, 0.85, 0.75, 0.85]      # Example scores\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, whisper_scores, width, label='Whisper (STT)', alpha=0.8, color='skyblue')\n",
    "axes[0, 0].bar(x + width/2, tts_scores, width, label='TTS', alpha=0.8, color='lightcoral')\n",
    "axes[0, 0].set_xlabel('Capabilities')\n",
    "axes[0, 0].set_ylabel('Score (0-1)')\n",
    "axes[0, 0].set_title('Whisper vs TTS Performance Comparison')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(metrics, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Application scenarios matrix\n",
    "applications = ['Real-time\\nTranscription', 'Audiobook\\nCreation', 'Voice\\nAssistants', \n",
    "                'Accessibility\\nTools', 'Language\\nLearning']\n",
    "whisper_suitability = [0.8, 0.9, 0.85, 0.95, 0.7]\n",
    "tts_suitability = [0.6, 0.95, 0.9, 0.9, 0.85]\n",
    "\n",
    "axes[0, 1].scatter(whisper_suitability, tts_suitability, s=100, alpha=0.7, c='purple')\n",
    "for i, app in enumerate(applications):\n",
    "    axes[0, 1].annotate(app, (whisper_suitability[i], tts_suitability[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[0, 1].set_xlabel('Whisper Suitability')\n",
    "axes[0, 1].set_ylabel('TTS Suitability')\n",
    "axes[0, 1].set_title('Application Suitability Matrix')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.3)  # Diagonal line\n",
    "\n",
    "# Pipeline workflow visualization\n",
    "axes[1, 0].axis('off')\n",
    "pipeline_text = \"Audio Processing Pipeline:\\n\\n\"\n",
    "pipeline_text += \"1. Audio Input\\n\"\n",
    "pipeline_text += \"   ‚Üì\\n\"\n",
    "pipeline_text += \"2. Whisper STT\\n\"\n",
    "pipeline_text += \"   ‚Üì\\n\"\n",
    "pipeline_text += \"3. Text Processing\\n\"\n",
    "pipeline_text += \"   ‚Üì\\n\"\n",
    "pipeline_text += \"4. TTS Generation\\n\"\n",
    "pipeline_text += \"   ‚Üì\\n\"\n",
    "pipeline_text += \"5. Audio Output\\n\\n\"\n",
    "pipeline_text += \"Key Considerations:\\n\"\n",
    "pipeline_text += \"‚Ä¢ Quality preservation\\n\"\n",
    "pipeline_text += \"‚Ä¢ Latency requirements\\n\"\n",
    "pipeline_text += \"‚Ä¢ Error accumulation\\n\"\n",
    "pipeline_text += \"‚Ä¢ Computational costs\\n\"\n",
    "pipeline_text += \"‚Ä¢ User experience\"\n",
    "\n",
    "axes[1, 0].text(0.1, 0.9, pipeline_text, transform=axes[1, 0].transAxes,\n",
    "               fontsize=11, verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "# Integration recommendations\n",
    "axes[1, 1].axis('off')\n",
    "recommendations = \"Integration Recommendations:\\n\\n\"\n",
    "recommendations += \"üéØ Real-time Applications:\\n\"\n",
    "recommendations += \"  ‚Ä¢ Use smaller Whisper models\\n\"\n",
    "recommendations += \"  ‚Ä¢ Implement streaming TTS\\n\"\n",
    "recommendations += \"  ‚Ä¢ Optimize for latency\\n\\n\"\n",
    "recommendations += \"üì± Mobile Deployment:\\n\"\n",
    "recommendations += \"  ‚Ä¢ Consider on-device models\\n\"\n",
    "recommendations += \"  ‚Ä¢ Implement fallback to cloud\\n\"\n",
    "recommendations += \"  ‚Ä¢ Battery optimization\\n\\n\"\n",
    "recommendations += \"üåê Multilingual Support:\\n\"\n",
    "recommendations += \"  ‚Ä¢ Language detection\\n\"\n",
    "recommendations += \"  ‚Ä¢ Appropriate TTS voices\\n\"\n",
    "recommendations += \"  ‚Ä¢ Cultural considerations\\n\\n\"\n",
    "recommendations += \"‚ôø Accessibility:\\n\"\n",
    "recommendations += \"  ‚Ä¢ High accuracy requirements\\n\"\n",
    "recommendations += \"  ‚Ä¢ Multiple output formats\\n\"\n",
    "recommendations += \"  ‚Ä¢ User customization\"\n",
    "\n",
    "axes[1, 1].text(0.05, 0.95, recommendations, transform=axes[1, 1].transAxes,\n",
    "               fontsize=9, verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final insights and future directions\n",
    "print(\"\\nüîÆ Future Directions and Improvements:\")\n",
    "print(\"\\n1. Model Improvements:\")\n",
    "print(\"   ‚Ä¢ Larger, more capable Whisper models\")\n",
    "print(\"   ‚Ä¢ Neural TTS with better prosody\")\n",
    "print(\"   ‚Ä¢ Emotion and style control\")\n",
    "\n",
    "print(\"\\n2. Integration Opportunities:\")\n",
    "print(\"   ‚Ä¢ Real-time voice translation\")\n",
    "print(\"   ‚Ä¢ Personalized voice synthesis\")\n",
    "print(\"   ‚Ä¢ Multi-modal AI assistants\")\n",
    "\n",
    "print(\"\\n3. Optimization Strategies:\")\n",
    "print(\"   ‚Ä¢ Model quantization and distillation\")\n",
    "print(\"   ‚Ä¢ Edge deployment techniques\")\n",
    "print(\"   ‚Ä¢ Streaming and chunked processing\")\n",
    "\n",
    "print(\"\\n4. Ethical Considerations:\")\n",
    "print(\"   ‚Ä¢ Voice cloning and consent\")\n",
    "print(\"   ‚Ä¢ Bias in speech recognition\")\n",
    "print(\"   ‚Ä¢ Privacy and data protection\")\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
