{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª **Prompt and Compare Across LLMs**\n",
    "\n",
    "**Time Estimate:** 60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Overview\n",
    "\n",
    "This activity is designed to give you hands-on experience with comparing outputs across different Large Language Models (LLMs) by crafting innovative prompts and observing how variations in prompting can impact model responses. Understanding these differences will empower you to effectively use LLMs for various applications, tailoring prompts to achieve desired outputs.\n",
    "\n",
    "In this activity, you'll connect with multiple free LLMs, analyze their outputs, and refine your prompting techniques‚Äîa vital skill for AI practitioners seeking optimal results across diverse tasks in creative industries, technical communication, and content generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Activity Goals\n",
    "\n",
    "By completing this activity, you will:\n",
    "- Develop the ability to analyze LLM outputs for tone, clarity, and factual accuracy\n",
    "- Learn to craft and modify prompts to achieve desired responses from multiple LLMs\n",
    "- Understand the impact of tuning parameters on LLM output for both creative and precise tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåé Scenario\n",
    "\n",
    "Your company, AI Innovations Inc., is evaluating the capabilities of different Large Language Models for a new AI-driven content generation tool. The goal is to discern which model offers the best balance of creativity and accuracy, adapting responses based on user prompting. Your task is to test several free LLMs, comparing their responses to various prompts and adjust prompting techniques to measure output variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Setup Free LLM Access [15 minutes]\n",
    "\n",
    "In this task, you will set up access to multiple free LLMs using Hugging Face's transformers library and free online platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers torch accelerate requests\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup local models - these are free and don't require API keys\n",
    "# We'll use smaller models that can run locally\n",
    "\n",
    "# Model 1: DistilGPT-2 (lightweight GPT-2 variant)\n",
    "model1_name = \"distilgpt2\"\n",
    "generator1 = pipeline('text-generation', model=model1_name, max_length=150)\n",
    "\n",
    "# Model 2: GPT-2 small\n",
    "model2_name = \"gpt2\"\n",
    "generator2 = pipeline('text-generation', model=model2_name, max_length=150)\n",
    "\n",
    "print(\"‚úÖ Local models loaded successfully!\")\n",
    "print(f\"Model 1: {model1_name}\")\n",
    "print(f\"Model 2: {model2_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Setup access to free online APIs (no authentication required)\n",
    "# Hugging Face Inference API (free tier available)\n",
    "\n",
    "def query_hf_api(model_id, prompt, max_tokens=100):\n",
    "    \"\"\"Query Hugging Face's free inference API\"\"\"\n",
    "    API_URL = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "    headers = {\"Authorization\": \"Bearer YOUR_HF_TOKEN\"}  # Optional: add your free HF token\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"temperature\": 0.7,\n",
    "            \"return_full_text\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test function (will work even without token for many models)\n",
    "print(\"üåê Online API function ready (optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ Practice\n",
    "1. Run the setup code and confirm both local models load successfully\n",
    "2. Test each model with a simple prompt to ensure they're working\n",
    "3. (Optional) Create a free Hugging Face account and get a token for expanded access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to test both models with a simple prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "- [ ] Both local models loaded without errors\n",
    "- [ ] Test prompts generate responses from both models\n",
    "- [ ] Understanding of free vs. paid LLM access options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Points\n",
    "- Local models don't require API keys or costs but have limited capabilities\n",
    "- Free online APIs often have rate limits but provide access to larger models\n",
    "- Consider model size vs. performance trade-offs for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Baseline Prompt Evaluation [15 minutes]\n",
    "\n",
    "Create a baseline prompt and evaluate how different models respond to the same input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline prompt\n",
    "baseline_prompt = \"Explain the concept of sustainability in simple terms.\"\n",
    "\n",
    "def compare_models(prompt, temperature=0.7, max_tokens=100):\n",
    "    \"\"\"Compare responses from multiple models\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Model 1 response\n",
    "    try:\n",
    "        response1 = generator1(prompt, max_length=len(prompt.split()) + max_tokens, \n",
    "                              temperature=temperature, do_sample=True, pad_token_id=50256)\n",
    "        results['DistilGPT-2'] = response1[0]['generated_text'][len(prompt):].strip()\n",
    "    except Exception as e:\n",
    "        results['DistilGPT-2'] = f\"Error: {e}\"\n",
    "    \n",
    "    # Model 2 response\n",
    "    try:\n",
    "        response2 = generator2(prompt, max_length=len(prompt.split()) + max_tokens, \n",
    "                              temperature=temperature, do_sample=True, pad_token_id=50256)\n",
    "        results['GPT-2'] = response2[0]['generated_text'][len(prompt):].strip()\n",
    "    except Exception as e:\n",
    "        results['GPT-2'] = f\"Error: {e}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the function\n",
    "print(\"üîç Testing baseline prompt comparison...\")\n",
    "baseline_results = compare_models(baseline_prompt)\n",
    "\n",
    "for model, response in baseline_results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"{response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ Practice\n",
    "1. Run the baseline prompt and document the outputs from both models\n",
    "2. Analyze differences in tone, clarity, and factual accuracy\n",
    "3. Create your own baseline prompt and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to create and test your own baseline prompt\n",
    "my_baseline_prompt = \"\"  # Add your prompt here\n",
    "\n",
    "# Test your prompt and compare outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "- [ ] Baseline prompt outputs recorded and analyzed\n",
    "- [ ] Differences in model responses documented\n",
    "- [ ] Personal baseline prompt tested successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Points\n",
    "- Even similar models can produce notably different responses\n",
    "- Smaller models may be less coherent but still useful for comparison\n",
    "- Baseline evaluation helps establish model characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Prompt Variation and Comparison [15 minutes]\n",
    "\n",
    "Experiment with different prompting techniques to see how they affect model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different prompting techniques\n",
    "prompts = {\n",
    "    \"Basic\": \"Explain sustainability.\",\n",
    "    \"Context\": \"As an environmental expert, explain sustainability to a 10-year-old.\",\n",
    "    \"Few-shot\": \"\"\"Q: What is recycling?\n",
    "A: Recycling is reusing materials to make new products instead of throwing them away.\n",
    "\n",
    "Q: What is sustainability?\n",
    "A:\"\"\",\n",
    "    \"Detailed\": \"Provide a comprehensive explanation of sustainability that covers environmental, economic, and social aspects.\"\n",
    "}\n",
    "\n",
    "# Test all prompt variations\n",
    "all_results = {}\n",
    "for prompt_type, prompt in prompts.items():\n",
    "    print(f\"\\n{'='*20} {prompt_type.upper()} PROMPT {'='*20}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"\\nResponses:\")\n",
    "    \n",
    "    results = compare_models(prompt, temperature=0.7)\n",
    "    all_results[prompt_type] = results\n",
    "    \n",
    "    for model, response in results.items():\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(response)\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ Practice\n",
    "1. Run the prompt variations and observe how each technique affects responses\n",
    "2. Create your own prompt variation using a different technique\n",
    "3. Compare which prompting style works best for different types of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to create and test your own prompt variation\n",
    "my_prompt_variation = \"\"  # Add your creative prompt here\n",
    "\n",
    "# Test and analyze the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "- [ ] All prompt variations tested successfully\n",
    "- [ ] Differences in output style and quality documented\n",
    "- [ ] Best prompting techniques identified for different tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Points\n",
    "- Context and role-playing can significantly improve response quality\n",
    "- Few-shot examples help models understand the desired format\n",
    "- Prompt structure heavily influences LLM output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Creativity vs. Precision Tuning [15 minutes]\n",
    "\n",
    "Experiment with temperature settings to understand the creativity-precision trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different temperature settings\n",
    "creative_prompt = \"Write a short story about a world where plants can talk.\"\n",
    "factual_prompt = \"List the top 5 renewable energy sources and their efficiency ratings.\"\n",
    "\n",
    "temperatures = [0.1, 0.5, 0.9]\n",
    "\n",
    "def test_temperature_effects(prompt, description):\n",
    "    print(f\"\\n{'='*15} {description} {'='*15}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n--- Temperature: {temp} ---\")\n",
    "        results = compare_models(prompt, temperature=temp, max_tokens=80)\n",
    "        \n",
    "        for model, response in results.items():\n",
    "            print(f\"{model}: {response[:100]}...\")\n",
    "\n",
    "# Test creative task\n",
    "test_temperature_effects(creative_prompt, \"CREATIVE TASK\")\n",
    "\n",
    "# Test factual task\n",
    "test_temperature_effects(factual_prompt, \"FACTUAL TASK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ Practice\n",
    "1. Run the temperature experiments and observe output differences\n",
    "2. Create your own creative and factual prompts to test\n",
    "3. Determine optimal temperature settings for different task types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to test your own prompts with different temperatures\n",
    "my_creative_prompt = \"\"  # Add your creative prompt\n",
    "my_factual_prompt = \"\"   # Add your factual prompt\n",
    "\n",
    "# Test both with different temperature settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Checklist\n",
    "- [ ] Temperature effects on creativity and precision observed\n",
    "- [ ] Optimal settings identified for different task types\n",
    "- [ ] Trade-offs between creativity and accuracy understood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Points\n",
    "- Lower temperature (0.1-0.3) = more focused, consistent, factual outputs\n",
    "- Higher temperature (0.7-0.9) = more creative, diverse, but potentially less accurate\n",
    "- Parameter adjustments are crucial for tailoring responses to specific needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Mistakes to Avoid\n",
    "\n",
    "- **Not considering model limitations**: Free/smaller models have constraints compared to larger commercial models\n",
    "- **Overlooking prompt engineering**: Small changes in wording can dramatically affect outputs\n",
    "- **Ignoring temperature effects**: Using wrong temperature settings for your task type\n",
    "- **Insufficient comparison criteria**: Not establishing clear metrics for evaluation\n",
    "- **Changing too many variables**: Modify one parameter at a time for clear comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>\n",
    "\n",
    "```python\n",
    "# Complete LLM comparison analysis\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Setup models\n",
    "models = {\n",
    "    'DistilGPT-2': pipeline('text-generation', model='distilgpt2'),\n",
    "    'GPT-2': pipeline('text-generation', model='gpt2')\n",
    "}\n",
    "\n",
    "def comprehensive_comparison(prompt, temperatures=[0.1, 0.5, 0.9]):\n",
    "    \"\"\"Comprehensive comparison across models and temperatures\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        for temp in temperatures:\n",
    "            try:\n",
    "                response = model(prompt, \n",
    "                               max_length=len(prompt.split()) + 50,\n",
    "                               temperature=temp,\n",
    "                               do_sample=True,\n",
    "                               pad_token_id=50256)\n",
    "                \n",
    "                generated_text = response[0]['generated_text'][len(prompt):].strip()\n",
    "                \n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Temperature': temp,\n",
    "                    'Response': generated_text[:200],  # First 200 chars\n",
    "                    'Length': len(generated_text.split()),\n",
    "                    'Coherence': 'High' if len(generated_text.split()) > 10 else 'Low'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Temperature': temp,\n",
    "                    'Response': f'Error: {e}',\n",
    "                    'Length': 0,\n",
    "                    'Coherence': 'Error'\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test prompts\n",
    "sustainability_prompt = \"Explain sustainable development in simple terms.\"\n",
    "creative_prompt = \"Write a haiku about renewable energy.\"\n",
    "\n",
    "# Analyze sustainability prompt\n",
    "print(\"=== SUSTAINABILITY ANALYSIS ===\")\n",
    "sustainability_df = comprehensive_comparison(sustainability_prompt)\n",
    "print(sustainability_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== CREATIVE ANALYSIS ===\")\n",
    "creative_df = comprehensive_comparison(creative_prompt)\n",
    "print(creative_df.to_string(index=False))\n",
    "\n",
    "# Analysis insights\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(\"1. Temperature Effects:\")\n",
    "print(\"   - Low temp (0.1): More focused, factual responses\")\n",
    "print(\"   - High temp (0.9): More creative, varied responses\")\n",
    "\n",
    "print(\"\\n2. Model Differences:\")\n",
    "print(\"   - DistilGPT-2: Faster but less coherent for complex tasks\")\n",
    "print(\"   - GPT-2: More coherent but requires more computational resources\")\n",
    "\n",
    "print(\"\\n3. Best Practices:\")\n",
    "print(\"   - Use low temperature (0.1-0.3) for factual questions\")\n",
    "print(\"   - Use high temperature (0.7-0.9) for creative tasks\")\n",
    "print(\"   - Always test multiple models for important applications\")\n",
    "print(\"   - Consider prompt engineering for better results\")\n",
    "\n",
    "# Evaluation metrics\n",
    "def evaluate_response_quality(response, task_type):\n",
    "    \"\"\"Simple quality evaluation\"\"\"\n",
    "    metrics = {\n",
    "        'length': len(response.split()),\n",
    "        'coherence': 'High' if len(response.split()) > 10 and '.' in response else 'Low',\n",
    "        'relevance': 'High' if any(keyword in response.lower() for keyword in \n",
    "                                 ['sustain', 'environment', 'future', 'energy']) else 'Medium'\n",
    "    }\n",
    "    \n",
    "    if task_type == 'creative':\n",
    "        metrics['creativity'] = 'High' if any(word in response.lower() for word in \n",
    "                                             ['beautiful', 'flowing', 'whisper', 'gentle']) else 'Medium'\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example evaluation\n",
    "sample_response = \"Sustainable development means meeting our current needs without compromising the ability of future generations to meet their own needs. It focuses on balancing economic growth, environmental protection, and social equity.\"\n",
    "\n",
    "quality_metrics = evaluate_response_quality(sample_response, 'factual')\n",
    "print(f\"\\nSample Response Quality: {quality_metrics}\")\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
