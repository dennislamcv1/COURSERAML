{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñüíª **Deploy a Text Generation Model with Streamlit + MLflow**\n",
    "\n",
    "**Time Estimate:** 45‚Äì60 minutes\n",
    "\n",
    "## üìã **Overview**\n",
    "\n",
    "In this programming activity, you will deploy a pre-trained text generation model using MLflow for model management and Streamlit for building an intuitive web interface. Understanding how to integrate AI into accessible applications is crucial for AI specialists who aim to build user-friendly tools and solutions.\n",
    "\n",
    "This activity demonstrates the practical process of deploying AI models, a skill highly relevant for roles in AI development and machine learning engineering.\n",
    "\n",
    "## üéØ **Learning Outcomes**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Set up and manage models with MLflow\n",
    "- Develop an interactive web interface using Streamlit\n",
    "- Deploy an AI application locally using Jupyter and Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Wrap the Model with MLflow [20 minutes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure and wrap a pre-trained model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç **Practice**\n",
    "\n",
    "Consider how MLflow helps manage model lifecycle and versioning. Think about:\n",
    "\n",
    "- How to create a custom model wrapper that handles different input formats.\n",
    "- The importance of proper predict method implementation for MLflow compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Model is successfully initialized and wrapped\n",
    "- MLflow model wrapper handles input correctly\n",
    "- Model is saved to the specified path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Key Points**\n",
    "\n",
    "- MLflow model wrappers need proper predict method signatures\n",
    "- Handle different input formats (pandas Series/DataFrame, lists)\n",
    "- Text generation parameters should be configurable\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Not handling pandas input formats in the predict method\n",
    "- Forgetting to extract generated text from Hugging Face output format\n",
    "- Not setting appropriate generation parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Create Streamlit Interface [20 minutes]\n",
    "Develop a user-friendly interface for text input and output.\n",
    "1. Create Streamlit components for user input\n",
    "2. Load the MLflow model within the interface\n",
    "3. Handle text generation and display results\n",
    "4. Add interactive elements for better user experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç **Practice**\n",
    "\n",
    "Think about how to create an effective user interface. Consider:\n",
    "\n",
    "- What Streamlit components are best for text input and display.\n",
    "- How to handle model loading and text generation within the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Streamlit interface displays properly in notebook\n",
    "- Text input and generation button work correctly\n",
    "- Generated text displays in the interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Key Points**\n",
    "\n",
    "- Streamlit can run directly in Jupyter notebook cells\n",
    "- ScriptRunContext warnings are normal in notebook environment\n",
    "- Model loading should happen within the button click handler\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Trying to use session state without proper Streamlit context\n",
    "- Not handling the model input format correctly\n",
    "- Ignoring the ScriptRunContext warnings (they're expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Test the Deployment [15 minutes]\n",
    "Launch and test your application using Streamlit.\n",
    "1. Use shell command to run Streamlit properly\n",
    "2. Test the text generation functionality\n",
    "3. Verify the complete workflow works\n",
    "4. Document the deployment URLs and access methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç **Practice**\n",
    "\n",
    "Testing deployment requires understanding the environment. Consider:\n",
    "\n",
    "- How to properly launch Streamlit from the notebook environment.\n",
    "- What URLs and ports will be available for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Streamlit application launches successfully\n",
    "- Local and network URLs are displayed\n",
    "- Text generation works in the web interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Key Points**\n",
    "\n",
    "- Shell commands with `!` can launch Streamlit from notebooks\n",
    "- Multiple URL options (local, network, external) may be available\n",
    "- Deployment testing validates the complete pipeline\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Not using the correct launcher path for the environment\n",
    "- Expecting immediate browser opening (manual navigation required)\n",
    "- Not testing the full text generation workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ **Next Steps**\n",
    "\n",
    "Explore more on optimizing model inference pipelines for improved performance, and learn about deploying scalable AI services using cloud-based solutions like AWS or GCP. Consider containerizing your application with Docker for more robust deployment options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>\n",
    "\n",
    "### Task 1 Solution\n",
    "    \n",
    "```python\n",
    "# Load your pre-trained model\n",
    "model_name = \"distilgpt2\"\n",
    "generator = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "\n",
    "# Create MLflow model wrapper\n",
    "class TextGenWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, generator, max_new_tokens=50):\n",
    "        self.generator = generator\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # Handle pandas Series/DataFrame or list-like input\n",
    "        if isinstance(model_input, (pd.Series, pd.DataFrame)):\n",
    "            prompts = model_input.squeeze().tolist()\n",
    "        else:\n",
    "            prompts = model_input\n",
    "        \n",
    "        # Generate text\n",
    "        outputs = self.generator(\n",
    "            prompts,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Extract generated text from Hugging Face output format\n",
    "        return [out[0][\"generated_text\"] for out in outputs]\n",
    "\n",
    "# Create the model wrapper\n",
    "pyfunc_model = TextGenWrapper(generator)\n",
    "\n",
    "# Save model with MLflow\n",
    "mlflow.pyfunc.save_model(\n",
    "    path=\"mlflow_model\",\n",
    "    python_model=pyfunc_model\n",
    ")\n",
    "\n",
    "print(\"Model successfully saved to 'mlflow_model'\")\n",
    "```\n",
    "\n",
    "### Task 2 Solution\n",
    "    \n",
    "```python\n",
    "import streamlit as st\n",
    "\n",
    "# Set up the Streamlit interface\n",
    "st.title(\"ü§ñ LLM-powered Text Generation\")\n",
    "st.markdown(\"Generate creative text using our pre-trained language model!\")\n",
    "\n",
    "# User input\n",
    "user_input = st.text_area(\n",
    "    \"Enter your prompt:\",\n",
    "    placeholder=\"Once upon a time in a distant galaxy...\",\n",
    "    height=100\n",
    ")\n",
    "\n",
    "# Generation settings\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    max_tokens = st.slider(\"Max new tokens:\", 10, 100, 50)\n",
    "with col2:\n",
    "    temperature = st.slider(\"Temperature:\", 0.1, 1.0, 0.7)\n",
    "\n",
    "# Generate button\n",
    "if st.button(\"üöÄ Generate Text\"):\n",
    "    if user_input.strip():\n",
    "        with st.spinner(\"Generating text...\"):\n",
    "            try:\n",
    "                # Load the MLflow model\n",
    "                model = mlflow.pyfunc.load_model(\"mlflow_model\")\n",
    "                \n",
    "                # Generate response\n",
    "                result = model.predict([user_input])\n",
    "                \n",
    "                # Display result\n",
    "                st.subheader(\"üìù Generated Text:\")\n",
    "                st.write(result[0])\n",
    "                \n",
    "            except Exception as e:\n",
    "                st.error(f\"Error generating text: {e}\")\n",
    "    else:\n",
    "        st.warning(\"‚ö†Ô∏è Please enter a prompt to generate text!\")\n",
    "\n",
    "# Note about warnings\n",
    "st.info(\"‚ÑπÔ∏è ScriptRunContext warnings in the console are normal when running Streamlit in notebooks.\")\n",
    "```\n",
    "\n",
    "### Task 3 Solution\n",
    "\n",
    "```python\n",
    "# Launch Streamlit app using shell command\n",
    "# This will start the Streamlit server and display available URLs\n",
    "\n",
    "print(\"üöÄ Launching Streamlit application...\")\n",
    "print(\"The app will display Local URL, Network URL, and possibly External URL\")\n",
    "print(\"Click on any of these URLs to access your deployed application\")\n",
    "print(\"\\n‚ö†Ô∏è Note: The app will run until you stop the cell execution\")\n",
    "\n",
    "# Run Streamlit (this will use the code from Task 2)\n",
    "!streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
