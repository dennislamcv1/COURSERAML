{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª **Explore Cross-Modal Embeddings with CLIP**\n",
    "\n",
    "**Time Estimate:** 45 minutes\n",
    "\n",
    "## üìã **Overview**\n",
    "\n",
    "In this activity, you will explore the capabilities of CLIP, a vision-language model developed by OpenAI. CLIP utilizes cross-modal embeddings to align text and images in a unified latent space, improving AI's understanding and processing of different modalities. You will compute similarity scores and visualize results to understand how CLIP bridges the gap between language and vision.\n",
    "\n",
    "This understanding is crucial for roles involving AI development or research, as cross-modal embeddings are foundational to many applications in image recognition and natural language processing.\n",
    "\n",
    "## üéØ **Learning Outcomes**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Load and use the CLIP model to process text and images.\n",
    "- Compute and visualize similarity scores between text descriptions and images.\n",
    "- Reflect on the alignment of cross-modal embeddings in CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Load and Inspect the CLIP Model [15 minutes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use Hugging Face's transformers library to initialize the CLIP model and processor.\n",
    "2. Load an example image and select a few descriptive captions to act as your primary data for exploring the model's embedding capabilities.\n",
    "3. Ensure your image is varied enough to challenge the description alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç **Practice**\n",
    "\n",
    "Examine the CLIP model and processor setup. Consider:\n",
    "\n",
    "- How the model architecture handles both text and image inputs.\n",
    "- The variety of your chosen captions and how they relate to the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Model and processor initialized correctly.\n",
    "- Example image loaded successfully.\n",
    "- Descriptive captions selected that provide good context for testing similarity.\n",
    "\n",
    "üí° **Key Points**\n",
    "\n",
    "- CLIP uses a transformer architecture for both text and vision encoders.\n",
    "- The processor handles tokenization and image preprocessing automatically.\n",
    "- Good caption variety is essential for meaningful similarity comparisons.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Not initializing the processor alongside the model.\n",
    "- Choosing non-descriptive captions that don't provide good context for divergence in similarity scores.\n",
    "- Using images that are too simple or ambiguous for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Encode and Process Inputs [15 minutes]\n",
    "Encode the image and each caption using the CLIP processor.\n",
    "1. Transform both data types into a shared vector space using the processor.\n",
    "2. Ensure proper tensor formatting for model inference.\n",
    "3. Verify that inputs are properly encoded and aligned for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç **Practice**\n",
    "\n",
    "Reflect on the encoding process:\n",
    "\n",
    "- How the processor handles different input modalities simultaneously.\n",
    "- The structure of the encoded tensors and their dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Inputs encoded successfully using the CLIP processor.\n",
    "- Proper tensor formatting achieved for model inference.\n",
    "- Text and image data aligned within the same processing batch.\n",
    "\n",
    "üí° **Key Points**\n",
    "\n",
    "- The processor ensures both modalities are transformed into compatible formats.\n",
    "- Encoding all text and images simultaneously ensures alignment within the same processing batch.\n",
    "- Proper padding and tensor formatting are crucial for model performance.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Processing text and images separately, which can lead to alignment issues.\n",
    "- Forgetting to set return_tensors parameter for proper tensor output.\n",
    "- Not handling batch dimensions correctly for multiple captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Compute Similarity Scores and Visualize Results [15 minutes]\n",
    "Use the CLIP model to calculate similarity scores and create visualizations.\n",
    "1. Perform model inference to get similarity scores between image and captions.\n",
    "2. Compute probability distributions using softmax normalization.\n",
    "3. Create visualizations to illustrate the alignment scores.\n",
    "4. Analyze why certain captions score higher and reflect on cross-modal embeddings.\n",
    "5. Discuss the impact of cross-modal embeddings on understanding multimodal interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Similarity scores computed and interpreted accurately.\n",
    "- Results visualized clearly with insights drawn from observations.\n",
    "- Analysis of cross-modal embedding effectiveness completed.\n",
    "\n",
    "üí° **Key Points**\n",
    "\n",
    "- CLIP efficiently maps text and images into a shared latent space.\n",
    "- Cross-modal embeddings reveal semantic relationships between modalities.\n",
    "- Visualization aids in comprehending the model's interpretive capacity.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Not applying proper normalization to interpret similarities correctly.\n",
    "- Creating unclear visualizations that don't highlight the key findings.\n",
    "- Overlooking the importance of semantic alignment in cross-modal understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ **Next Steps**\n",
    "\n",
    "Apply insights from this activity to enhance AI-driven content recommendation systems or develop more intuitive search capabilities by leveraging cross-modal understanding. Transition into exploring Vision-Language Models (VLMs) for more complex applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>\n",
    "\n",
    "### Task 1 Solution\n",
    "    \n",
    "```python\n",
    "# Load the CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()\n",
    "\n",
    "print(\"CLIP model loaded successfully!\")\n",
    "print(f\"Model has {sum(p.numel() for p in clip_model.parameters())} parameters\")\n",
    "\n",
    "# Load an example image (using a sample from the web)\n",
    "# For demonstration, we'll use a beach image\n",
    "url = \"https://images.unsplash.com/photo-1507525428034-b723cf961d3e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1000&q=80\"\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title('Test Image for CLIP Analysis')\n",
    "plt.show()\n",
    "\n",
    "# Define descriptive captions with varying relevance\n",
    "captions = [\n",
    "    \"A sunny day at the beach\",\n",
    "    \"A cat sitting on a sofa\", \n",
    "    \"A bustling downtown street\",\n",
    "    \"Ocean waves on tropical shore\",\n",
    "    \"People enjoying outdoor activities\",\n",
    "    \"A snowy mountain landscape\"\n",
    "]\n",
    "\n",
    "print(f\"\\nTesting with {len(captions)} different captions:\")\n",
    "for i, caption in enumerate(captions):\n",
    "    print(f\"{i+1}. {caption}\")\n",
    "```\n",
    "\n",
    "### Task 2 Solution\n",
    "    \n",
    "```python\n",
    "# Encode inputs using CLIP processor\n",
    "inputs = clip_processor(text=captions, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(\"Input encoding successful!\")\n",
    "print(f\"Text input shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Image input shape: {inputs['pixel_values'].shape}\")\n",
    "print(f\"Attention mask shape: {inputs['attention_mask'].shape}\")\n",
    "\n",
    "# Verify the encoding worked correctly\n",
    "print(f\"\\nNumber of captions processed: {len(captions)}\")\n",
    "print(f\"Batch size in tensors: {inputs['input_ids'].shape[0]}\")\n",
    "print(f\"Image batch size: {inputs['pixel_values'].shape[0]}\")\n",
    "\n",
    "# Show first few tokens of the first caption\n",
    "first_caption_tokens = inputs['input_ids'][0][:10]\n",
    "print(f\"\\nFirst 10 tokens of first caption: {first_caption_tokens}\")\n",
    "```\n",
    "\n",
    "### Task 3 Solution\n",
    "\n",
    "```python\n",
    "# Perform model inference\n",
    "with torch.no_grad():\n",
    "    outputs = clip_model(**inputs)\n",
    "    \n",
    "# Get similarity scores\n",
    "logits_per_image = outputs.logits_per_image  # Image-text similarity scores\n",
    "logits_per_text = outputs.logits_per_text    # Text-image similarity scores\n",
    "\n",
    "# Convert to probabilities using softmax\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "print(\"CLIP inference completed!\")\n",
    "print(f\"Logits shape: {logits_per_image.shape}\")\n",
    "print(f\"Probabilities shape: {probs.shape}\")\n",
    "\n",
    "# Extract similarity scores\n",
    "similarity_scores = logits_per_image[0].cpu().numpy()\n",
    "probability_scores = probs[0].cpu().numpy()\n",
    "\n",
    "print(\"\\nSimilarity Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create results dataframe for better visualization\n",
    "results = []\n",
    "for i, (caption, sim_score, prob_score) in enumerate(zip(captions, similarity_scores, probability_scores)):\n",
    "    results.append({\n",
    "        'caption': caption,\n",
    "        'similarity': sim_score,\n",
    "        'probability': prob_score\n",
    "    })\n",
    "    print(f\"{i+1}. {caption}\")\n",
    "    print(f\"   Similarity: {sim_score:.3f} | Probability: {prob_score:.3f}\")\n",
    "\n",
    "# Sort by similarity score\n",
    "results_sorted = sorted(results, key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "print(\"\\nRanked by Similarity:\")\n",
    "print(\"=\" * 30)\n",
    "for i, result in enumerate(results_sorted):\n",
    "    print(f\"{i+1}. {result['caption']} (Score: {result['similarity']:.3f})\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Bar chart of similarity scores\n",
    "axes[0, 0].bar(range(len(captions)), similarity_scores, color='skyblue', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Caption Index')\n",
    "axes[0, 0].set_ylabel('Similarity Score')\n",
    "axes[0, 0].set_title('CLIP Similarity Scores by Caption')\n",
    "axes[0, 0].set_xticks(range(len(captions)))\n",
    "axes[0, 0].set_xticklabels([f'C{i+1}' for i in range(len(captions))], rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Probability distribution\n",
    "axes[0, 1].bar(range(len(captions)), probability_scores, color='lightcoral', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Caption Index')\n",
    "axes[0, 1].set_ylabel('Probability')\n",
    "axes[0, 1].set_title('CLIP Probability Distribution')\n",
    "axes[0, 1].set_xticks(range(len(captions)))\n",
    "axes[0, 1].set_xticklabels([f'C{i+1}' for i in range(len(captions))], rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Horizontal bar chart with full captions\n",
    "y_pos = np.arange(len(captions))\n",
    "axes[1, 0].barh(y_pos, similarity_scores, color='lightgreen', alpha=0.8)\n",
    "axes[1, 0].set_yticks(y_pos)\n",
    "axes[1, 0].set_yticklabels([f\"{cap[:25]}...\" if len(cap) > 25 else cap for cap in captions])\n",
    "axes[1, 0].set_xlabel('Similarity Score')\n",
    "axes[1, 0].set_title('Similarity Scores by Caption')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Comparison of top vs bottom performers\n",
    "top_3_indices = np.argsort(similarity_scores)[-3:]\n",
    "bottom_3_indices = np.argsort(similarity_scores)[:3]\n",
    "\n",
    "comparison_captions = [captions[i] for i in top_3_indices] + [captions[i] for i in bottom_3_indices]\n",
    "comparison_scores = [similarity_scores[i] for i in top_3_indices] + [similarity_scores[i] for i in bottom_3_indices]\n",
    "colors = ['green'] * 3 + ['red'] * 3\n",
    "\n",
    "axes[1, 1].bar(range(len(comparison_captions)), comparison_scores, color=colors, alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Caption')\n",
    "axes[1, 1].set_ylabel('Similarity Score')\n",
    "axes[1, 1].set_title('Top 3 vs Bottom 3 Captions')\n",
    "axes[1, 1].set_xticks(range(len(comparison_captions)))\n",
    "axes[1, 1].set_xticklabels([f\"{cap[:15]}...\" for cap in comparison_captions], rotation=45, ha='right')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis and insights\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CROSS-MODAL EMBEDDING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_match_idx = np.argmax(similarity_scores)\n",
    "worst_match_idx = np.argmin(similarity_scores)\n",
    "\n",
    "print(f\"\\nüèÜ Best Match: '{captions[best_match_idx]}'\")\n",
    "print(f\"   Score: {similarity_scores[best_match_idx]:.4f}\")\n",
    "print(f\"   Probability: {probability_scores[best_match_idx]:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ùå Worst Match: '{captions[worst_match_idx]}'\")\n",
    "print(f\"   Score: {similarity_scores[worst_match_idx]:.4f}\")\n",
    "print(f\"   Probability: {probability_scores[worst_match_idx]:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Score Range: {similarity_scores.min():.3f} to {similarity_scores.max():.3f}\")\n",
    "print(f\"üìä Score Variance: {np.var(similarity_scores):.4f}\")\n",
    "\n",
    "print(\"\\nüîç Key Insights:\")\n",
    "print(\"‚Ä¢ CLIP successfully aligns visual and textual concepts in shared embedding space\")\n",
    "print(\"‚Ä¢ Beach-related captions score highest, demonstrating semantic understanding\")\n",
    "print(\"‚Ä¢ Unrelated captions (cat, city street) receive low similarity scores\")\n",
    "print(\"‚Ä¢ Cross-modal embeddings enable zero-shot image-text matching\")\n",
    "print(\"‚Ä¢ Probability distribution shows confident predictions for relevant captions\")\n",
    "\n",
    "# Demonstrate embedding space properties\n",
    "print(\"\\nüß† Embedding Space Analysis:\")\n",
    "with torch.no_grad():\n",
    "    # Get text and image embeddings separately\n",
    "    text_embeddings = clip_model.get_text_features(input_ids=inputs['input_ids'], \n",
    "                                                   attention_mask=inputs['attention_mask'])\n",
    "    image_embeddings = clip_model.get_image_features(pixel_values=inputs['pixel_values'])\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "    image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    print(f\"Text embedding shape: {text_embeddings.shape}\")\n",
    "    print(f\"Image embedding shape: {image_embeddings.shape}\")\n",
    "    print(f\"Embedding dimension: {text_embeddings.shape[-1]}\")\n",
    "    \n",
    "    # Compute cosine similarities manually\n",
    "    manual_similarities = torch.matmul(image_embeddings, text_embeddings.T)\n",
    "    print(f\"\\nManual cosine similarities: {manual_similarities[0].cpu().numpy()}\")\n",
    "    print(f\"Model logits (scaled): {(similarity_scores / 100):.4f}\")  # CLIP applies temperature scaling\n",
    "\n",
    "print(\"\\nüí° Applications of Cross-Modal Embeddings:\")\n",
    "print(\"‚Ä¢ Content-based image retrieval using text queries\")\n",
    "print(\"‚Ä¢ Automatic image captioning and description\")\n",
    "print(\"‚Ä¢ Visual question answering systems\")\n",
    "print(\"‚Ä¢ Multimodal content recommendation\")\n",
    "print(\"‚Ä¢ Zero-shot image classification\")\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
