{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# M1L3 Screencasts"
      ],
      "metadata": {
        "id": "0jXZu5-gy-Df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## M1L3SC1: What Can LLMs Do Today? Real Use Cases Across Providers"
      ],
      "metadata": {
        "id": "MZ0pg_kbzCTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: # Step 1: Setting Up Your Environment"
      ],
      "metadata": {
        "id": "O8AAFCfueu3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install openai  #removed pip install command\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = 'your-openai-api-key' #Replace with your actual API key\n",
        "\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "2UGjT9dEetWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Text Summarization"
      ],
      "metadata": {
        "id": "-kFNEA7be8JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(input_text: str,\n",
        "                   model: str = \"gpt-4o-mini\") -> str:\n",
        "    \"\"\"\n",
        "    Return a concise summary of `input_text` using a modern chat model.\n",
        "    `client` must be an instance of openai.OpenAI(), e.g. client = OpenAI().\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a concise and accurate summarizer.\"},\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": f\"Summarize the following text:\\n\\n{input_text}\"}\n",
        "        ],\n",
        "        temperature=0.3,        # low temp → faithful, deterministic summary\n",
        "        max_tokens=150\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "long_text = \"Large language models (LLMs) are a type of artificial intelligence model that can understand, generate, and manipulate human language. They are trained on massive amounts of text data and are able to perform a wide variety of tasks, including text summarization, code generation, language translation, and logical reasoning. LLMs are becoming increasingly popular and are being used in a wide range of applications, such as chatbots, virtual assistants, and content creation tools. Some of the most popular LLMs include GPT-4, Claude, and Gemini.\"\n",
        "summary = summarize_text(long_text)\n",
        "print(\"Summary:\", summary)\n"
      ],
      "metadata": {
        "id": "n22ASwFJfITx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Coding Assistance"
      ],
      "metadata": {
        "id": "2TXqKMJ8fH-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_factorial(n):\n",
        "    return n * calculate_factorial(n-1) if n else 1"
      ],
      "metadata": {
        "id": "kf8sZp05fMKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Language Translation\n",
        "\n",
        "_note: This section would be handled within a real IDE, so there's no direct code execution here._"
      ],
      "metadata": {
        "id": "_LbjYAiCfYGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text(input_text: str,\n",
        "                   target_language: str = \"Spanish\",\n",
        "                   model: str = \"gpt-4o-mini\") -> str:\n",
        "    \"\"\"\n",
        "    Translate `input_text` into `target_language` using an OpenAI chat model.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": f\"You are a professional translator. \"\n",
        "                        f\"Translate any user message into {target_language}.\"},\n",
        "            {\"role\": \"user\", \"content\": input_text}\n",
        "        ],\n",
        "        temperature=0.3,            # low temp → faithful translation\n",
        "        max_tokens=120\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "english_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "translated_text = translate_text(english_text)\n",
        "print(\"Translated Text:\", translated_text)"
      ],
      "metadata": {
        "id": "srbT2vCFfYYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Logical Reasoning"
      ],
      "metadata": {
        "id": "-J9Z2o5GfeZm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2IST3mey9fC",
        "outputId": "3591ff04-75ea-4434-b876-aa5a0f80956b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What Can LLMs Do Today? Real Use Cases Across Providers\n",
            "Summary: Large language models (LLMs) are AI models designed to understand and generate human language, trained on extensive text data. They can perform tasks like text summarization, code generation, language translation, and logical reasoning. LLMs are widely used in applications such as chatbots, virtual assistants, and content creation tools, with notable examples including GPT-4, Claude, and Gemini.\n",
            "Translated Text: El rápido zorro marrón salta sobre el perro perezoso.\n",
            "Logical Answer: To determine how far the train travels, we can use the formula for distance, which is:\n",
            "\n",
            "\\[ \\text{Distance} = \\text{Speed} \\times \\text{Time} \\]\n",
            "\n",
            "In this case, the speed of the train is 60 km/h, and the time it travels is 2 hours. \n",
            "\n",
            "Now, we can plug in the values:\n",
            "\n",
            "\\[ \\text{Distance} = 60 \\, \\text{km/h} \\times 2 \\, \\text{h} \\]\n",
            "\n",
            "Calculating this gives:\n",
            "\n",
            "\\[ \\text{Distance} = 120 \\, \\text{km} \\]\n",
            "\n",
            "Therefore, the train travels 120 kilometers.\n"
          ]
        }
      ],
      "source": [
        "def logical_reasoning_question(question: str,\n",
        "                               model: str = \"gpt-4o-mini\") -> str:\n",
        "    \"\"\"\n",
        "    Ask a free-form logical-reasoning question and return the model’s answer.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a highly logical assistant. \"\n",
        "                        \"Think step-by-step and explain your reasoning.\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ],\n",
        "        temperature=0.2,            # crisp, deterministic reasoning\n",
        "        max_tokens=200\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "question = \"If a train travels at 60 km/h for 2 hours, how far does it travel?\"\n",
        "answer = logical_reasoning_question(question)\n",
        "print(\"Logical Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## M1L3SC2: What Can Vision-Language Models Do? Image + Text in Action"
      ],
      "metadata": {
        "id": "l6O2WKqrzokT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Setting Up for Vision-Language Tasks"
      ],
      "metadata": {
        "id": "KsNW-iVPg4fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers torch pillow #remove pip command\n",
        "\n",
        "# Import necessary libraries\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "\n",
        "# Define function for fetching images from online\n",
        "def fetch_image(url: str, timeout: int = 10):\n",
        "    \"\"\"\n",
        "    Download an image and return a PIL.Image, or None on failure.\n",
        "\n",
        "    * Adds a realistic User-Agent to avoid 403/404 from some CDNs.\n",
        "    * Raises on HTTP errors so you see the real reason immediately.\n",
        "    * Converts to RGB to avoid “cannot write mode RGBA as JPEG” later on.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\":\n",
        "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "            \"(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=timeout)\n",
        "        resp.raise_for_status()               # surfaces 403/404 as an exception\n",
        "        return Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
        "    except Exception as err:\n",
        "        print(f\"[fetch_image] {err}  -  {url}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "sa_tB_c9g-ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Image Captioning with BLIP"
      ],
      "metadata": {
        "id": "sIOt1PQBhBiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Captioning with BLIP\n",
        "model_name_blip = \"Salesforce/blip-image-captioning-base\"\n",
        "processor_blip = BlipProcessor.from_pretrained(model_name_blip)\n",
        "model_blip = BlipForConditionalGeneration.from_pretrained(model_name_blip)\n",
        "\n",
        "image_url_blip = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\" # Using a sample image instead\n",
        "image_blip = fetch_image(image_url_blip)\n",
        "\n",
        "if image_blip:\n",
        "    inputs_blip = processor_blip(image_blip, return_tensors=\"pt\")\n",
        "    outputs_blip = model_blip.generate(**inputs_blip, max_new_tokens=50, do_sample=True)\n",
        "    caption_blip = processor_blip.decode(outputs_blip[0], skip_special_tokens=True)\n",
        "    print(\"Generated Caption with BLIP:\", caption_blip)\n",
        "else:\n",
        "    print(\"Failed to process image for BLIP.\")"
      ],
      "metadata": {
        "id": "_4p3cjhrhEJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abd81b8-e7b8-41e5-93cc-fe0b06654dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Caption with BLIP: a woman and a yellow dog are sitting at the beach on a bright warm day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Zero-Shot Image Classification with CLIP"
      ],
      "metadata": {
        "id": "nfVJTBQghGRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Classification with CLIP\n",
        "model_name_clip = \"openai/clip-vit-base-patch32\"\n",
        "processor_clip = CLIPProcessor.from_pretrained(model_name_clip)\n",
        "model_clip = CLIPModel.from_pretrained(model_name_clip)\n",
        "\n",
        "labels_clip = [\"a cat\", \"a dog\", \"a bird\"]\n",
        "image_url_clip = \"https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\"\n",
        "image_clip = fetch_image(image_url_clip)\n",
        "\n",
        "if image_clip:\n",
        "    inputs_clip = processor_clip(text=labels_clip, images=image_clip, return_tensors=\"pt\", padding=True)\n",
        "    outputs_clip = model_clip(**inputs_clip)\n",
        "    logits_per_image_clip = outputs_clip.logits_per_image\n",
        "    probs_clip = logits_per_image_clip.softmax(dim=1)\n",
        "\n",
        "    pred_label_clip = labels_clip[probs_clip.argmax()]\n",
        "    print(f\"Predicted Label with CLIP: {pred_label_clip}\")\n",
        "else:\n",
        "    print(\"Failed to process image for CLIP.\")"
      ],
      "metadata": {
        "id": "Rksp0d3yhJkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513e3dbe-0c1b-46db-a48a-8156069465cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label with CLIP: a cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Visual Question Answering (VQA)"
      ],
      "metadata": {
        "id": "nz-uzjh2hMla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text-based evaluation with CLIP\n",
        "model_name_clip_vqa = \"openai/clip-vit-base-patch32\"\n",
        "processor_clip_vqa = CLIPProcessor.from_pretrained(model_name_clip_vqa)\n",
        "model_clip_vqa = CLIPModel.from_pretrained(model_name_clip_vqa)\n",
        "\n",
        "question_clip_vqa = \"What is in this image?\" # Changing the question, VQA isn't possible with this model\n",
        "image_url_clip_vqa = \"https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\" # Sample image from above\n",
        "image_clip_vqa = fetch_image(image_url_clip_vqa)\n",
        "\n",
        "if image_clip_vqa:\n",
        "    inputs_clip_vqa = processor_clip_vqa(text=[question_clip_vqa], images=image_clip_vqa, return_tensors=\"pt\", padding=True)\n",
        "    outputs_clip_vqa = model_clip_vqa(**inputs_clip_vqa)\n",
        "    # This CLIP model does not give the answer directly, just a way of evaluating AI fitness\n",
        "    probs_vqa = outputs_clip_vqa.logits_per_image.softmax(dim=1)\n",
        "    answer_clip_vqa = labels_clip[probs_vqa.argmax()]\n",
        "    print(\"Answer with CLIP (simulated for VQA):\", answer_clip_vqa)\n",
        "else:\n",
        "    print(\"Failed to process image for CLIP VQA.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AQkEAh4ze3h",
        "outputId": "520064b8-d0d2-4370-edd4-90bb2a3dd23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer with CLIP (simulated for VQA): a cat\n"
          ]
        }
      ]
    }
  ]
}