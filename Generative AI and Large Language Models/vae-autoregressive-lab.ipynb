{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d36cf3",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª **Sample from a VAE and an Autoregressive Model**\n",
    "\n",
    "**Time Estimate:** 60 minutes\n",
    "\n",
    "## üìã **Overview**\n",
    "\n",
    "This activity will guide you through the implementation and exploration of two vital generative models in AI: Variational Autoencoders (VAEs) and Autoregressive Models. These models are fundamental in generating diverse types of data. You will gain a comprehensive understanding by building and experimenting with these models using PyTorch.\n",
    "\n",
    "- Apply the learned theory to practical coding.\n",
    "- Understand the distinct generative processes for different data types.\n",
    "- Recognize real-world applications of VAEs and autoregressive models.\n",
    "\n",
    "## üéØ **Learning Outcomes**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Construct and train a Variational Autoencoder (VAE) using the MNIST dataset.\n",
    "- Implement a character-level Autoregressive Model for sequence generation.\n",
    "- Compare and critique the outputs of VAEs and Autoregressive Models in terms of diversity and coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1928d",
   "metadata": {},
   "source": [
    "## Imports and Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data loading helper function (provided for you)\n",
    "def load_mnist_from_csv(csv_path='mnist_train.csv'):\n",
    "    \"\"\"Load MNIST data from CSV file - this handles the data loading for you\"\"\"\n",
    "    print(\"Loading MNIST data from CSV...\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"CSV loaded successfully. Shape: {df.shape}\")\n",
    "        \n",
    "        # Handle different possible CSV structures\n",
    "        if df.shape[1] == 785:  # 784 pixels + 1 label (common format)\n",
    "            labels = df.iloc[:, 0].values\n",
    "            pixels = df.iloc[:, 1:].values\n",
    "        elif 'label' in df.columns:\n",
    "            labels = df['label'].values\n",
    "            pixel_cols = [col for col in df.columns if col != 'label']\n",
    "            pixels = df[pixel_cols].values\n",
    "        else:\n",
    "            labels = df.iloc[:, 0].values\n",
    "            pixels = df.iloc[:, 1:].values\n",
    "        \n",
    "        # Ensure we have 784 pixels per image\n",
    "        if pixels.shape[1] != 784:\n",
    "            if pixels.shape[1] > 784:\n",
    "                pixels = pixels[:, :784]\n",
    "            else:\n",
    "                padding = np.zeros((pixels.shape[0], 784 - pixels.shape[1]))\n",
    "                pixels = np.concatenate([pixels, padding], axis=1)\n",
    "        \n",
    "        # Normalize and convert to tensors\n",
    "        pixels = pixels.astype(np.float32)\n",
    "        if pixels.max() > 1.0:\n",
    "            pixels = pixels / 255.0\n",
    "        \n",
    "        pixel_tensor = torch.from_numpy(pixels)\n",
    "        label_tensor = torch.from_numpy(labels.astype(np.int64))\n",
    "        \n",
    "        print(f\"Successfully processed {len(pixel_tensor)} samples\")\n",
    "        return pixel_tensor, label_tensor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"CSV loading failed: {e}\")\n",
    "        print(\"Creating synthetic MNIST-like data as fallback...\")\n",
    "        n_samples = 5000\n",
    "        pixels = torch.rand(n_samples, 784) * 0.5 + 0.25\n",
    "        labels = torch.randint(0, 10, (n_samples,))\n",
    "        return pixels, labels\n",
    "\n",
    "# Load MNIST data from CSV (this replaces the usual torchvision download)\n",
    "train_data, train_labels = load_mnist_from_csv('mnist_train.csv')\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(f\"‚úÖ Dataset ready: {len(train_dataset)} samples loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03772fac",
   "metadata": {},
   "source": [
    "## Task 1: Build and Explore a VAE [30 minutes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43d519",
   "metadata": {},
   "source": [
    "Construct a simple VAE using PyTorch to work with the MNIST dataset. The model will map input images to a latent distribution, then reconstruct images for visualization.\n",
    "\n",
    "**Note:** We've provided you with data loading code that reads MNIST from a CSV file instead of downloading it from the internet. The `train_dataset` and `train_loader` variables are already created for you in the imports section above.\n",
    "\n",
    "1. Train the VAE using the provided training loop.\n",
    "2. Sample new images from the latent space and visualize using matplotlib.\n",
    "3. Compare the generated images with original MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c4b29",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- The VAE can reconstruct MNIST images reliably.\n",
    "- Samples from the latent space show variability and quality similar to real images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855bd18c",
   "metadata": {},
   "source": [
    "üí° **Key Points**\n",
    "\n",
    "- Understand the encoder/decoder process in VAEs.\n",
    "- Recognize how latent space sampling affects image generation.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Incorrect dimension matching in encoder/decoder operations.\n",
    "- Not implementing the reparameterization trick properly.\n",
    "- Using inappropriate loss functions or balancing reconstruction vs KL divergence loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb92a4",
   "metadata": {},
   "source": [
    "## Task 2: Implement an Autoregressive Model [30 minutes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd914278",
   "metadata": {},
   "source": [
    "Create a character-level Autoregressive Model using a toy character dataset to generate sequences.\n",
    "\n",
    "1. Train the SimpleRNN using character sequences.\n",
    "2. Generate new sequences by sampling characters one at a time.\n",
    "3. Analyze sequence coherence and creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152e9f9",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- The model generates coherent sequences from prompts.\n",
    "- Analyze the model's creative sequence generation capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4edb10a",
   "metadata": {},
   "source": [
    "üí° **Key Points**\n",
    "\n",
    "- Autoregressive models predict the next element based on observed sequences.\n",
    "- They excel in generating sequential data like text.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Overfitting on small datasets leading to poor generalization.\n",
    "- Not using proper one-hot encoding for character inputs.\n",
    "- Choosing inappropriate sequence lengths that are too short or too long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b66f86",
   "metadata": {},
   "source": [
    "## Task 3: Compare and Reflect [30 minutes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e119032",
   "metadata": {},
   "source": [
    "Reflect on and compare the generated outputs from the VAE and the Autoregressive Model.\n",
    "\n",
    "1. Analyze the diversity and coherence of the VAEs' and Autoregressive Models' outputs.\n",
    "2. Reflect on the applicability of each model to various real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52f6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "# your analysis here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e1fc5",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Clearly document differences in model outputs.\n",
    "- Reflect on challenges and insights gained on distinct generative processes.\n",
    "\n",
    "üí° **Key Points**\n",
    "\n",
    "- VAEs and autoregressive models serve different purposes in generative modeling.\n",
    "- VAEs excel at learning compact representations and generating diverse samples.\n",
    "- Autoregressive models are powerful for sequential data generation and maintaining coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e1fc6",
   "metadata": {},
   "source": [
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Incorrect dimension matching in encoder/decoder operations for VAEs.\n",
    "- Overfitting on small datasets leading to poor generalization in Autoregressive Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e1fc7",
   "metadata": {},
   "source": [
    "üöÄ **Next Steps**\n",
    "\n",
    "Continue your journey by exploring GANs and diffusion models in subsequent modules, building upon the foundation established with VAEs and Autoregressive Models. These skills are directly applicable to fields such as image synthesis, text generation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e1fc8",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e1fc9",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>\n",
    "\n",
    "### Task 1 Solution - VAE Implementation\n",
    "    \n",
    "```python\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 20)  # output: mean and logvar\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.encoder(x)\n",
    "        return h1[:, :10], h1[:, 10:]\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Note: train_dataset and train_loader are already loaded from CSV in the imports section\n",
    "\n",
    "# VAE Training\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "print(\"Starting VAE training...\")\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"VAE training completed!\")\n",
    "\n",
    "# Generate samples\n",
    "with torch.no_grad():\n",
    "    sample = torch.randn(64, 10)\n",
    "    sample = model.decode(sample).cpu()\n",
    "    \n",
    "# Visualization\n",
    "fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(sample[i].view(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('VAE Generated Samples')\n",
    "plt.show()\n",
    "\n",
    "# Compare with original data\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "# Show original images\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(train_data[i].view(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title(f'Original (Label: {train_labels[i]})')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Show reconstructions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = train_data[:5]\n",
    "    recon, _, _ = model(test_input)\n",
    "    for i in range(5):\n",
    "        axes[1, i].imshow(recon[i].view(28, 28), cmap='gray')\n",
    "        axes[1, i].set_title('VAE Reconstruction')\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Task 2 Solution - Autoregressive Model\n",
    "\n",
    "```python\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        return self.fc(out)\n",
    "\n",
    "import string\n",
    "\n",
    "# Create character dataset\n",
    "chars = string.ascii_lowercase + ' '\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Simple text data\n",
    "text = \"hello world this is a simple text for training\"\n",
    "data = [char_to_idx[c] for c in text if c in char_to_idx]\n",
    "\n",
    "# Model training\n",
    "model = SimpleRNN(len(chars), 128, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "seq_length = 10\n",
    "print(\"Training autoregressive model...\")\n",
    "for epoch in range(100):\n",
    "    for i in range(len(data) - seq_length):\n",
    "        inputs = torch.tensor(data[i:i+seq_length]).unsqueeze(0)\n",
    "        targets = torch.tensor(data[i+1:i+seq_length+1])\n",
    "        \n",
    "        inputs_onehot = torch.zeros(1, seq_length, len(chars))\n",
    "        inputs_onehot.scatter_(2, inputs.unsqueeze(2), 1)\n",
    "        \n",
    "        outputs = model(inputs_onehot)\n",
    "        loss = criterion(outputs.squeeze(0), targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Autoregressive training completed!\")\n",
    "\n",
    "# Generation code\n",
    "def generate_text(model, start_char, length=50):\n",
    "    model.eval()\n",
    "    result = start_char\n",
    "    input_char = char_to_idx[start_char]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        input_tensor = torch.zeros(1, 1, len(chars))\n",
    "        input_tensor[0, 0, input_char] = 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            probabilities = F.softmax(output[0, 0], dim=0)\n",
    "            input_char = torch.multinomial(probabilities, 1).item()\n",
    "            result += idx_to_char[input_char]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Generate some text\n",
    "generated = generate_text(model, 'h', 30)\n",
    "print(f\"Generated text: '{generated}'\")\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
