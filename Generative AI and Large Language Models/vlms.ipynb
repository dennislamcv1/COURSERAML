{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª **Image Captioning and Classification with VLMs**\n",
    "\n",
    "**Time Estimate:** 45 minutes\n",
    "\n",
    "## üìã **Overview**\n",
    "\n",
    "In this hands-on activity, you'll explore Vision-Language Models (VLMs) to enhance your skills in image captioning and zero-shot classification. You'll use BLIP to generate detailed captions for unseen images and employ CLIP for zero-shot image classification with natural language labels. This activity will deepen your understanding of how VLMs integrate visual and textual data to perform complex tasks, crucial for roles in AI-powered content analysis and multimedia applications.\n",
    "\n",
    "## üéØ **Learning Outcomes**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Implement image captioning using the BLIP model.\n",
    "- Perform zero-shot classification with the CLIP model.\n",
    "- Analyze and evaluate model outputs for accuracy and relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Image Captioning with BLIP [20 minutes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load a pre-trained BLIP model using Hugging Face tools.\n",
    "2. Select a diverse set of images from your dataset.\n",
    "3. Generate and analyze captions for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç **Practice**\n",
    "\n",
    "Examine the BLIP-generated captions. Consider:\n",
    "\n",
    "- How well the captions describe the main elements in each image.\n",
    "- The level of detail and accuracy in the generated descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- BLIP model and processor loaded successfully.\n",
    "- Diverse set of images processed for captioning.\n",
    "- Generated captions accurately reflect image contents.\n",
    "\n",
    "üí° **Key Points**\n",
    "\n",
    "- BLIP combines vision and language understanding for automatic captioning.\n",
    "- Pre-trained models can generate human-like descriptions without fine-tuning.\n",
    "- Image diversity is important for evaluating model robustness.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Not considering image diversity can lead to biased outputs.\n",
    "- Ignoring function outputs may overlook key limitations.\n",
    "- Check for alignment between image content and generated captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Zero-Shot Classification with CLIP [20 minutes]\n",
    "Perform zero-shot image classification using CLIP.\n",
    "1. Initialize a pre-trained CLIP model.\n",
    "2. Compile a list of descriptive labels relevant to your images.\n",
    "3. Compute similarity scores and deduce the most likely label.\n",
    "4. Analyze classification accuracy and model confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç **Practice**\n",
    "\n",
    "Reflect on the CLIP classification results:\n",
    "\n",
    "- How confident the model is in its predictions.\n",
    "- The relationship between label specificity and classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- CLIP model initialized and configured properly.\n",
    "- Appropriate labels compiled for classification tasks.\n",
    "- Similarity scores computed and most likely labels identified.\n",
    "\n",
    "üí° **Key Points**\n",
    "\n",
    "- CLIP enables zero-shot classification without additional training data.\n",
    "- Label selection significantly impacts classification performance.\n",
    "- Similarity scores provide insight into model confidence.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Limited label vocabulary may miss nuances in images.\n",
    "- Overlooking metadata could reduce model prediction transparency.\n",
    "- A broad interpretation of labels helps in varied attribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Comparative Analysis and Evaluation [5 minutes]\n",
    "Compare and evaluate the outputs from both models.\n",
    "1. Analyze the strengths and limitations of BLIP captions vs CLIP classifications.\n",
    "2. Evaluate model performance on different types of images.\n",
    "3. Discuss practical applications and use cases for each approach.\n",
    "4. Reflect on how VLMs can be integrated into real-world systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Captions accurately reflect image contents.\n",
    "- Appropriate labels are assigned using CLIP.\n",
    "- Models are understood in terms of both strengths and limitations.\n",
    "\n",
    "üí° **Key Points**\n",
    "\n",
    "- VLMs bridge visual and textual data for advanced AI applications.\n",
    "- Pre-trained models offer powerful baseline capabilities.\n",
    "- Careful prompt selection enhances prediction accuracy.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Not systematically comparing model outputs across different image types.\n",
    "- Overlooking the complementary nature of captioning and classification.\n",
    "- Failing to consider practical deployment constraints and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ **Next Steps**\n",
    "\n",
    "Explore additional datasets or create custom labels to test the flexibility and robustness of VLMs further. Reflect on extending applications using VLMs in domains such as e-commerce, accessibility technology, and digital content curation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>\n",
    "\n",
    "### Task 1 Solution\n",
    "    \n",
    "```python\n",
    "# Load the pre-trained BLIP model and processor\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "print(\"BLIP model loaded successfully!\")\n",
    "print(f\"Model has {sum(p.numel() for p in blip_model.parameters())} parameters\")\n",
    "\n",
    "# Load diverse set of images for testing\n",
    "image_urls = [\n",
    "    \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?ixlib=rb-4.0.3&auto=format&fit=crop&w=500&q=80\",  # Cat\n",
    "    \"https://images.unsplash.com/photo-1552053831-71594a27632d?ixlib=rb-4.0.3&auto=format&fit=crop&w=500&q=80\",  # Dog\n",
    "    \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?ixlib=rb-4.0.3&auto=format&fit=crop&w=500&q=80\",  # Landscape\n",
    "    \"https://images.unsplash.com/photo-1485827404703-89b55fcc595e?ixlib=rb-4.0.3&auto=format&fit=crop&w=500&q=80\",  # Food\n",
    "]\n",
    "\n",
    "images = []\n",
    "for url in image_urls:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        images.append(image)\n",
    "    except:\n",
    "        print(f\"Failed to load image from {url}\")\n",
    "\n",
    "print(f\"Successfully loaded {len(images)} images for captioning\")\n",
    "\n",
    "# Generate captions for each image\n",
    "captions = []\n",
    "for i, image in enumerate(images):\n",
    "    # Prepare the image for model input\n",
    "    inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate caption\n",
    "    output = blip_model.generate(**inputs, max_length=50, num_beams=5)\n",
    "    caption = blip_processor.decode(output[0], skip_special_tokens=True)\n",
    "    captions.append(caption)\n",
    "    \n",
    "    print(f\"Image {i+1} Caption: {caption}\")\n",
    "\n",
    "# Display images with their captions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (image, caption) in enumerate(zip(images, captions)):\n",
    "    if i < len(axes):\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f\"BLIP: {caption}\", fontsize=10, wrap=True)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze caption quality\n",
    "print(\"\\nCaption Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "for i, caption in enumerate(captions):\n",
    "    print(f\"Image {i+1}: {caption}\")\n",
    "    print(f\"  - Length: {len(caption.split())} words\")\n",
    "    print(f\"  - Contains descriptive elements: {'Yes' if any(word in caption.lower() for word in ['cat', 'dog', 'mountain', 'food', 'plate', 'sitting', 'lying', 'beautiful']) else 'No'}\")\n",
    "```\n",
    "\n",
    "### Task 2 Solution\n",
    "    \n",
    "```python\n",
    "# Load the CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "print(\"CLIP model loaded successfully!\")\n",
    "print(f\"Model has {sum(p.numel() for p in clip_model.parameters())} parameters\")\n",
    "\n",
    "# Define comprehensive label sets for different categories\n",
    "animal_labels = [\"a cat\", \"a dog\", \"a bird\", \"a horse\", \"a cow\", \"a sheep\"]\n",
    "general_labels = [\"an animal\", \"a landscape\", \"food\", \"a vehicle\", \"a person\", \"architecture\"]\n",
    "detailed_labels = [\n",
    "    \"a fluffy cat\", \"a playful dog\", \"a mountain landscape\", \"a delicious meal\",\n",
    "    \"a sunny day\", \"indoor scene\", \"outdoor scene\", \"natural environment\"\n",
    "]\n",
    "\n",
    "# Test each image with different label sets\n",
    "results = []\n",
    "\n",
    "for img_idx, image in enumerate(images):\n",
    "    print(f\"\\nClassifying Image {img_idx + 1}:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Test with different label sets\n",
    "    for label_set_name, labels in [(\"General\", general_labels), (\"Detailed\", detailed_labels)]:\n",
    "        # Encode text and image\n",
    "        inputs = clip_processor(text=labels, images=image, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Generate predictions\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=1)\n",
    "        \n",
    "        # Get top predictions\n",
    "        top_probs, top_indices = probs[0].topk(3)\n",
    "        \n",
    "        print(f\"\\n{label_set_name} Labels - Top 3 Predictions:\")\n",
    "        for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "            print(f\"  {i+1}. {labels[idx]}: {prob:.3f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'image_idx': img_idx,\n",
    "            'label_set': label_set_name,\n",
    "            'top_label': labels[top_indices[0]],\n",
    "            'confidence': top_probs[0].item(),\n",
    "            'all_probs': probs[0].tolist()\n",
    "        })\n",
    "\n",
    "# Visualize classification results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    if i < len(axes):\n",
    "        # Get results for this image\n",
    "        img_results = [r for r in results if r['image_idx'] == i]\n",
    "        \n",
    "        axes[i].imshow(image)\n",
    "        title = f\"Image {i+1}\\n\"\n",
    "        for result in img_results:\n",
    "            title += f\"{result['label_set']}: {result['top_label']} ({result['confidence']:.2f})\\n\"\n",
    "        axes[i].set_title(title, fontsize=9)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze classification confidence\n",
    "print(\"\\nClassification Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "for result in results:\n",
    "    print(f\"Image {result['image_idx']+1} ({result['label_set']}): {result['top_label']} - Confidence: {result['confidence']:.3f}\")\n",
    "```\n",
    "\n",
    "### Task 3 Solution\n",
    "\n",
    "```python\n",
    "# Comprehensive comparison and analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE VLM ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare BLIP captions with CLIP classifications\n",
    "comparison_data = []\n",
    "for i in range(len(images)):\n",
    "    blip_caption = captions[i]\n",
    "    general_result = next(r for r in results if r['image_idx'] == i and r['label_set'] == 'General')\n",
    "    detailed_result = next(r for r in results if r['image_idx'] == i and r['label_set'] == 'Detailed')\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'image_idx': i + 1,\n",
    "        'blip_caption': blip_caption,\n",
    "        'clip_general': general_result['top_label'],\n",
    "        'clip_detailed': detailed_result['top_label'],\n",
    "        'general_confidence': general_result['confidence'],\n",
    "        'detailed_confidence': detailed_result['confidence']\n",
    "    })\n",
    "\n",
    "print(\"\\nüîç Model Comparison by Image:\")\n",
    "for data in comparison_data:\n",
    "    print(f\"\\nImage {data['image_idx']}:\")\n",
    "    print(f\"  BLIP Caption: '{data['blip_caption']}'\")\n",
    "    print(f\"  CLIP General: '{data['clip_general']}' (conf: {data['general_confidence']:.3f})\")\n",
    "    print(f\"  CLIP Detailed: '{data['clip_detailed']}' (conf: {data['detailed_confidence']:.3f})\")\n",
    "\n",
    "# Performance analysis\n",
    "avg_general_confidence = np.mean([d['general_confidence'] for d in comparison_data])\n",
    "avg_detailed_confidence = np.mean([d['detailed_confidence'] for d in comparison_data])\n",
    "avg_caption_length = np.mean([len(d['blip_caption'].split()) for d in comparison_data])\n",
    "\n",
    "print(f\"\\nüìä Performance Metrics:\")\n",
    "print(f\"  Average CLIP General Confidence: {avg_general_confidence:.3f}\")\n",
    "print(f\"  Average CLIP Detailed Confidence: {avg_detailed_confidence:.3f}\")\n",
    "print(f\"  Average BLIP Caption Length: {avg_caption_length:.1f} words\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Confidence comparison\n",
    "x = np.arange(len(comparison_data))\n",
    "width = 0.35\n",
    "axes[0, 0].bar(x - width/2, [d['general_confidence'] for d in comparison_data], width, label='General Labels', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, [d['detailed_confidence'] for d in comparison_data], width, label='Detailed Labels', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Image Index')\n",
    "axes[0, 0].set_ylabel('CLIP Confidence')\n",
    "axes[0, 0].set_title('CLIP Classification Confidence by Label Type')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels([f'Img {i+1}' for i in range(len(comparison_data))])\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Caption length distribution\n",
    "caption_lengths = [len(d['blip_caption'].split()) for d in comparison_data]\n",
    "axes[0, 1].bar(range(len(caption_lengths)), caption_lengths, color='lightcoral', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Image Index')\n",
    "axes[0, 1].set_ylabel('Caption Length (words)')\n",
    "axes[0, 1].set_title('BLIP Caption Length by Image')\n",
    "axes[0, 1].set_xticks(range(len(caption_lengths)))\n",
    "axes[0, 1].set_xticklabels([f'Img {i+1}' for i in range(len(comparison_data))])\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Model agreement analysis\n",
    "agreement_scores = []\n",
    "for i, data in enumerate(comparison_data):\n",
    "    # Simple keyword matching between BLIP caption and CLIP labels\n",
    "    caption_words = set(data['blip_caption'].lower().split())\n",
    "    general_words = set(data['clip_general'].lower().split())\n",
    "    detailed_words = set(data['clip_detailed'].lower().split())\n",
    "    \n",
    "    general_overlap = len(caption_words.intersection(general_words)) / max(len(caption_words), 1)\n",
    "    detailed_overlap = len(caption_words.intersection(detailed_words)) / max(len(caption_words), 1)\n",
    "    \n",
    "    agreement_scores.append(max(general_overlap, detailed_overlap))\n",
    "\n",
    "axes[0, 2].bar(range(len(agreement_scores)), agreement_scores, color='lightgreen', alpha=0.8)\n",
    "axes[0, 2].set_xlabel('Image Index')\n",
    "axes[0, 2].set_ylabel('Agreement Score')\n",
    "axes[0, 2].set_title('BLIP-CLIP Agreement (Keyword Overlap)')\n",
    "axes[0, 2].set_xticks(range(len(agreement_scores)))\n",
    "axes[0, 2].set_xticklabels([f'Img {i+1}' for i in range(len(comparison_data))])\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Model strengths comparison\n",
    "strengths_data = {\n",
    "    'BLIP Strengths': ['Detailed descriptions', 'Natural language', 'Contextual understanding', 'Creative captions'],\n",
    "    'CLIP Strengths': ['Zero-shot classification', 'Fast inference', 'Flexible labels', 'Reliable confidence']\n",
    "}\n",
    "\n",
    "axes[1, 0].axis('off')\n",
    "axes[1, 0].text(0.5, 0.9, 'Model Strengths Comparison', ha='center', va='top', fontsize=14, fontweight='bold')\n",
    "\n",
    "y_pos = 0.7\n",
    "for model, strengths in strengths_data.items():\n",
    "    axes[1, 0].text(0.05 if 'BLIP' in model else 0.55, y_pos, model, fontsize=12, fontweight='bold')\n",
    "    for i, strength in enumerate(strengths):\n",
    "        axes[1, 0].text(0.05 if 'BLIP' in model else 0.55, y_pos - 0.1*(i+1), f\"‚Ä¢ {strength}\", fontsize=10)\n",
    "    y_pos -= 0.5\n",
    "\n",
    "# Plot 5: Application scenarios\n",
    "axes[1, 1].axis('off')\n",
    "axes[1, 1].text(0.5, 0.9, 'Practical Applications', ha='center', va='top', fontsize=14, fontweight='bold')\n",
    "\n",
    "applications = [\n",
    "    'Content Moderation: CLIP for quick classification',\n",
    "    'Accessibility: BLIP for detailed descriptions',\n",
    "    'E-commerce: Both for product categorization and descriptions',\n",
    "    'Social Media: BLIP for alt-text, CLIP for content filtering',\n",
    "    'Digital Libraries: Combined for comprehensive metadata'\n",
    "]\n",
    "\n",
    "for i, app in enumerate(applications):\n",
    "    axes[1, 1].text(0.05, 0.8 - i*0.15, f\"‚Ä¢ {app}\", fontsize=10, wrap=True)\n",
    "\n",
    "# Plot 6: Performance summary\n",
    "metrics = ['Caption Quality', 'Classification Accuracy', 'Speed', 'Flexibility', 'Detail Level']\n",
    "blip_scores = [0.85, 0.70, 0.60, 0.75, 0.90]  # Example scores\n",
    "clip_scores = [0.60, 0.85, 0.90, 0.85, 0.65]  # Example scores\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 2].bar(x - width/2, blip_scores, width, label='BLIP', alpha=0.8, color='skyblue')\n",
    "axes[1, 2].bar(x + width/2, clip_scores, width, label='CLIP', alpha=0.8, color='lightcoral')\n",
    "axes[1, 2].set_xlabel('Capabilities')\n",
    "axes[1, 2].set_ylabel('Performance Score')\n",
    "axes[1, 2].set_title('BLIP vs CLIP Performance Comparison')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(metrics, rotation=45, ha='right')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final insights and recommendations\n",
    "print(\"\\nüéØ Key Insights and Recommendations:\")\n",
    "print(\"\\n1. Model Complementarity:\")\n",
    "print(\"   ‚Ä¢ BLIP excels at generating rich, descriptive captions\")\n",
    "print(\"   ‚Ä¢ CLIP provides fast, accurate zero-shot classification\")\n",
    "print(\"   ‚Ä¢ Combined use maximizes information extraction from images\")\n",
    "\n",
    "print(\"\\n2. Practical Deployment Considerations:\")\n",
    "print(\"   ‚Ä¢ BLIP: Best for accessibility, content generation, detailed analysis\")\n",
    "print(\"   ‚Ä¢ CLIP: Ideal for content filtering, quick categorization, search\")\n",
    "print(\"   ‚Ä¢ Label design significantly impacts CLIP performance\")\n",
    "\n",
    "print(\"\\n3. Integration Strategies:\")\n",
    "print(\"   ‚Ä¢ Use CLIP for initial categorization, BLIP for detailed descriptions\")\n",
    "print(\"   ‚Ä¢ Combine outputs for comprehensive metadata generation\")\n",
    "print(\"   ‚Ä¢ Consider computational resources when choosing between models\")\n",
    "\n",
    "print(\"\\n4. Future Improvements:\")\n",
    "print(\"   ‚Ä¢ Fine-tune models on domain-specific data\")\n",
    "print(\"   ‚Ä¢ Implement confidence thresholding for quality control\")\n",
    "print(\"   ‚Ä¢ Explore newer VLM architectures for enhanced performance\")\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
