{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d36cf3",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª **Perform Lightweight Fine-Tuning with LoRA**\n",
    "\n",
    "**Time Estimate:** 60 minutes\n",
    "\n",
    "## üìã **Overview**\n",
    "\n",
    "In this lab, you will explore Lightweight Fine-Tuning using LoRA (Low-Rank Adaptation), a technique to efficiently adapt pre-trained language models. This method is particularly useful for cases with limited computational resources as it modifies only specific parameters instead of retraining the entire model. You will fine-tune a model for a sentiment classification task, using the IMDB dataset, which is an essential skill in deploying AI solutions with constrained resources.\n",
    "\n",
    "## üéØ **Learning Outcomes**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Apply LoRA to fine-tune a pre-trained language model for a targeted task.\n",
    "- Evaluate the trade-offs between performance gains and computational efficiency.\n",
    "- Understand the practical considerations when adapting large models with limited resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03772fac",
   "metadata": {},
   "source": [
    "## Task 1: Dataset Preparation [15 minutes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43d519",
   "metadata": {},
   "source": [
    "You will start by setting up the IMDB dataset for sentiment analysis, a common task in NLP.\n",
    "\n",
    "1. Inspect the dataset structure. Think about:\n",
    "   - What are the inputs and outputs?\n",
    "   - How is the data labeled?\n",
    "2. Analyze data distribution and consider if further manipulation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c4b29",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Dataset is loaded successfully.\n",
    "- Training and test datasets are correctly identified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855bd18c",
   "metadata": {},
   "source": [
    "üí° **Key Points**\n",
    "\n",
    "- Understand dataset formats and load them correctly for usage.\n",
    "- Consider label distribution and dataset integrity.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Not exploring the dataset structure before proceeding with modeling.\n",
    "- Failing to verify that the dataset splits are appropriate for your task.\n",
    "- Ignoring data quality issues like missing values or inconsistent formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb92a4",
   "metadata": {},
   "source": [
    "## Task 2: Model Setup and Initialization [20 minutes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd914278",
   "metadata": {},
   "source": [
    "Set up the environment and initialize a pre-trained transformer model to work with LoRA.\n",
    "\n",
    "1. Ensure the required libraries are installed, such as Hugging Face Transformers and PyTorch.\n",
    "2. Load the model and tokenizer without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152e9f9",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- Libraries are installed correctly.\n",
    "- Model and tokenizer are successfully initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4edb10a",
   "metadata": {},
   "source": [
    "üí° **Key Points**\n",
    "\n",
    "- Familiarity with model and library initialization is crucial for starting any adaptation task.\n",
    "- Make sure version compatibility between libraries is maintained.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Using incompatible library versions that cause import errors.\n",
    "- Not verifying model configuration matches your task requirements.\n",
    "- Failing to handle potential CUDA/device compatibility issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b66f86",
   "metadata": {},
   "source": [
    "## Task 3: Implement LoRA and Fine-Tune [40 minutes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e119032",
   "metadata": {},
   "source": [
    "Apply the LoRA methodology to adapt and fine-tune your model on the training data.\n",
    "\n",
    "1. Fine-tune the model using the training data.\n",
    "2. During training, monitor for potential overfitting by analyzing evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52f6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e1fc5",
   "metadata": {},
   "source": [
    "‚úÖ **Success Checklist**\n",
    "\n",
    "- LoRA is configured and applied correctly.\n",
    "- Model trains without errors and completes all epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e1fc6",
   "metadata": {},
   "source": [
    "üí° **Key Points**\n",
    "\n",
    "- LoRA allows for efficient adaptation by adjusting fewer model parameters.\n",
    "- Fine-tuning requires careful setup of hyperparameters to balance learning.\n",
    "\n",
    "‚ùó **Common Mistakes to Avoid**\n",
    "\n",
    "- Setting LoRA rank too high, negating efficiency benefits.\n",
    "- Not properly configuring the adapter for the specific model architecture.\n",
    "- Ignoring memory constraints when setting batch sizes and sequence lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e1fc7",
   "metadata": {},
   "source": [
    "üöÄ **Next Steps**\n",
    "\n",
    "In the next module, you will explore other fine-tuning strategies and evaluate their effectiveness for different machine learning challenges, enhancing your ability to make strategic model adaptations for specific scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e1fc8",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e1fc9",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>\n",
    "\n",
    "### Task 1 Solution - Dataset Preparation\n",
    "    \n",
    "```python\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Explore dataset structure\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Features: {train_dataset.features}\")\n",
    "\n",
    "# Examine sample entries\n",
    "print(\"\\nSample entry:\")\n",
    "print(f\"Text: {train_dataset[0]['text'][:200]}...\")\n",
    "print(f\"Label: {train_dataset[0]['label']} (0=negative, 1=positive)\")\n",
    "\n",
    "# Check label distribution\n",
    "train_labels = [example['label'] for example in train_dataset]\n",
    "print(f\"\\nLabel distribution in training set:\")\n",
    "print(f\"Negative (0): {train_labels.count(0)}\")\n",
    "print(f\"Positive (1): {train_labels.count(1)}\")\n",
    "\n",
    "# Limit dataset size for faster training (optional)\n",
    "train_dataset = train_dataset.select(range(5000))\n",
    "test_dataset = test_dataset.select(range(1000))\n",
    "print(f\"\\nUsing {len(train_dataset)} training and {len(test_dataset)} test samples\")\n",
    "```\n",
    "\n",
    "### Task 2 Solution - Model Setup and Initialization\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,  # Binary classification for sentiment\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Test tokenizer with sample text\n",
    "sample_text = \"This movie was amazing!\"\n",
    "tokens = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(f\"\\nSample tokenization:\")\n",
    "print(f\"Input IDs shape: {tokens['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {tokens['attention_mask'].shape}\")\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "print(f\"\\nModel loaded successfully: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "```\n",
    "\n",
    "### Task 3 Solution - LoRA Implementation and Fine-Tuning\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification\n",
    "    r=8,  # Rank of adaptation\n",
    "    lora_alpha=16,  # LoRA scaling parameter\n",
    "    lora_dropout=0.1,  # LoRA dropout\n",
    "    target_modules=[\"query\", \"value\"]  # Target attention modules\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Evaluation metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"\\nEvaluation Results: {eval_results}\")\n",
    "\n",
    "# Test prediction on sample text\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1)\n",
    "    \n",
    "    sentiment = \"positive\" if predicted_class.item() == 1 else \"negative\"\n",
    "    confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "# Test predictions\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic!\",\n",
    "    \"I hated this film, it was terrible.\",\n",
    "    \"The movie was okay, nothing special.\"\n",
    "]\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "for text in test_texts:\n",
    "    sentiment, confidence = predict_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted: {sentiment} (confidence: {confidence:.3f})\\n\")\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
