{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Course 4 Module 2 Screencasts"
      ],
      "metadata": {
        "id": "4kdZickuxW_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## M2L1SC1: The Problem with RNNs and How Transformers Fix It"
      ],
      "metadata": {
        "id": "j_4kfIh3xZH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Understanding RNNs and Their Limitations"
      ],
      "metadata": {
        "id": "c-NAlOCpxlyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGxgXcxmxUXe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: The Power of Self-Attention in Transformers"
      ],
      "metadata": {
        "id": "kXosoyS4xk-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.q_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.k_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.v_linear = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, value, key, query, mask=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.embed_size ** 0.5)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attention = nn.functional.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attention, value)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Ht_KKIDyxp_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Adding Position Information"
      ],
      "metadata": {
        "id": "rRja--RfxsIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pos_encoding = torch.zeros((max_len, embed_size))\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * -(math.log(10000.0) / embed_size))\n",
        "        pos_encoding[:, 0::2] = torch.sin(pos * div_term)\n",
        "        pos_encoding[:, 1::2] = torch.cos(pos * div_term)\n",
        "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pos_encoding', pos_encoding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_encoding[:x.size(0), :]\n"
      ],
      "metadata": {
        "id": "EIO3C3a4xtyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Putting it All Together"
      ],
      "metadata": {
        "id": "jHHFsYeBxwwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, num_layers, forward_expansion):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads)\n",
        "        self.position = PositionalEncoding(embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=forward_expansion * embed_size)\n",
        "            for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.position(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "53hi8zBTxwe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## M2L1SC2: Self-Attention, Multi-Head Attention, and Feedforward Networks"
      ],
      "metadata": {
        "id": "_wD5yb8xx9u4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Understanding Self-Attention"
      ],
      "metadata": {
        "id": "ajaDmeqOyXnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def attention(query, key, value):\n",
        "    # Calculate the attention scores\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(key.size(-1))\n",
        "    attention = torch.softmax(scores, dim=-1)\n",
        "    return torch.matmul(attention, value)\n"
      ],
      "metadata": {
        "id": "RnkjHU33x8uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Preparing Key, Query, and Value Matrices for Self-Attention"
      ],
      "metadata": {
        "id": "rdXaKBc6yWOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 64  # Example embedding size\n",
        "batch_size = 4   # Example batch size\n",
        "sequence_len = 10  # Example sequence length\n",
        "query = torch.rand((batch_size, sequence_len, embed_size))\n",
        "key = query.clone()\n",
        "value = query.clone()\n",
        "\n",
        "output = attention(query, key, value)\n",
        "\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jrbwBaGyab3",
        "outputId": "451c3819-3cba-4cb5-ab1d-caf4ee3edc44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 10, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Creating Multi-Head Attention"
      ],
      "metadata": {
        "id": "2nDsp3WoyfZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_size = embed_size // num_heads\n",
        "        assert embed_size % num_heads == 0\n",
        "\n",
        "        self.q_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.k_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.v_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, value, key, query):\n",
        "      N, query_len, embed_size = query.shape\n",
        "      head_dim = embed_size // self.num_heads\n",
        "\n",
        "      Q = self.q_linear(query)\n",
        "      K = self.k_linear(key)\n",
        "      V = self.v_linear(value)\n",
        "\n",
        "      Q = Q.view(N, query_len, self.num_heads, head_dim)\n",
        "      K = K.view(N, -1, self.num_heads, head_dim)\n",
        "      V = V.view(N, -1, self.num_heads, head_dim)\n",
        "\n",
        "      attention = attention(Q, K, V)\n",
        "      out = attention.view(N, query_len, -1)\n",
        "      return self.fc_out(out)"
      ],
      "metadata": {
        "id": "p6HDTaW1yeEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Adding Feedforward Networks"
      ],
      "metadata": {
        "id": "_vQrvoS8yjhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, expansion_factor=4):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.layer1 = nn.Linear(embed_size, expansion_factor * embed_size)\n",
        "        self.layer2 = nn.Linear(expansion_factor * embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        return self.layer2(x)"
      ],
      "metadata": {
        "id": "CZvfgYdyyirv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Combining Multi-Head Attention and Feedforward Networks"
      ],
      "metadata": {
        "id": "kQTp77xwyyJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = FeedForward(embed_size, forward_expansion)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query):\n",
        "        attention = self.attention(value, key, query)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "uKr466rEyw51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## M2L1SC3: Tuning LLM Output with Temperature and Top-p"
      ],
      "metadata": {
        "id": "y6FJzry3y3jW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Setting Up the Environment for Temperature Control"
      ],
      "metadata": {
        "id": "PgGoeFtsy3M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "api_key = '' # Add your API key here.\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def generate_with_temperature(prompt: str,\n",
        "                              temp_value: float,\n",
        "                              model: str = \"gpt-4o-mini\",\n",
        "                              max_tokens: int = 100) -> str:\n",
        "    \"\"\"\n",
        "    Generate a completion for `prompt` at the specified `temp_value`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt : str\n",
        "        The user prompt to send to the model.\n",
        "    temp_value : float\n",
        "        Sampling temperature (0.0–2.0). Higher → more randomness.\n",
        "    model : str, optional\n",
        "        Chat model name (default \"gpt-4o-mini\").\n",
        "    max_tokens : int, optional\n",
        "        Max tokens in the completion (default 100).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The model’s response text.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a helpful, creative assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temp_value,\n",
        "        max_tokens=max_tokens,\n",
        "        n=1                    # single completion\n",
        "        # stop=None  → no custom stop sequence\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "prompt = \"Write a short story about the future of transportation.\"\n",
        "for temp in [0.2, 0.5, 0.8]:\n",
        "    print(f\"\\nTemperature: {temp}\")\n",
        "    print(generate_with_temperature(prompt, temp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGykkoQxy2jP",
        "outputId": "3b309fdc-6bfe-4ad3-8e26-6a548824e028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Temperature: 0.2\n",
            "In the year 2145, the skyline of New Metropolis shimmered with the glow of neon lights and the hum of advanced technology. Hovercars zipped through the air, weaving effortlessly between towering skyscrapers, while magnetic trains glided silently along their elevated tracks. The city was a marvel of innovation, but it was the underground network of hyperloop tunnels that truly captured the imagination of its citizens.\n",
            "\n",
            "At the heart of this transportation revolution was a young engineer named Lila. She had grown up dreaming\n",
            "\n",
            "Temperature: 0.5\n",
            "In the year 2045, the city of Neoterica stood as a shining example of innovation and sustainability. Its skyline was punctuated by gleaming vertical gardens and solar-paneled skyscrapers, but what truly set Neoterica apart was its revolutionary transportation system known as the Nexus.\n",
            "\n",
            "The Nexus was a web of interconnected, autonomous vehicles that operated seamlessly, using advanced AI to predict and adapt to the needs of the citizens. Gone were the days of traffic jams and pollution; the streets were now\n",
            "\n",
            "Temperature: 0.8\n",
            "In the year 2145, the skyline of New Eden shimmered under a blanket of electric blue. Skyscrapers twisted and turned in elegant spirals, their surfaces lined with vertical gardens and solar panels that harnessed the sun’s energy. Below, a network of sleek, silent vehicles glided seamlessly through the air, each one a marvel of engineering and design. In this city of tomorrow, transportation had evolved into a symphony of interconnected technologies, reshaping the way people moved, interacted,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Refining Output Creativitiy with Top-p"
      ],
      "metadata": {
        "id": "A4S7Bn7v0VTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_top_p(prompt: str,\n",
        "                        p_value: float,\n",
        "                        model: str = \"gpt-4o-mini\",\n",
        "                        max_tokens: int = 100,\n",
        "                        temperature: float = 0.7) -> str:\n",
        "    \"\"\"\n",
        "    Generate a response to `prompt` using nucleus sampling (`top_p`).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt : str\n",
        "        The user prompt.\n",
        "    p_value : float\n",
        "        top_p nucleus–sampling cutoff (0.0–1.0).\n",
        "    model : str, optional\n",
        "        Chat model name (default \"gpt-4o-mini\").\n",
        "    max_tokens : int, optional\n",
        "        Maximum tokens in the completion (default 100).\n",
        "    temperature : float, optional\n",
        "        Sampling temperature (default 0.7).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The model’s response text.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a helpful, creative assistant.\"},\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        top_p=p_value,\n",
        "        max_tokens=max_tokens,\n",
        "        n=1                          # one completion\n",
        "        # stop=None is implicit when no stop list is given\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "for p in [0.9, 0.7, 0.5]:\n",
        "    print(f\"\\nTop-p: {p}\")\n",
        "    print(generate_with_top_p(prompt, p))"
      ],
      "metadata": {
        "id": "TJNruajd0WCy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5b303a-b7c3-4f72-cb91-fb2b88626303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-p: 0.9\n",
            "In the year 2142, the world had transformed into a vibrant tapestry of interconnected cities, each pulsating with the rhythm of advanced technology. Gone were the days of congested highways and gas-guzzling vehicles; the air was now filled with the hum of silent, sleek transports gliding effortlessly through the sky and across the land.\n",
            "\n",
            "In the heart of Neo-Tokyo, a young woman named Mira stepped out of her floating apartment, a cube of glass and steel suspended high above the bustling\n",
            "\n",
            "Top-p: 0.7\n",
            "In the year 2045, the skyline of New Haven shimmered with innovation. The city, once a bustling hub of traffic jams and honking horns, had transformed into a marvel of transportation technology. Above the streets, a network of aerial pods glided silently through the air, while below, electric trams and self-driving vehicles zipped along designated lanes, all choreographed by an advanced AI traffic management system known as Aether.\n",
            "\n",
            "Lila, a young urban planner, stood on the observation deck\n",
            "\n",
            "Top-p: 0.5\n",
            "In the year 2145, the world had transformed into a vibrant tapestry of interconnected cities, each pulsating with the rhythm of advanced technology. The skies were alive with drones and personal air vehicles, while the ground below thrummed with the energy of magnetic levitation trains and autonomous electric pods. The concept of transportation had evolved beyond mere movement; it was now an experience, a seamless blend of convenience, sustainability, and connectivity.\n",
            "\n",
            "In the heart of New Eden, a sprawling metropolis known for its lush\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qRhxg4Tz0aIR"
      }
    }
  ]
}