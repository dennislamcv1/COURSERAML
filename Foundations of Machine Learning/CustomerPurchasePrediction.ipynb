{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ca1a65",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5aaf752fe0b117f14747251e2ca6b08e",
     "grade": false,
     "grade_id": "cell-dd67a9579558ec31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 👩‍💻 E-commerce Customer Purchase Prediction: End-to-End ML Pipeline\n",
    "\n",
    "## 📋 Overview\n",
    "In this capstone project, you'll step into the role of a data scientist working for an e-commerce company. Your task is to build a complete machine learning pipeline that predicts whether a customer will make a purchase based on their behavior and demographic information. This represents a real business challenge that can help companies optimize their marketing strategies, personalize customer experiences, and increase conversion rates.\n",
    "\n",
    "This project integrates key machine learning skills including data preprocessing, exploratory data analysis, feature engineering, model training, evaluation, and hyperparameter tuning\n",
    "You'll work with realistic e-commerce customer data that includes demographics, browsing behavior, and purchase history\n",
    "Your final model will provide actionable insights that could directly impact business decisions\n",
    "\n",
    "## 🎯 Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Build a complete end-to-end machine learning pipeline from raw data to optimized model\n",
    "- Apply data preprocessing techniques to handle real-world data challenges\n",
    "- Train and evaluate multiple classification models using appropriate metrics\n",
    "- Optimize model performance through hyperparameter tuning\n",
    "- Interpret model results and translate them into business insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff484b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ef38f1f1efb039ab91c672d82f2a011",
     "grade": false,
     "grade_id": "cell-629b2ac0932851ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1: Data Loading and Exploration\n",
    "As the first step in your project, you need to understand the dataset you're working with. You'll load the e-commerce customer data and perform initial exploration to understand the structure, identify patterns, and detect any data quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd50fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "602edc79b1a498464e17f99a20155463",
     "grade": false,
     "grade_id": "cell-a59bd2f7de658adb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Steps:**\n",
    "1. Load the dataset `customer_purchase_data.csv` into a pandas DataFrame named `df`.\n",
    "\n",
    "\n",
    "2. Display:\n",
    "\n",
    "    - The dataset shape and the first few rows.\n",
    "    - Information about data types and missing values using df.info().\n",
    "    - A statistical summary using df.describe().\n",
    "    - Missing values for each column.      \n",
    "    \n",
    "    \n",
    "3. Explore the target variable `PurchaseStatus`:\n",
    "\n",
    "    - Display value counts (both absolute and normalized).\n",
    "    - Plot the distribution using a countplot.    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset 'customer_purchase_data.csv' into a dataframe named 'df'\n",
    "# your code here ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b03adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "# your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542c98a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac93e9322aec7a56d21f92c676b6e765",
     "grade": false,
     "grade_id": "cell-449061fa69aeb637",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Grading of Lab Assignments:\n",
    "The grading of this assignment is based on the test cases throughout this notebook within the `# BEGIN TESTS` and `#END TESTS` comments. \n",
    "\n",
    "Each task has a number of test cells. For example, the three cells below are confirming the data has been loaded into a dataframe named `df` as expected. \n",
    "\n",
    "Run all of these test cells throughout the project to confirm you pass the tests and are on the right track. Once you have passed all the tests in the entire notebook, or are happy with your results you can click the `Submit Assignment` button in the top right corner for your final submission and grading. \n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f935808",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8b0a9e772c9756142dc1e383272912e",
     "grade": true,
     "grade_id": "cell-23bbfea6fe3ceeae",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN TESTS\n",
    "assert isinstance(df, pd.DataFrame), \"there should be a pandas DataFrame named df\"\n",
    "assert df.shape[0] > 0 and df.shape[1] > 0, \"df should not be empty\"\n",
    "print(\"✅ TEST PASSED!\")\n",
    "# END TESTS\n",
    "\n",
    "# Note: There may be hidden tests that learners cannot see, as to not give away the full solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d2e08",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf6a7ad70a65d93b579278b9fe962f13",
     "grade": true,
     "grade_id": "cell-440e3c00565587e9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN TEST\n",
    "assert df.shape[0] == 1500, \"df should contain 1500 rows\"\n",
    "assert df.shape[1] == 9, \"df should contain 9 columns\"\n",
    "print(\"✅ TEST PASSED!\")\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ca99a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "192103a87563a54015a4c3ac703bd0ab",
     "grade": true,
     "grade_id": "cell-780a615b2c724e32",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN TEST\n",
    "assert 'PurchaseStatus' in df.columns, \"df should contain the target column PurchaseStatus\"\n",
    "assert df['PurchaseStatus'].nunique() == 2, \"The PurchaseStatus column should only have 2 unique values (0 or 1)\"\n",
    "print(\"✅ TEST PASSED!\")\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d79d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b380dc92570832e7bea81dffd41fa90",
     "grade": false,
     "grade_id": "cell-85588ab743eeb140",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ✅ Success Checklist\n",
    "- You've loaded the dataset successfully\n",
    "- You've identified any data quality issues like missing values or outliers\n",
    "- You've analyzed the distribution of features and the target variable\n",
    "- You've created visualizations that provide insights into the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad0bb1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "80f002138649f17d6e680792bd9bad1a",
     "grade": false,
     "grade_id": "cell-ef6c999eb56ce291",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 💡 Key Points\n",
    "- Exploratory data analysis is a critical first step in any machine learning project\n",
    "- Understanding the data distribution helps inform preprocessing decisions\n",
    "- Checking for class imbalance in the target variable affects model selection and evaluation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac51d30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0448a3e4c1c09af81d29c2fbd10f9373",
     "grade": false,
     "grade_id": "cell-fb867aa39c854f14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: Data Preprocessing and Feature Engineering\n",
    "Now that you understand the dataset, it's time to prepare it for machine learning. You'll handle any data quality issues, encode categorical variables, scale numerical features, and create any additional features that might improve your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80ee28",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2955df03c5336ddeac5274d0d44c2c4d",
     "grade": false,
     "grade_id": "cell-4d4361ae09bcc4fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Steps:**\n",
    "1. Handle Missing Values:\n",
    " - Check for missing values in all columns.\n",
    " - For `numerical_features` (Age, AnnualIncome, NumberOfPurchases, TimeSpentOnWebsite, DiscountsAvailed):\n",
    "    - Fill missing values using the median of each column.\n",
    " - For `categorical_features` (Gender, ProductCategory, LoyaltyProgram):\n",
    "    - Fill missing values using the mode (most frequent value) of each column.\n",
    "    \n",
    "    \n",
    "2. Create a New Feature:\n",
    " - Add a feature called `AvgSpendingPerPurchase` calculated as:\n",
    "    - AvgSpendingPerPurchase = AnnualIncome / NumberOfPurchases\n",
    " - Replace NumberOfPurchases == 0 with 1 in the denominator to avoid division by zero.\n",
    " \n",
    " \n",
    "3. Update the Feature Lists:\n",
    " - After creating the new feature, append `AvgSpendingPerPurchase` to the list of numerical features.\n",
    " \n",
    "\n",
    "4. Build a Preprocessing Pipeline named `preprocessor`:\n",
    " - Use `ColumnTransformer` to preprocess your data:\n",
    "    - Scale numerical features using StandardScaler.\n",
    "    - One-hot encode categorical features using OneHotEncoder(drop='first').\n",
    "    \n",
    "\n",
    "5. Split the Dataset:\n",
    " - Separate the target variable `PurchaseStatus` from the feature matrix. Split the data into features `X` and target `y`\n",
    " - Use `train_test_split` with `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)`\n",
    "        \n",
    "\n",
    "6. Fit and Transform:\n",
    " - Fit the preprocessor on the training data only.\n",
    " - Transform both training and test sets using the fitted preprocessor.\n",
    " - Ensure the processed outputs contain no missing values and are in numerical form.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40570fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Check if missing values exist and handle them\n",
    "\n",
    "# Define feature types\n",
    "categorical_features = ['Gender', 'ProductCategory', 'LoyaltyProgram']\n",
    "numerical_features = ['Age', 'AnnualIncome', 'NumberOfPurchases',\n",
    "                      'TimeSpentOnWebsite', 'DiscountsAvailed']\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e02744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a new feature AvgSpendingPerPurchase\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Append AvgSpendingPerPurchase to the list of numerical features\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create preprocessor with pipelines for categorical and numerical features\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split the Dataset as outlined in the instructional steps above\n",
    "# Split the data into features and target\n",
    "# your code here\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82aa214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Fit and Transform\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2455f931",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14ebbc3b79bfa05bdd541d5b3caa6487",
     "grade": true,
     "grade_id": "cell-d919a90a6753e0c6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN TEST\n",
    "assert len(categorical_features) == 3, \"there should be 3 categorical features\"\n",
    "assert len(numerical_features) == 6, \"there should be 6 numerical features\"\n",
    "print(\"✅ TEST PASSED!\")\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b96e4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fdacdbf9678f00376000897349cf0d9",
     "grade": true,
     "grade_id": "cell-d43c555d2dfb64d4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN TEST: Check that there are no missing values in df\n",
    "assert df.isnull().sum().sum() == 0\n",
    "print(\"✅ TEST PASSED!\")\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180215d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69b90a6b4c4b9dc939437f761d8e58e1",
     "grade": true,
     "grade_id": "cell-33f42d3efa5481ab",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN TEST: Check that AvgSpendingPerPurchase was created and added\n",
    "assert 'AvgSpendingPerPurchase' in df.columns\n",
    "assert 'AvgSpendingPerPurchase' in numerical_features\n",
    "print(\"✅ TEST PASSED!\")\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680e860",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92906e391f4f35f25216b093a2955a06",
     "grade": true,
     "grade_id": "cell-64bf5d9a547a7be8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN TEST: Check train-test split shapes\n",
    "assert X_train.shape[0] > 0 and X_test.shape[0] > 0\n",
    "assert y_train.shape[0] > 0 and y_test.shape[0] > 0\n",
    "print(\"✅ TEST PASSED!\")\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa545a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "394f4eec1ec586a07c661ff959061724",
     "grade": false,
     "grade_id": "cell-5008359fdb298bca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ✅ Success Checklist\n",
    "- You've successfully handled any missing values in the dataset\n",
    "- Categorical features are properly encoded\n",
    "- Numerical features are appropriately scaled\n",
    "- You've created meaningful engineered features\n",
    "- Data is properly split into training and testing sets\n",
    "\n",
    "## 💡 Key Points\n",
    "- Feature engineering can significantly improve model performance\n",
    "- Proper data preprocessing reduces the impact of outliers and improves model stability\n",
    "- Standardizing numerical features ensures all features contribute equally to the model\n",
    "- Stratifying the train-test split ensures balanced representation of target classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0599128",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbfe552cd09e65631df571e6b4954007",
     "grade": false,
     "grade_id": "cell-116fabd510157a0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3: Model Training and Baseline Evaluation\n",
    "In this task, you'll train multiple classification models to predict customer purchase behavior. You'll establish baseline performance metrics to later compare with your optimized models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ef709",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bed5bbed3f7116802c6a62d6c55fb762",
     "grade": false,
     "grade_id": "cell-b84b5b643b9ee13e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Steps:**\n",
    "1. Define and train the following models using the processed training data:\n",
    "    - Logistic Regression\n",
    "    - Decision Tree Classifier\n",
    "    - Random Forest Classifier\n",
    "    \n",
    "\n",
    "2. Use each model to predict on the test set.\n",
    "\n",
    "\n",
    "3. Calculate the following metrics for each model:\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1 Score\n",
    "    - ROC-AUC Score\n",
    "    \n",
    "\n",
    "4. Store each model’s results in a dictionary named `results` with metric names as keys. So the dictionary may look something like:\n",
    "```\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'Model': model,\n",
    "        'Predictions': y_pred,\n",
    "        'Probabilities': y_pred_proba\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31001b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modeling libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Create a dictionary to store models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")    \n",
    "      \n",
    "    # Step 1: Train the model\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Step 2: Make predictions\n",
    "    # your code here\n",
    "    \n",
    "        \n",
    "    # Step 3: Calculate metrics\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Step 4: Store results\n",
    "    # complete the code\n",
    "    results[name] = {\n",
    "        \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b7de1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05c3720f23d85f20b2753bf67cb26cfb",
     "grade": true,
     "grade_id": "cell-b223f82a1a02676d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN TEST 1: Ensure all models are evaluated\n",
    "assert len(results) == 3\n",
    "assert all(name in results for name in ['Logistic Regression', 'Decision Tree', 'Random Forest'])\n",
    "print(\"✅ TEST PASSED!\")\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704bd5d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "171f90ab40530d07afbbc273fdbfdf04",
     "grade": true,
     "grade_id": "cell-7ed04e5f78f0b7d2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN TEST 2: Check structure of each result\n",
    "required_keys = {'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC'}\n",
    "for res in results.values():\n",
    "    assert required_keys.issubset(res.keys()), \"The results dictionary is required to contain the keys 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC'\"\n",
    "print(\"✅ TEST PASSED!\")\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40234b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b73f7f36ebeff889fe2e8ac609aa2880",
     "grade": true,
     "grade_id": "cell-86b1ac5707e8e09d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN TEST 3: Ensure all required metric values are within range\n",
    "required_keys = {'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC'}\n",
    "for res in results.values():\n",
    "    for key in required_keys:\n",
    "        assert 0.6 <= res[key] <= 1, f\"{key} value {res[key]} is out of range\"\n",
    "print(\"✅ TEST PASSED!\")\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6587f8e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06888b08ea3a60b6fafe5d5f56b8f278",
     "grade": false,
     "grade_id": "cell-0b30a648381bd733",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ✅ Success Checklist\n",
    "- You've successfully trained multiple classification models\n",
    "- You've evaluated each model using appropriate metrics\n",
    "- You've visualized model performance using confusion matrices and ROC curves\n",
    "- You've identified the strengths and weaknesses of each model\n",
    "\n",
    "## 💡 Key Points\n",
    "- Different metrics reveal different aspects of model performance\n",
    "- The confusion matrix helps understand the types of errors a model makes\n",
    "- ROC curves help visualize the trade-off between true positive rate and false positive rate\n",
    "- The best model depends on the specific business objectives and costs associated with different types of errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc7493",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70740b8a96e8917cfb2d2057fb5f52f2",
     "grade": false,
     "grade_id": "cell-643a30c8b2094100",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 4: Feature Importance and Model Interpretation (Optional)\n",
    "Understanding which features drive your model's predictions is crucial for business insights. In this task, you'll analyze feature importance and interpret what your model has learned about customer purchase behavior. Run the code below to see a plot of Random Forest Feature Importances. Explore further as an optional task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance, partial_dependence\n",
    "\n",
    "# In this case we will focus on the RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Use the feature names from your preprocessing pipeline\n",
    "feature_names = numerical_features + list(cat_feature_names)\n",
    "\n",
    "importances = rf_model.feature_importances_\n",
    "importances_df = pd.Series(importances, index=feature_names).sort_values(ascending=True)\n",
    "\n",
    "# Plot\n",
    "importances_df.plot(kind='barh', figsize=(10, 8))\n",
    "plt.title('Random Forest Feature Importances')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a7a53",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "172a16ce642a3e5bd1661fd053eb8062",
     "grade": false,
     "grade_id": "cell-fed65a37c7ac8765",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 🔍 Reflect\n",
    "1. Analyze the feature importance plot:\n",
    "    - Which features have the strongest influence on purchase predictions?\n",
    "    - Do different models identify similar important features?\n",
    "    - Are there any surprises in the feature importance results?\n",
    "\n",
    "\n",
    "2. Consider 3-5 business insights based on the feature importance analysis:\n",
    "    - What customer behaviors or attributes are most predictive of purchases?\n",
    "    - How might the business use these insights for targeted marketing?\n",
    "\n",
    "## ✅ Success Checklist\n",
    "\n",
    "- You've identified the most important features for predicting purchases\n",
    "- You've translated model insights into actionable business recommendations\n",
    "\n",
    "## 💡 Key Points\n",
    "\n",
    "- Feature importance helps understand what drives model predictions\n",
    "- Different models may identify different important features\n",
    "- Permutation importance is model-agnostic and often more reliable than built-in feature importance\n",
    "- Partial dependence plots reveal how features affect predictions, which is valuable for business interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9221a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a36960ba70ab71fb16dfd1047720dc6f",
     "grade": false,
     "grade_id": "cell-4fbb660a5f5366f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 💻Exemplar Solution\n",
    "After completing this activity (or if you get stuck!), take a moment to review the exemplar solution. This sample solution can offer insights into different techniques and approaches.\n",
    "Reflect on what you can learn from the exemplar solution to improve your coding skills.\n",
    "Remember, multiple solutions can exist for some problems; the goal is to learn and grow as a programmer by exploring various approaches.\n",
    "Use the exemplar solution as a learning tool to enhance your understanding and refine your approach to coding challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d64038",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e5cc1a7fc9d3d9ab0f8fb85039c968c",
     "grade": false,
     "grade_id": "cell-fc93877b82eebd75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "\n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>    \n",
    "    \n",
    "```python    \n",
    "###################\n",
    "# TASK 1\n",
    "###################    \n",
    "    \n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('customer_purchase_data.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check data types and missing values\n",
    "print(\"\\nData types and non-null counts:\")\n",
    "print(df.info())\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check target variable distribution\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(df['PurchaseStatus'].value_counts())\n",
    "print(df['PurchaseStatus'].value_counts(normalize=True).round(3))\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='PurchaseStatus', data=df)\n",
    "plt.title('Purchase Status Distribution')\n",
    "plt.xlabel('Purchase Status (0: No, 1: Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "    \n",
    "###################\n",
    "# TASK 2\n",
    "###################  \n",
    "\n",
    "# Import additional libraries\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define feature types\n",
    "categorical_features = ['Gender', 'ProductCategory', 'LoyaltyProgram']\n",
    "numerical_features = ['Age', 'AnnualIncome', 'NumberOfPurchases',\n",
    "                      'TimeSpentOnWebsite', 'DiscountsAvailed']\n",
    "\n",
    "\n",
    "# Check if missing values exist and handle them\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    # For numerical features, fill with median\n",
    "    for col in numerical_features:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # For categorical features, fill with mode\n",
    "    for col in categorical_features:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Feature engineering\n",
    "# Create a new feature: Average spending per purchase\n",
    "if 'AnnualIncome' in df.columns and 'NumberOfPurchases' in df.columns:\n",
    "    # Avoid division by zero\n",
    "    df['AvgSpendingPerPurchase'] = df['AnnualIncome'] / df['NumberOfPurchases'].replace(0, 1)\n",
    "    numerical_features.append('AvgSpendingPerPurchase')\n",
    "\n",
    "# Create preprocessor with pipelines for categorical and numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop('PurchaseStatus', axis=1)\n",
    "y = df['PurchaseStatus']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Fit the preprocessor on the training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Apply preprocessing to create processed training and testing data\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Check the shape of the processed data\n",
    "print(\"Processed training data shape:\", X_train_processed.shape)\n",
    "print(\"Processed testing data shape:\", X_test_processed.shape)\n",
    "\n",
    "# If using OneHotEncoder, get the feature names\n",
    "cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "feature_names = numerical_features + list(cat_feature_names)\n",
    "print(\"\\nFeature names after preprocessing:\")\n",
    "print(feature_names)\n",
    "\n",
    "# Visualize correlation matrix of numerical features\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = df[numerical_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "###################\n",
    "# TASK 3\n",
    "###################  \n",
    "    \n",
    "# Import modeling libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Create a dictionary to store models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_processed, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'Model': model,\n",
    "        'Predictions': y_pred,\n",
    "        'Probabilities': y_pred_proba\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['No Purchase', 'Purchase'],\n",
    "                yticklabels=['No Purchase', 'Purchase'])\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['Probabilities'])\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {result['ROC-AUC']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for All Models')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe to compare model performances\n",
    "comparison_df = pd.DataFrame({\n",
    "    model_name: {\n",
    "        metric: results[model_name][metric]\n",
    "        for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "    }\n",
    "    for model_name in results.keys()\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "comparison_df.plot(kind='bar')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Model')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
