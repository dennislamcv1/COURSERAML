{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4e497f",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Feature Scaling for Loan Approval Prediction\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you'll work with a loan approval dataset to implement feature scaling techniques‚Äîa critical preprocessing step for machine learning models. You'll transform raw data features into consistent scales to improve model performance and interpretability. By the end, you'll have a scaled dataset ready for building a loan approval prediction model and understand how different scaling methods impact your results.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Apply different feature scaling techniques (StandardScaler and MinMaxScaler) to prepare data for machine learning\n",
    "\n",
    "- Visualize and compare feature distributions before and after scaling\n",
    "\n",
    "- Evaluate how feature scaling affects model performance for loan approval prediction\n",
    "\n",
    "- Select appropriate scaling methods based on data characteristics and model requirements\n",
    "\n",
    "## üöÄ Starting Point\n",
    "Access the starter code provided below. You'll need to load the loan approval prediction dataset during the lab.\n",
    "\n",
    "Required tools/setup:\n",
    "\n",
    "- Python 3.x\n",
    "- Pandas, NumPy, Scikit-learn, Matplotlib/Seaborn libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder  # Import both scalers\n",
    "import warnings\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('loan_approval_dataset.csv')\n",
    "df.columns = df.columns.str.strip() # Remove whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e653600",
   "metadata": {},
   "source": [
    "## Task 1: Explore the Dataset\n",
    "**Context:** Data scientists need to understand their data before applying preprocessing techniques. In a real-world loan approval system, understanding feature distributions helps identify which scaling methods would be most appropriate.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Examine the first few rows of the dataset using`head()`method\n",
    "2. Display summary statistics with `describe()` to identify features with varying scales\n",
    "3. Check for missing values using `isnull().sum()`\n",
    "4. Examine the data types and convert categorical variables as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49542730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04414b",
   "metadata": {},
   "source": [
    "üí° **Tip:** Pay special attention to numerical features like income and loan amount. Their scales likely differ significantly from other features.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "\n",
    "- You should see features like income and loan amount having much larger values than others\n",
    "- Confirm whether any missing values need to be addressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e245002",
   "metadata": {},
   "source": [
    "## Task 2: Visualize Feature Distributions\n",
    "**Context:** Visualization helps identify which features require scaling and provides a baseline for comparing scaled results.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create histograms of numerical features using `matplotlib.pyplot.hist()` or `seaborn.histplot()`\n",
    "2. Generate a boxplot to visualize the range and outliers using `seaborn.boxplot()`\n",
    "3. Create a correlation matrix heatmap using `seaborn.heatmap(df.corr())` to understand relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for feature visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81131c6",
   "metadata": {},
   "source": [
    "üí° **Tip:** Compare feature ranges side-by-side using subplots to clearly see scale differences.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "\n",
    "- Your histograms should clearly show different scales across features\n",
    "- The boxplots should reveal potential outliers that might affect scaling choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447bf90",
   "metadata": {},
   "source": [
    "## Task 3: Apply Standard Scaling\n",
    "**Context:** StandardScaler transforms features to have a mean of 0 and standard deviation of 1, which is crucial for many machine learning algorithms, particularly those that assume normally distributed data.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Separate features and target variable\n",
    "2. Split data into training and testing sets using `train_test_split()`\n",
    "3. Initialize a `StandardScaler object`\n",
    "4. Fit the scaler on the training data and transform both training and testing data\n",
    "5. Visualize the scaled features to confirm transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5667fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for standard scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6ab97",
   "metadata": {},
   "source": [
    "üí° **Tip:** Always fit your scaler on training data only to avoid data leakage, then apply the same transformation to test data.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "\n",
    "- Check that scaled features have a mean close to 0 and standard deviation close to 1\n",
    "- Verify that the shape of your data remains unchanged after scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d312c8c",
   "metadata": {},
   "source": [
    "## Task 4: Apply Min-Max Scaling\n",
    "**Context:** MinMaxScaler transforms features to a specific range (typically [0,1]), which is useful for algorithms that expect bounded input values.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Initialize a `MinMaxScaler object`\n",
    "2. Fit the scaler on the training data and transform both training and testing data\n",
    "3. Visualize the min-max scaled features\n",
    "4. Compare the distributions before and after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7707fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for min-max scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de035a2",
   "metadata": {},
   "source": [
    "üí° **Tip:** MinMaxScaler is sensitive to outliers, so consider how they might affect your scaled data.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "\n",
    "- Verify all scaled values fall between 0 and 1\n",
    "- Compare histograms of features before and after scaling to see how distributions change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844d3d6",
   "metadata": {},
   "source": [
    "## Task 5: Build and Evaluate Models with Different Scaling Methods\n",
    "**Context:** Different scaling methods can impact model performance, so it's important to compare results.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Initialize a LogisticRegression model\n",
    "2. Train separate models using unscaled, standard-scaled, and min-max-scaled data\n",
    "3. Make predictions with each model\n",
    "4. Compare performance using metrics like accuracy, precision, recall, and F1-score\n",
    "5. Generate confusion matrices to visualize prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for model building and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08284b3",
   "metadata": {},
   "source": [
    "üí° **Tip:** For fair comparison, use the same random state when initializing models and splitting data.\n",
    "\n",
    "‚öôÔ∏è **Test Your Work:**\n",
    "\n",
    "- Note performance differences between models with different scaling methods\n",
    "- Check if there's a clear winner among the scaling approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ab5ba5",
   "metadata": {},
   "source": [
    "## Task 6: Document and Reflect on Results\n",
    "**Context:** Documenting preprocessing steps and their impact is essential for model reproducibility and future reference.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Summarize the impact of different scaling methods on model performance\n",
    "2. Document which features benefited most from scaling\n",
    "3. Explain why certain scaling methods might be more appropriate for this specific dataset\n",
    "4. Reflect on how feature scaling contributes to the overall loan approval prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01921054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your documentation and reflection as comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb214a3",
   "metadata": {},
   "source": [
    "## ‚úÖ Success Checklist\n",
    "\n",
    "- Dataset successfully loaded and explored\n",
    "- Feature distributions visualized before scaling\n",
    "- StandardScaler correctly applied to training and testing data\n",
    "- MinMaxScaler correctly applied to training and testing data\n",
    "- Model performance compared across different scaling methods\n",
    "- Documentation completed with insights on scaling impact\n",
    "- All code runs without errors\n",
    "\n",
    "## üîç Common Issues & Solutions\n",
    "\n",
    "**Problem:** Forgetting to fit scalers only on training data\n",
    "\n",
    "**Solution:** Always use fit_transform() on training data and only transform() on test data\n",
    "\n",
    "**Problem:** Poor model performance despite scaling \n",
    "\n",
    "**Solution:** Check for outliers that might be affecting your scaling results, or consider robust scaling methods\n",
    "\n",
    "**Problem:** Confusion about which features to scale\n",
    "\n",
    "**Solution:** Only scale numerical features; categorical features should be encoded separately\n",
    "\n",
    "## üîë Key Points\n",
    "\n",
    "- Feature scaling is essential for algorithms sensitive to feature magnitudes\n",
    "- Different scaling methods (StandardScaler vs. MinMaxScaler) serve different purposes\n",
    "- The choice of scaling method should be informed by your data distribution and model requirements\n",
    "- Always scale after splitting data to prevent data leakage\n",
    "\n",
    "## ‚û°Ô∏è Next Steps\n",
    "\n",
    "What you'll build next: In upcoming labs, you'll extend this work by implementing more advanced preprocessing techniques and building ensemble models for loan approval prediction.\n",
    "\n",
    "How this connects to the next lab: The scaled features you've prepared will be used for feature selection and dimensionality reduction techniques in the next lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c77eaec",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution\n",
    "\n",
    "After completing this activity (or if you get stuck!), take a moment to review the exemplar solution. This sample solution can offer insights into different techniques and approaches. Reflect on what you can learn from the exemplar solution to improve your coding skills. Remember, multiple solutions can exist for some problems; the goal is to learn and grow as a programmer by exploring various approaches. Use the exemplar solution as a learning tool to enhance your understanding and refine your approach to coding challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33aeaae",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>    \n",
    "    \n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder  # Import both scalers\n",
    "import warnings\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('loan_approval_dataset.csv')\n",
    "df.columns = df.columns.str.strip()  # Remove whitespace\n",
    "\n",
    "# View first few rows\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Label encode categorical columns\n",
    "categorical_cols = ['education', 'self_employed']\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Task 2: Visualizing Feature Distributions\n",
    "\n",
    "# Select numerical columns for visualization from the original DataFrame\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Histograms for numerical features\n",
    "print(\"Generating histograms for numerical features...\")\n",
    "plt.figure(figsize=(16, 12))\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    plt.subplot((len(numerical_cols) + 2) // 3, 3, idx + 1)\n",
    "    sns.histplot(df[col], kde=True, bins=30)\n",
    "    plt.title(f'Histogram of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplots for numerical features\n",
    "print(\"Generating boxplots for numerical features...\")\n",
    "plt.figure(figsize=(16, 12))\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    plt.subplot((len(numerical_cols) + 2) // 3, 3, idx + 1)\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "print(\"Generating correlation heatmap...\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[numerical_cols].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# --- Prepare features and target variable ---\n",
    "# Features (X) are all columns except loan_id and loan_status\n",
    "# Target (y) is loan_status\n",
    "X = df.drop(['loan_id', 'loan_status'], axis=1)\n",
    "y = df['loan_status']\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "\n",
    "# --- Split the dataset ---\n",
    "print(\"\\n--- Splitting Dataset ---\")\n",
    "# Split data into training and testing sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "\n",
    "# --- Apply Feature Scaling (StandardScaler and MinMaxScaler) ---\n",
    "print(\"\\n--- Applying Feature Scaling ---\")\n",
    "\n",
    "# Identify numerical columns after Label Encoding for scaling\n",
    "# All columns in X are now numerical (original numeric + label encoded)\n",
    "numerical_features = X_train.columns.tolist()\n",
    "print(f\"\\nApplying scaling to features: {numerical_features}\")\n",
    "\n",
    "# Apply StandardScaler\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_scaled_standard = scaler_standard.fit_transform(X_train)\n",
    "X_test_scaled_standard = scaler_standard.transform(X_test)\n",
    "\n",
    "# Convert to DataFrame for easy plotting\n",
    "X_train_scaled_standard_df = pd.DataFrame(X_train_scaled_standard, columns=X_train.columns)\n",
    "\n",
    "# Visualize scaled features\n",
    "print(\"\\nVisualizing Scaled Features...\")\n",
    "for feature in X_train_scaled_standard_df.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(data=X_train_scaled_standard_df, x=feature, kde=True, bins=30)\n",
    "    plt.title(f'Distribution of Scaled Feature: {feature}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Apply MinMaxScaler\n",
    "print(\"Applying MinMaxScaler...\")\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_scaled_minmax = scaler_minmax.fit_transform(X_train)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_scaled_minmax_df = pd.DataFrame(X_train_scaled_minmax, columns=X_train.columns)\n",
    "print(\"Min-Max Scaling complete on training data.\")   \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
