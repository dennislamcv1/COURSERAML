{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19489c9",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Data Imputation Lab: Cleaning a Customer Purchase Dataset\n",
    "\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you'll tackle the crucial task of cleaning a customer purchase dataset by addressing missing data. You'll apply various imputation techniques and make decisions about when to remove or replace missing values, preparing the dataset for use in machine learning models. These skills are essential for real-world data science roles where data rarely arrives in a clean, complete state.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Identify and visualize patterns of missing data in datasets\n",
    "- Apply appropriate imputation techniques based on data characteristics\n",
    "- Evaluate the impact of imputation methods on data integrity\n",
    "- Make informed decisions about handling missing values in preparation for machine learning\n",
    "\n",
    "## üöÄ Starting Point\n",
    "Access the starter code below.\n",
    "\n",
    "Required tools/setup:\n",
    "\n",
    "- Python 3.x\n",
    "- Pandas, NumPy, Matplotlib libraries\n",
    "- scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df7f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # For enhanced visualizations\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# --- Data Loading and Initial Examination ---\n",
    "print(\"--- Loading and Examining Data ---\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('sample_dataset.csv')\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Your code will continue below..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b113bd6",
   "metadata": {},
   "source": [
    "## Task 1: Examine the Dataset\n",
    "**Context:** Data scientists often begin with exploratory data analysis to understand the structure, content, and quality of their data. This helps identify potential problems, including missing values, before proceeding with analysis.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Use Pandas functions like `head()`, `describe()`, and `info()` to explore the dataset structure.\n",
    "\n",
    "\n",
    "2. Identify the columns containing missing values using the `isnull().sum()` method.\n",
    "\n",
    "\n",
    "3. Calculate the percentage of missing values for each column to understand the extent of the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for loading and examining the dataset\n",
    "# Display first few rows\n",
    "# Count missing values per column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796d13d",
   "metadata": {},
   "source": [
    "**üí° Tip:** Look for patterns in the missing data. Are certain columns missing more values than others? Are there rows with multiple missing values?\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "- Your output should show the dataset structure, summary statistics, and a count of missing values by column.\n",
    "- Verify that you can identify which columns have the highest percentage of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29384bbd",
   "metadata": {},
   "source": [
    "## Task 2: Visualize Missing Data\n",
    "**Context:** Visualization helps identify patterns in missing data that might not be obvious from numerical summaries. These patterns can inform your imputation strategy.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a bar chart showing the count of missing values for each column.\n",
    "\n",
    "\n",
    "2. Optional: Consider creating any other visualizations as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for visualizing missing data\n",
    "# Create bar chart of missing values\n",
    "# Optional: Create additional visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff284f8c",
   "metadata": {},
   "source": [
    "**üí° Tip:** Pay attention to whether missing values appear randomly distributed or follow patterns that might suggest issues with data collection.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "- Your visualization clearly shows which columns have missing values and their relative proportions.\n",
    "- You can identify any patterns in the missing data that might influence your imputation strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e14db",
   "metadata": {},
   "source": [
    "## Task 3: Implement Basic Imputation Methods\n",
    "**Context:** Simple imputation techniques like mean or median replacement are often suitable first approaches for handling missing numerical data.\n",
    "\n",
    "**Steps:**\n",
    "1. **Split the data** into training and testing sets using `train_test_split`. Remember to split features (X) and target (y) separately if you have a target variable, or just split the main DataFrame if not predicting a target (though typically you would). Imputation is applied to the features (X).\n",
    "\n",
    "\n",
    "2. Identify which columns are **numerical** and which are **categorical** and have missing values.\n",
    "\n",
    "\n",
    "3. For the **numerical columns** with missing values, use `SimpleImputer` with `strategy='mean'` or `strategy='median'`.\n",
    "\n",
    "\n",
    "4. For **categorical columns** with missing values, use `SimpleImputer` with `strategy='most_frequent'`.\n",
    "\n",
    "\n",
    "5. **Combine these strategies** using a `ColumnTransformer` to apply the correct imputer to the correct subset of columns.\n",
    "\n",
    "\n",
    "6. **Fit the** `ColumnTransformer` **ONLY on the training data.**\n",
    "\n",
    "\n",
    "7. **Transform BOTH the training and testing data** using the fitted transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d36ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for basic imputation techniques\n",
    "# Implement mean imputation\n",
    "# Implement median imputation\n",
    "# Compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ad68a",
   "metadata": {},
   "source": [
    "**üí° Tip:** Consider whether mean or median imputation is more appropriate based on the distribution of each column. Skewed distributions often benefit from median imputation.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "- Verify that no missing values remain after imputation.\n",
    "- Compare summary statistics before and after imputation to assess the impact on data distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f2e7b7",
   "metadata": {},
   "source": [
    "## Task 4: Evaluate the Impact on Dataset\n",
    "**Context:** After imputation, it's crucial to evaluate how the techniques have affected the dataset's statistical properties.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Generate descriptive statistics for each imputed dataset (mean and median).\n",
    "\n",
    "\n",
    "2. Create visualizations (histograms or box plots) to compare distributions before and after imputation.\n",
    "\n",
    "\n",
    "3. Assess whether the imputation techniques have preserved the overall data structure and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d5c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for evaluating imputation impact\n",
    "# Generate descriptive statistics for each imputed dataset\n",
    "# Create comparative visualizations\n",
    "# Note observations about the impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e998bd",
   "metadata": {},
   "source": [
    "**üí° Tip:** Look for significant changes in mean, median, or distribution shape that might indicate the imputation has distorted the data.\n",
    "\n",
    "**‚öôÔ∏è Test Your Work:**\n",
    "- Your analysis should show how each imputation technique has affected the dataset's statistical properties.\n",
    "- You can identify which technique preserved the data's integrity best.\n",
    "\n",
    "## ‚úÖ Success Checklist\n",
    "- Successfully loaded and examined the dataset, identifying columns with missing values\n",
    "- Created clear visualizations showing the distribution of missing data\n",
    "- Implemented and compared mean and median imputation techniques\n",
    "- Evaluated the impact of different imputation techniques on data integrity\n",
    "- Documented your approach and findings throughout the process\n",
    "- Code runs without errors\n",
    "\n",
    "## üîç Common Issues & Solutions\n",
    "**Problem:** Imputation dramatically changes the distribution of a column.\n",
    "\n",
    "**Solution:** This often indicates that the chosen imputation method isn't appropriate. Try an alternative method or consider whether the column should be handled differently.\n",
    "\n",
    "**Problem:** Missing values in categorical columns cannot be handled with mean/median imputation.\n",
    "\n",
    "**Solution:** For categorical data, consider mode imputation or creating a new \"Missing\" category.\n",
    "\n",
    "## üîë Key Points\n",
    "- Different imputation techniques work better for different data distributions.\n",
    "- Visualizing missing data patterns helps inform appropriate imputation strategies.\n",
    "- Evaluating the impact of imputation is crucial to ensure data integrity is maintained.\n",
    "- When dealing with skewed data, median imputation often preserves the distribution better than mean.\n",
    "\n",
    "## ‚û°Ô∏è Next Steps\n",
    "In your next lab, you'll build on these data cleaning skills by exploring feature engineering techniques that prepare your cleaned data for machine learning algorithms. You'll learn how to transform variables, create new features, and select the most relevant attributes for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f5b8a",
   "metadata": {},
   "source": [
    "## üíª Exemplar Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84a854",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Click HERE to see an examplar solution</summary><strong>\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # For enhanced visualizations\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# --- Data Loading and Initial Examination ---\n",
    "print(\"--- Loading and Examining Data ---\")\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    data = pd.read_csv('sample_dataset.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: sample_dataset.csv not found. Please check download/unzip.\")\n",
    "    exit() # Exit if data loading fails\n",
    "\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "data.info()\n",
    "\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values[missing_values > 0]) # Print only columns with missing values\n",
    "\n",
    "\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "missing_percentage = (missing_values / len(data)) * 100\n",
    "print(missing_percentage[missing_percentage > 0]) # Print percentage for columns with missing data\n",
    "\n",
    "\n",
    "# --- Visualize Missing Data ---\n",
    "print(\"\\n--- Visualizing Missing Data ---\")\n",
    "\n",
    "\n",
    "if missing_values[missing_values > 0].empty:\n",
    "    print(\"No missing values to visualize.\")\n",
    "else:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    missing_values[missing_values > 0].plot(kind='bar')\n",
    "    plt.ylabel('Count of Missing Values')\n",
    "    plt.title('Missing Values Across Features')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Implement Basic Imputation Methods ---\n",
    "print(\"\\n--- Implementing Basic Imputation Methods ---\")\n",
    "\n",
    "\n",
    "# Based on examination, only 'Gender' has missing values and is categorical.\n",
    "# 'Transaction Amount' is numeric but has no NaNs - used here for mean/median demo.\n",
    "\n",
    "\n",
    "data_imputed = data.copy() # Create a copy to work on\n",
    "\n",
    "\n",
    "# Impute 'Gender' (Categorical) with 'most_frequent'\n",
    "if 'Gender' in data_imputed.columns and data_imputed['Gender'].isnull().sum() > 0:\n",
    "    print(\"\\nImputing 'Gender' with mode...\")\n",
    "    gender_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    # Apply imputer and flatten the 2D output\n",
    "    data_imputed['Gender'] = gender_imputer.fit_transform(data_imputed[['Gender']])[:, 0]\n",
    "    print(\"'Gender' imputation complete.\")\n",
    "else:\n",
    "     print(\"\\nNo missing values in 'Gender'. Skipping mode imputation.\")\n",
    "\n",
    "\n",
    "# Demonstrate Mean/Median Imputation (for numeric columns if they had NaNs)\n",
    "# Applying to 'Transaction Amount' as a demo, though it has no NaNs.\n",
    "print(\"\\nDemonstrating mean/median imputation on 'Transaction Amount' (no NaNs):\")\n",
    "numeric_col_demo = 'Transaction Amount'\n",
    "if numeric_col_demo in data_imputed.columns:\n",
    "    # Mean Imputation Demo\n",
    "    data_imputed_mean_demo = data.copy()\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    data_imputed_mean_demo[numeric_col_demo] = mean_imputer.fit_transform(data_imputed_mean_demo[[numeric_col_demo]])[:, 0]\n",
    "    print(f\"'{numeric_col_demo}' mean imputation demo complete.\")\n",
    "\n",
    "\n",
    "    # Median Imputation Demo\n",
    "    data_imputed_median_demo = data.copy()\n",
    "    median_imputer = SimpleImputer(strategy='median')\n",
    "    data_imputed_median_demo[numeric_col_demo] = median_imputer.fit_transform(data_imputed_median_demo[[numeric_col_demo]])[:, 0]\n",
    "    print(f\"'{numeric_col_demo}' median imputation demo complete.\")\n",
    "else:\n",
    "    print(f\"Warning: Numeric demo column '{numeric_col_demo}' not found.\")\n",
    "\n",
    "\n",
    "# --- Evaluate the Impact on Dataset ---\n",
    "print(\"\\n--- Evaluating Imputation Impact ---\")\n",
    "\n",
    "\n",
    "print(\"\\nMissing values after imputation on 'Gender':\")\n",
    "print(data_imputed.isnull().sum()['Gender']) # Should be 0\n",
    "\n",
    "\n",
    "print(\"\\n'Gender' distribution before and after mode imputation:\")\n",
    "print(\"Original:\")\n",
    "print(data['Gender'].value_counts(dropna=False))\n",
    "print(\"\\nAfter Mode Imputation:\")\n",
    "print(data_imputed['Gender'].value_counts(dropna=False)) # dropna=False to show the change\n",
    "\n",
    "\n",
    "# Evaluate numeric demo imputation impact (expected to be minimal as no NaNs)\n",
    "if numeric_col_demo in data_imputed.columns:\n",
    "    print(f\"\\nDescriptive stats for '{numeric_col_demo}':\")\n",
    "    print(\"Original:\", data[numeric_col_demo].describe())\n",
    "    print(\"Mean Demo:\", data_imputed_mean_demo[numeric_col_demo].describe())\n",
    "    print(\"Median Demo:\", data_imputed_median_demo[numeric_col_demo].describe())\n",
    "\n",
    "\n",
    "    # Comparative Histograms for the numeric demo column\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(data=data, x=numeric_col_demo, kde=True)\n",
    "    plt.title(f'Original {numeric_col_demo}')\n",
    "\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.histplot(data=data_imputed_mean_demo, x=numeric_col_demo, kde=True)\n",
    "    plt.title(f'{numeric_col_demo} (Mean Imputation Demo)')\n",
    "\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.histplot(data=data_imputed_median_demo, x=numeric_col_demo, kde=True)\n",
    "    plt.title(f'{numeric_col_demo} (Median Imputation Demo)')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Comparative Count Plots for Gender distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(data=data, x='Gender', order=data['Gender'].value_counts(dropna=False).index, palette='viridis', hue='Gender')\n",
    "    plt.title('Original Gender Distribution (with NaNs)')\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.countplot(data=data_imputed, x='Gender', order=data_imputed['Gender'].value_counts().index, palette='viridis', hue='Gender')\n",
    "    plt.title('Gender Distribution After Mode Imputation')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
