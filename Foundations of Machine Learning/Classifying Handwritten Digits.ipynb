{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ae4b1f",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Classifying Handwritten Digits Using SVMs\n",
    "\n",
    "## üìã Overview\n",
    "In this lab, you‚Äôll train **Support Vector Machine (SVM)** classifiers with both **linear** and **RBF kernels** to classify digits from the **MNIST dataset**. You'll evaluate, tune, and compare the models to understand the critical role of kernel selection in SVM performance.\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Load and prepare image-based numerical data\n",
    "\n",
    "- Train and evaluate linear and RBF kernel-SVMs\n",
    "\n",
    "- Visualize and compare classification results\n",
    "\n",
    "- Tune hyperparameters for SVM performance improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cd4d0",
   "metadata": {},
   "source": [
    "## Task 1: Load the MNIST Dataset\n",
    "**Context:** Understand how digit images are represented numerically.\n",
    "\n",
    "**Steps:**\n",
    "1. Load the digits dataset from Scikit-learn.\n",
    "2. Preview the shape of `X` (features) and `y` (target labels).\n",
    "3. Visualize a few digit images with `matshow()`.\n",
    "\n",
    "**Prompting Questions:**\n",
    "\n",
    "- What does each row in `X` represent?\n",
    "- How is pixel intensity encoded?\n",
    "\n",
    "üí° **Tip:** Pixel values typically range from 0 to 16 ‚Äî we'll normalize them next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load MNIST digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Features and target\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Preview dataset\n",
    "# <your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab17c5f",
   "metadata": {},
   "source": [
    "## Task 2: Splitting Data\n",
    "**Context:** Separate data into training and testing subsets.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Use `train_test_split()` to divide data (e.g., 80% train, 20% test).\n",
    "2. Use stratification `(stratify=y)` to maintain balanced class distributions.\n",
    "\n",
    "**Prompting Questions:**\n",
    "\n",
    "- Why is stratified sampling important in classification tasks?\n",
    "\n",
    "üí° **Tip:** Set `random_state=42` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e84f6ca",
   "metadata": {},
   "source": [
    "## Task 3: Data Preparation\n",
    "**Context:** Normalize features to help SVM training.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Use `StandardScaler` to normalize pixel values (center the data to mean 0 and scale to unit variance).\n",
    "2. Standardization helps create more stable and fairer decision boundaries.\n",
    "\n",
    "**Prompting Questions:**\n",
    "\n",
    "- Are features roughly normalized?\n",
    "- Why does SVM performance improve with scaling?\n",
    "\n",
    "üí° **Tip:**`StandardScaler` or simple division by 16 can work ‚Äî SVMs are sensitive to feature scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c630dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pixel intensity features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a600d2f",
   "metadata": {},
   "source": [
    "## Task 4: Train SVM with a Linear Kernel\n",
    "**Context:** Baseline SVM model with linear decision boundaries.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create and train an `SVC(kernel='linear')`.\n",
    "2. Fit the model to the training data.\n",
    "3. Measure training time if interested.\n",
    "\n",
    "**Prompting Questions:**\n",
    "\n",
    "- How fast was training with a linear kernel?\n",
    "- How complex are the decision boundaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2898f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an SVM with a linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177117fe",
   "metadata": {},
   "source": [
    "## Task 5: Evaluate Linear SVM\n",
    "**Context:** Assess how well the linear SVM performs.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Predict labels for the test set.\n",
    "2. Calculate accuracy and F1-score.\n",
    "3. Generate a confusion matrix.\n",
    "\n",
    "**Prompting Questions:**\n",
    "\n",
    "- Which digits are most commonly confused?\n",
    "- Is overall performance satisfactory?\n",
    "\n",
    "üí° **Tip:** `classification_report()` gives precision, recall, and F1 easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af978c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the linear kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c25c52",
   "metadata": {},
   "source": [
    "## Task 6: Train SVM with an RBF Kernel\n",
    "**Context:** Move to a more flexible, non-linear SVM.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create and train an `SVC(kernel='rbf')`.\n",
    "2. Fit the model to the training data.\n",
    "3. Optionally set parameters like `C=1.0` and `gamma='scale'` initially.\n",
    "\n",
    "**Prompting Questions:**\n",
    "\n",
    "- Was RBF slower or faster to train?\n",
    "- Does the RBF kernel capture non-linear patterns better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an SVM with an RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a782d",
   "metadata": {},
   "source": [
    "## Task 7: Evaluate and Compare SVM Kernels \n",
    "**Context:** Compare performance between linear and RBF kernels.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Predict with the RBF model on test data.\n",
    "2. Calculate accuracy and F1-score again.\n",
    "3. Compare confusion matrices between kernels.\n",
    "\n",
    "**Prompting Questions:**\n",
    "\n",
    "- Which digits benefit most from non-linear decision boundaries?\n",
    "- Is there a noticeable improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and compare the RBF kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdcbaf9",
   "metadata": {},
   "source": [
    "## Task 8: Tune Hyperparameters (Optional Extension)\n",
    "**Context:** Further boost performance with careful tuning.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Use `GridSearchCV` to search over `C` and `gamma values`.\n",
    "2. Find the best model and re-evaluate on test data.\n",
    "\n",
    "**Prompting Questions:**\n",
    "\n",
    "- How sensitive is model performance to C and gamma?\n",
    "- Is the search space wide enough?\n",
    "\n",
    "üí° **Tip:** Try `C: [0.1, 1, 10` and `gamma: ['scale', 0.01, 0.001]` for a quick grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57054c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters for the RBF kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa0130",
   "metadata": {},
   "source": [
    "## Task 9: Reflect on Kernel Impact\n",
    "**Context:** Understand why kernels matter.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Consider advantages and trade-offs of linear vs RBF kernels.\n",
    "2. Reflect on model performance vs computational cost.\n",
    "\n",
    "**Prompting Questions:**\n",
    "\n",
    "- When would you prefer a linear kernel?\n",
    "- When is RBF worth the extra computational cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflect on Kernel Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110426b",
   "metadata": {},
   "source": [
    "## ‚úÖ Success Checklist\n",
    "- Dataset loaded and normalized\n",
    "- Train/test split completed\n",
    "- Linear SVM trained, evaluated, confusion matrix created\n",
    "- RBF SVM trained, evaluated, confusion matrix created\n",
    "- Comparison and reflection documented\n",
    "\n",
    "## üîç Common Issues & Solutions\n",
    "**Problem:** Training takes too long\n",
    "\n",
    " **Solution:** Try a smaller subset or adjust hyperparameters (especially C and gamma)\n",
    " \n",
    "**Problem:** Poor accuracy with linear kernel\n",
    "\n",
    " **Solution:** RBF may better capture non-linear patterns in image data\n",
    " \n",
    "**Problem:** Confusion matrix hard to read\n",
    "\n",
    " **Solution:** Use ConfusionMatrixDisplay from sklearn.metrics\n",
    "\n",
    "## üîë Key Points\n",
    "- SVMs with linear kernels are faster but limited in flexibility\n",
    "- RBF kernels capture complex relationships but need careful tuning\n",
    "- Scaling features is critical for SVM performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314930a",
   "metadata": {},
   "source": [
    "## üíªExemplar Solution\n",
    "After completing this activity (or if you get stuck!), take a moment to review the exemplar solution. This sample solution can offer insights into different techniques and approaches.\n",
    "Reflect on what you can learn from the exemplar solution to improve your coding skills.\n",
    "Remember, multiple solutions can exist for some problems; the goal is to learn and grow as a programmer by exploring various approaches.\n",
    "Use the exemplar solution as a learning tool to enhance your understanding and refine your approach to coding challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da3fb0e",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><strong>Click HERE to see an exemplar solution</strong></summary>    \n",
    "    \n",
    "```python\n",
    "\n",
    "# -------------------------------\n",
    "# Task 1: Load and Explore MNIST Dataset\n",
    "# -------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Features and labels\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Preview the dataset\n",
    "print(X.shape)  # (1797, 64)\n",
    "print(y.shape)  # (1797,)\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[0])\n",
    "plt.title(f\"Label: {digits.target[0]}\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Task 2: Splitting Data\n",
    "# -------------------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y # Use original X here\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape) # Optional prints\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# Task 3: Data Preparation (Normalize Features)\n",
    "# -------------------------------\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler ONLY on the training data and transform training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# Transform the test data using the scaler fitted on training data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Optional: Print shapes after scaling (should be the same)\n",
    "print(\"Training set shape after scaling:\", X_train_scaled.shape)\n",
    "print(\"Testing set shape after scaling:\", X_test_scaled.shape)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Task 4: Train SVM with a Linear Kernel\n",
    "# -------------------------------\n",
    "\n",
    "# Train SVM with a linear kernel\n",
    "svc_linear = SVC(kernel='linear', random_state=42)\n",
    "# Fit using the SCALED training data\n",
    "svc_linear.fit(X_train_scaled, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# Task 5: Evaluate Linear SVM\n",
    "# -------------------------------\n",
    "\n",
    "# Predict and evaluate using the SCALED test data\n",
    "y_pred_linear = svc_linear.predict(X_test_scaled)\n",
    "\n",
    "# Accuracy and Classification Report\n",
    "print(\"\\nLinear Kernel SVM Accuracy:\", accuracy_score(y_test, y_pred_linear))\n",
    "print(classification_report(y_test, y_pred_linear))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_linear = confusion_matrix(y_test, y_pred_linear)\n",
    "# Use the display class directly\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_linear)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix - Linear SVM')\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Task 6: Train SVM with an RBF Kernel\n",
    "# -------------------------------\n",
    "\n",
    "# Train SVM with RBF kernel\n",
    "# Optional initial parameters (as per guide)\n",
    "# svc_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svc_rbf = SVC(kernel='rbf', random_state=42) # Using defaults as in exemplar\n",
    "\n",
    "# Fit using the SCALED training data\n",
    "svc_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# Task 7: Evaluate and Compare SVM Kernels\n",
    "# -------------------------------\n",
    "\n",
    "# Predict and evaluate RBF SVM using the SCALED test data\n",
    "y_pred_rbf = svc_rbf.predict(X_test_scaled)\n",
    "\n",
    "# Accuracy and Classification Report\n",
    "print(\"\\nRBF Kernel SVM Accuracy:\", accuracy_score(y_test, y_pred_rbf))\n",
    "print(classification_report(y_test, y_pred_rbf))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_rbf = confusion_matrix(y_test, y_pred_rbf)\n",
    "# Use the display class directly\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_rbf)\n",
    "disp.plot(cmap='Purples')\n",
    "plt.title('Confusion Matrix - RBF SVM')\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Task 8: Tune Hyperparameters (Optional Extension)\n",
    "# -------------------------------\n",
    "\n",
    "# Set up grid search for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV with RBF kernel SVM\n",
    "# Note: The SVC estimator here should be one that will *also* be scaled if used independently,\n",
    "# but GridSearchCV handles fitting on folds correctly. The key is X_train_scaled input.\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf', random_state=42), param_grid, cv=3)\n",
    "\n",
    "# Fit Grid Search using the SCALED training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and performance\n",
    "print(\"\\nBest parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score (on training data):\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate best model found by Grid Search using the SCALED test data\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nTuned RBF SVM Test Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# -------------------------------\n",
    "# Task 9: Reflect on Kernel Impact\n",
    "# -------------------------------\n",
    "\n",
    "\"\"\"Reflections:\n",
    "- Linear kernel SVM performed reasonably well (~95% accuracy).\n",
    "- RBF kernel significantly improved classification (~98%+ accuracy).\n",
    "- Non-linear kernels like RBF capture more complex boundaries between digit classes.\n",
    "- RBF kernel requires more computational power but achieves better results.\n",
    "- Hyperparameter tuning (C and gamma) further optimized the RBF model for maximum accuracy.\n",
    "- Visualized confusion matrices show that some digits (e.g., 3 vs 5) were better separated with RBF than linear.\n",
    "\"\"\"\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
